{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Signature Nodes Documentation","text":"<p>A powerful collection of custom nodes for ComfyUI that provides essential image processing, data handling, and workflow management capabilities.</p> <p>\ud83d\udcda View Full Documentation</p>"},{"location":"#installation","title":"\ud83d\ude80 Installation","text":"<ol> <li>Navigate to your ComfyUI custom nodes directory:</li> </ol> <pre><code>cd ComfyUI/custom_nodes/\n</code></pre> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/signatureai/signature-nodes.git\n</code></pre> <ol> <li>Install the required dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"#node-categories","title":"\ud83d\udce6 Node Categories","text":"<ul> <li>\u26a1 Basic</li> <li>\ud83e\uddf1 Primitives - Basic data type nodes</li> <li>\ud83d\udd22 Numbers - Numerical operations and processing</li> <li>\ud83d\udd24 Text - Text processing and manipulation nodes</li> <li>\ud83d\udcc1 File - File handling operations</li> <li>\ud83d\uddbc\ufe0f Image - Basic image handling nodes</li> <li>\ud83c\udfad Mask - Mask generation and operations</li> <li>\ud83d\uddbc\ufe0f Image Processing - Advanced image processing and manipulation nodes</li> <li>\ud83e\udd16 Models - AI model integration nodes</li> <li>\ud83e\udde0 Logic - Logic operations and control flow</li> <li>\ud83d\udee0\ufe0f Utils - Utility functions</li> <li>\ud83d\udce6 Others</li> <li>\ud83d\udd00 Augmentations - Image augmentation tools</li> <li>\ud83d\udd0c Platform I/O - Platform integration nodes</li> <li>\ud83d\udcca Data - Data conversion and handling</li> <li>\ud83e\uddec Loras - LoRA model handling and integration</li> </ul>"},{"location":"#usage","title":"\ud83d\udcbb Usage","text":"<p>After installation, the Signature nodes will be available in your ComfyUI workspace under the \"\ud83d\udd32 Signature Nodes\" category. Each node is designed to be intuitive and includes proper input validation and error handling.</p>"},{"location":"#example-workflow","title":"Example Workflow","text":"<ol> <li>Load an image using <code>ImageFromWeb</code> or <code>ImageFromBase64</code></li> <li>Apply transformations using nodes like <code>ImageTranspose</code> or <code>UpscaleImage</code></li> <li>Process the image using various filter nodes</li> <li>Export the result using <code>PlatformOutput</code> or save directly</li> </ol>"},{"location":"#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<ul> <li><code>nodes/</code> - Node implementations</li> <li><code>web/</code> - Web interface components</li> <li><code>categories.py</code> - Node category definitions</li> <li><code>shared.py</code> - Shared utilities and constants</li> <li><code>platform_io.py</code> - Platform integration</li> <li><code>wrapper.py</code> - Workflow wrapper functionality</li> <li><code>docs/</code> - Documentation files</li> <li><code>scripts/</code> - Development and build scripts</li> </ul>"},{"location":"#development-setup","title":"\ud83d\udee0 Development Setup","text":"<ol> <li>Install uv if not already installed:</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <ol> <li>Install the python environment:</li> </ol> <pre><code>uv sync\n</code></pre> <ol> <li>Install the pre-commit hooks for checks and file format fixed (after doing this, git will \"remember\" that this uv    environment is the python environment which should be used to run the pre-commit hooks):</li> </ol> <pre><code>uv run pre-commit install\n</code></pre> <ol> <li>Generate documentation:</li> </ol> <pre><code>uv run python scripts/generate_docs.py\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Documentation is built using MkDocs with the Material theme. To view the documentation locally:</p> <ol> <li>Install MkDocs and dependencies:</li> </ol> <pre><code>uv sync --group doc\n</code></pre> <ol> <li>Serve the documentation:</li> </ol> <pre><code>uv run mkdocs serve\n</code></pre> <p>The documentation will be available at <code>http://localhost:8000</code>.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p>"},{"location":"#working-with-aws-codeartifact","title":"Working with AWS CodeArtifact","text":"<p>The process is documented on his Notion page.</p>"},{"location":"#git-flow-usage-guide","title":"Git Flow Usage Guide","text":"<p>[Setup Reference]  [Usage Reference]</p> <p>Git Flow is a branching model that helps teams manage feature development, releases, and hotfixes in a systematic way.</p>"},{"location":"#installation_1","title":"Installation","text":""},{"location":"#osx-homebrew","title":"OSX (Homebrew)","text":"<pre><code>brew install git-flow\n</code></pre>"},{"location":"#initial-setup","title":"Initial Setup","text":""},{"location":"#initialize-repository","title":"Initialize Repository","text":"<pre><code>cd my/project\ngit commit -am \"Initial commit\"\ngit remote add origin git@github.com:username/Project-Name.git\ngit push -u origin main\n</code></pre>"},{"location":"#prepare-for-development","title":"Prepare for Development","text":"<pre><code>git checkout -b develop\ngit push -u origin develop\n</code></pre> <ul> <li>Important: Set the default branch to <code>develop</code> on GitHub!</li> </ul>"},{"location":"#initialize-git-flow","title":"Initialize Git Flow","text":"<p>Run the initialization command:</p> <pre><code>git flow init [-d]\n</code></pre> <p>Use <code>-d</code> flag to accept all defaults, or configure the following settings:</p> <ul> <li>Production branch: [<code>main</code>]</li> <li>Development branch: [<code>develop</code>]</li> <li>Feature prefix: [<code>feature/</code>]</li> <li>Release prefix: [<code>release/</code>]</li> <li>Hotfix prefix: [<code>hotfix/</code>]</li> <li>Support prefix: [<code>support/</code>]</li> <li>Version tag prefix: []</li> </ul>"},{"location":"#working-with-features","title":"Working with Features","text":""},{"location":"#start-a-new-feature","title":"Start a New Feature","text":"<pre><code>git flow feature start ML-123-my-feature\n</code></pre> <p>This creates a new branch <code>feature/ML-123-my-feature</code> based on <code>develop</code>.</p>"},{"location":"#complete-a-feature","title":"Complete a Feature","text":"<pre><code>git flow feature finish ML-123-my-feature\n</code></pre> <p>This action:</p> <ul> <li>Merges <code>feature/ML-123-my-feature</code> into <code>develop</code></li> <li>Removes the feature branch</li> <li>Switches back to <code>develop</code></li> </ul>"},{"location":"#creating-releases","title":"Creating Releases","text":""},{"location":"#start-a-release","title":"Start a Release","text":"<pre><code>git fetch --tags  # Check existing tags first\ngit flow release start 1.0\n</code></pre>"},{"location":"#during-release","title":"During Release","text":"<ol> <li> <p>Update version numbers in documentation</p> </li> <li> <p>Make final adjustments (no new features!)</p> </li> <li> <p>Commit changes:</p> </li> </ol> <pre><code>git commit -am \"Bumped version number to 1.0\"\n</code></pre>"},{"location":"#finish-release","title":"Finish Release","text":"<pre><code>git flow release finish 1.0\ngit push origin main\ngit push origin develop\ngit push --tags\n</code></pre> <p>This action:</p> <ul> <li>Merges release into <code>main</code></li> <li>Tags the release</li> <li>Back-merges into <code>develop</code></li> <li>Removes the release branch</li> </ul>"},{"location":"#handling-hotfixes","title":"Handling Hotfixes","text":""},{"location":"#start-a-hotfix","title":"Start a Hotfix","text":"<pre><code>git flow hotfix start 1.0.1\n</code></pre>"},{"location":"#during-hotfix","title":"During Hotfix","text":"<ol> <li> <p>Update version numbers</p> </li> <li> <p>Fix the critical bug</p> </li> <li> <p>Commit changes:</p> </li> </ol> <pre><code>git commit -am \"Fixed critical bug\"\n</code></pre>"},{"location":"#finish-hotfix","title":"Finish Hotfix","text":"<pre><code>git flow hotfix finish 1.0.1\ngit push origin main\ngit push origin develop\ngit push --tags\n</code></pre> <p>This action:</p> <ul> <li>Merges hotfix into both <code>main</code> and <code>develop</code></li> <li>Tags the new version</li> <li>Removes the hotfix branch</li> </ul>"},{"location":"#best-practices","title":"Best Practices","text":"<ol> <li> <p>Keep feature branches focused and short-lived</p> </li> <li> <p>Only bug fixes in release branches</p> </li> <li> <p>Use hotfixes only for critical production issues</p> </li> <li> <p>Always push your changes to remote after finishing features/releases/hotfixes</p> </li> </ol>"},{"location":"nodes/agent/","title":"Agent Nodes","text":""},{"location":"nodes/agent_memory/","title":"Agent Memory Nodes","text":""},{"location":"nodes/agent_response_post_process/","title":"Agent Response Post Process Nodes","text":""},{"location":"nodes/agent_response_validators/","title":"Agent Response Validators Nodes","text":""},{"location":"nodes/agent_tools/","title":"Agent Tools Nodes","text":""},{"location":"nodes/any/","title":"Any Nodes","text":""},{"location":"nodes/any/#anytolist","title":"AnyToList","text":"<p>Converts any value to a list.</p>"},{"location":"nodes/any/#inputs","title":"Inputs","text":"Group Name Type Default Extras required any_value <code>any_type</code>"},{"location":"nodes/any/#returns","title":"Returns","text":"Name Type list <code>LIST</code> Source code <pre><code>class AnyToList:\n    \"\"\"Converts any value to a list.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"any_value\": (any_type, {}),\n            }\n        }\n\n    RETURN_TYPES = (\"LIST\",)\n    RETURN_NAMES = (\"list_value\",)\n    FUNCTION = \"process\"\n    CATEGORY = ANY_CAT\n    OUTPUT_NODE = True\n\n    def process(self, any_value):\n        return (any_value,)\n</code></pre>"},{"location":"nodes/any/#anytodict","title":"AnyToDict","text":"<p>Converts any value to a dictionary.</p>"},{"location":"nodes/any/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required any_value <code>any_type</code>"},{"location":"nodes/any/#returns_1","title":"Returns","text":"Name Type dict <code>DICT</code> Source code <pre><code>class AnyToDict:\n    \"\"\"Converts any value to a dictionary.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"any_value\": (any_type, {}),\n            }\n        }\n\n    RETURN_TYPES = (\"DICT\",)\n    RETURN_NAMES = (\"dict_value\",)\n    FUNCTION = \"process\"\n    CATEGORY = ANY_CAT\n    OUTPUT_NODE = True\n\n    def process(self, any_value):\n        return (any_value,)\n</code></pre>"},{"location":"nodes/audio/","title":"Audio Nodes","text":""},{"location":"nodes/augmentation/","title":"Augmentation Nodes","text":""},{"location":"nodes/augmentation/#randomcropaugmentation","title":"RandomCropAugmentation","text":"<p>Applies random crop augmentation to images with configurable dimensions and frequency.</p> <p>This node performs random cropping operations on input images. It allows precise control over the crop dimensions through minimum and maximum window sizes, target dimensions, and application probability.</p>"},{"location":"nodes/augmentation/#inputs","title":"Inputs","text":"Group Name Type Default Extras required height <code>INT</code> 1024 min=32, step=32 required width <code>INT</code> 1024 min=32, step=32 required min_window <code>INT</code> 256 step=32 required max_window <code>INT</code> 1024 step=32 required percent <code>FLOAT</code> 1.0 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class RandomCropAugmentation:\n    \"\"\"Applies random crop augmentation to images with configurable dimensions and frequency.\n\n    This node performs random cropping operations on input images. It allows precise control over the\n    crop dimensions through minimum and maximum window sizes, target dimensions, and application\n    probability.\n\n    Args:\n        height (int): Target height for the crop operation. Must be at least 32 and a multiple of 32.\n        width (int): Target width for the crop operation. Must be at least 32 and a multiple of 32.\n        min_window (int): Minimum size of the crop window. Must be a multiple of 32.\n        max_window (int): Maximum size of the crop window. Must be a multiple of 32.\n        percent (float): Probability of applying the crop, from 0.0 to 1.0.\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with. Defaults to None.\n\n    Returns:\n        tuple: Contains a single element:\n            augmentation (AUGMENTATION): The configured crop augmentation operation.\n\n    Raises:\n        ValueError: If any dimension parameters are not multiples of 32.\n        ValueError: If min_window is larger than max_window.\n        ValueError: If percent is not between 0.0 and 1.0.\n\n    Notes:\n        - Window size is randomly selected between min_window and max_window for each operation\n        - Can be chained with other augmentations through the augmentation parameter\n        - All dimension parameters must be multiples of 32 for proper operation\n        - Setting percent to 1.0 ensures the crop is always applied\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"height\": (\"INT\", {\"default\": 1024, \"min\": 32, \"step\": 32}),\n                \"width\": (\"INT\", {\"default\": 1024, \"min\": 32, \"step\": 32}),\n                \"min_window\": (\"INT\", {\"default\": 256, \"step\": 32}),\n                \"max_window\": (\"INT\", {\"default\": 1024, \"step\": 32}),\n                \"percent\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"\n    Performs random cropping on images with configurable dimensions and frequency.\n    Control crop window size range and target output dimensions.\n    Chain with other augmentations for diverse composition variations.\"\"\"\n\n    def execute(\n        self,\n        height: int = 1024,\n        width: int = 1024,\n        min_window: int = 256,\n        max_window: int = 1024,\n        percent: float = 1.0,\n        augmentation: list | None = None,\n    ) -&gt; tuple[list]:\n        augmentation = random_crop_augmentation(height, width, min_window, max_window, percent, augmentation)\n\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#composeaugmentation","title":"ComposeAugmentation","text":"<p>Combines and applies multiple augmentation operations with consistent random transformations.</p> <p>This node orchestrates the application of multiple augmentation operations to images and masks. It provides control over sample generation and reproducibility through seed management.</p>"},{"location":"nodes/augmentation/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required augmentation <code>AUGMENTATION</code> required samples <code>INT</code> 1 min=1 required seed <code>INT</code> max=10000000000000000 optional image <code>IMAGE</code> None optional mask <code>MASK</code> None"},{"location":"nodes/augmentation/#returns_1","title":"Returns","text":"Name Type image <code>IMAGE</code> mask <code>MASK</code> Source code <pre><code>class ComposeAugmentation:\n    \"\"\"Combines and applies multiple augmentation operations with consistent random transformations.\n\n    This node orchestrates the application of multiple augmentation operations to images and masks. It\n    provides control over sample generation and reproducibility through seed management.\n\n    Args:\n        augmentation (AUGMENTATION): The augmentation operation or chain to apply.\n        samples (int): Number of augmented versions to generate. Must be &gt;= 1.\n        seed (int): Random seed for reproducible results. Use -1 for random seeding.\n            Valid range: -1 to 10000000000000000.\n        image (IMAGE, optional): Input image to augment. Defaults to None.\n        mask (MASK, optional): Input mask to augment. Defaults to None.\n\n    Returns:\n        tuple: Contains two elements:\n            images (List[IMAGE]): List of augmented versions of the input image.\n            masks (List[MASK]): List of augmented versions of the input mask.\n\n    Raises:\n        ValueError: If neither image nor mask is provided.\n        ValueError: If samples is less than 1.\n        ValueError: If seed is outside valid range.\n\n    Notes:\n        - At least one of image or mask must be provided\n        - All augmentations are applied consistently to both image and mask\n        - Output is always returned as lists, even when samples=1\n        - Using a fixed seed ensures reproducible augmentations\n        - Supports chaining multiple augmentations through the augmentation parameter\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"augmentation\": (\"AUGMENTATION\",),\n                \"samples\": (\"INT\", {\"default\": 1, \"min\": 1}),\n                \"seed\": (\"INT\", {\"default\": -1, \"min\": -1, \"max\": 10000000000000000}),\n            },\n            \"optional\": {\n                \"image\": (\"IMAGE\", {\"default\": None}),\n                \"mask\": (\"MASK\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"IMAGE\",\n        \"MASK\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    OUTPUT_IS_LIST = (\n        True,\n        True,\n    )\n    DESCRIPTION = \"\"\"\n    Applies augmentations to images and masks, creating multiple variations with the same transformations.\n    Control the number of samples and use seeds for reproducible results.\n    Connect augmentation nodes to create complex transformation chains.\"\"\"\n\n    def execute(\n        self,\n        augmentation: list,\n        samples: int = 1,\n        seed: int = -1,\n        image: Optional[torch.Tensor] = None,\n        mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[list, list]:\n        # Create a dummy image if only mask is provided\n        if image is None and mask is not None:\n            image = torch.zeros_like(mask)\n\n        image_tensor = TensorImage.from_BWHC(image) if isinstance(image, torch.Tensor) else None\n        mask_tensor = TensorImage.from_BWHC(mask) if isinstance(mask, torch.Tensor) else None\n\n        total_images, total_masks = compose_augmentation(\n            augmentation=augmentation,\n            samples=samples,\n            image_tensor=image_tensor,\n            mask_tensor=mask_tensor,\n            seed=seed,\n        )\n\n        if total_images is None:\n            total_images = []\n        if total_masks is None:\n            total_masks = []\n        node_image = [image.get_BWHC() for image in total_images]\n        node_mask = [mask.get_BWHC() for mask in total_masks]\n\n        return (\n            node_image,\n            node_mask,\n        )\n</code></pre>"},{"location":"nodes/augmentation/#qualityaugmentation","title":"QualityAugmentation","text":"<p>Simulates image quality degradation through compression or downscaling.</p> <p>This node provides options to reduce image quality in ways that simulate real-world quality loss scenarios.</p>"},{"location":"nodes/augmentation/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required quality_type <code>LIST</code> compression required quality_limit <code>INT</code> 60 min=1, max=100 required percent <code>FLOAT</code> 0.2 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_2","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class QualityAugmentation:\n    \"\"\"Simulates image quality degradation through compression or downscaling.\n\n    This node provides options to reduce image quality in ways that simulate real-world\n    quality loss scenarios.\n\n    Args:\n        quality_type (str): Type of quality reduction:\n            - \"compression\": JPEG-like compression artifacts\n            - \"downscale\": Resolution reduction and upscaling\n        quality_limit (int): Quality parameter (1-100, lower = more degradation)\n        percent (float): Probability of applying the effect (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Compression type simulates JPEG artifacts\n        - Downscale type reduces and restores resolution\n        - Lower quality limits produce more visible artifacts\n        - Can be chained with other augmentations\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"quality_type\": (\n                    [\"compression\", \"downscale\"],\n                    {\"default\": \"compression\"},\n                ),\n                \"quality_limit\": (\"INT\", {\"default\": 60, \"min\": 1, \"max\": 100}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"\n    Simulates image quality degradation through compression artifacts or downscaling.\n    Control quality reduction level and application frequency.\n    Chain with other augmentations for realistic image imperfections.\"\"\"\n\n    def execute(\n        self,\n        quality_type: str = \"compression\",\n        quality_limit: int = 60,\n        percent: float = 0.2,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = quality_augmentation(\n            quality_type=quality_type,\n            quality_limit=quality_limit,\n            percent=percent,\n            augmentation=augmentation,\n        )\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#distortionaugmentation","title":"DistortionAugmentation","text":"<p>Applies geometric distortion effects to images.</p> <p>This node provides various types of geometric distortion with configurable severity and application probability.</p>"},{"location":"nodes/augmentation/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required distortion_type <code>LIST</code> optical required severity <code>INT</code> 1 min=1, max=5 required percent <code>FLOAT</code> 0.3 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_3","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class DistortionAugmentation:\n    \"\"\"Applies geometric distortion effects to images.\n\n    This node provides various types of geometric distortion with configurable severity\n    and application probability.\n\n    Args:\n        distortion_type (str): Type of distortion to apply:\n            - \"optical\": Lens-like distortion\n            - \"grid\": Grid-based warping\n            - \"elastic\": Elastic deformation\n        severity (int): Intensity of the distortion effect (1-5)\n        percent (float): Probability of applying the effect (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Each distortion type produces unique geometric deformations\n        - Higher severity values create stronger distortion effects\n        - Can be chained with other augmentations\n        - Maintains overall image structure while adding local deformations\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"distortion_type\": (\n                    [\"optical\", \"grid\", \"elastic\"],\n                    {\"default\": \"optical\"},\n                ),\n                \"severity\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 5}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"\n    Applies geometric distortion effects to images with optical, grid, or elastic deformations.\n    Control distortion intensity and application frequency.\n    Chain with other augmentations for complex transformations.\"\"\"\n\n    def execute(\n        self,\n        distortion_type: str = \"optical\",\n        severity: int = 1,\n        percent: float = 0.3,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = distortion_augmentation(\n            distortion_type=distortion_type,\n            severity=severity,\n            percent=percent,\n            augmentation=augmentation,\n        )\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#gridaugmentation","title":"GridAugmentation","text":"<p>Applies grid-based transformations to images.</p> <p>This node provides grid-based image modifications including shuffling and dropout effects.</p>"},{"location":"nodes/augmentation/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required grid_type <code>LIST</code> shuffle required grid_size <code>INT</code> 3 min=2, max=10 required percent <code>FLOAT</code> 0.3 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_4","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class GridAugmentation:\n    \"\"\"Applies grid-based transformations to images.\n\n    This node provides grid-based image modifications including shuffling and dropout\n    effects.\n\n    Args:\n        grid_type (str): Type of grid transformation:\n            - \"shuffle\": Randomly permute grid cells\n            - \"dropout\": Randomly remove grid cells\n        grid_size (int): Number of grid divisions (2-10)\n        percent (float): Probability of applying the effect (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Image is divided into grid_size x grid_size cells\n        - Shuffle randomly reorders grid cells\n        - Dropout replaces cells with black\n        - Can be chained with other augmentations\n        - Maintains overall image structure while adding local variations\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"grid_type\": ([\"shuffle\", \"dropout\"], {\"default\": \"shuffle\"}),\n                \"grid_size\": (\"INT\", {\"default\": 3, \"min\": 2, \"max\": 10}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"\n    Applies grid-based transformations to images with shuffle or dropout effects.\n    Control grid size and application frequency. Maintains overall image structure while adding local variations.\n    Chain with other augmentations for creative results.\"\"\"\n\n    def execute(\n        self,\n        grid_type: str = \"shuffle\",\n        grid_size: int = 3,\n        percent: float = 0.3,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = grid_augmentation(\n            grid_type=grid_type,\n            grid_size=grid_size,\n            percent=percent,\n            augmentation=augmentation,\n        )\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#shiftscaleaugmentation","title":"ShiftScaleAugmentation","text":"<p>Applies random shifting, scaling, and rotation transformations.</p> <p>This node combines multiple geometric transformations with configurable ranges and probability.</p>"},{"location":"nodes/augmentation/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required shift_limit <code>FLOAT</code> 0.1 min=0.0, max=1.0 required scale_limit <code>FLOAT</code> 0.2 min=0.0, max=1.0 required rotate_limit <code>INT</code> 45 min=0, max=180 required percent <code>FLOAT</code> 0.3 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_5","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class ShiftScaleAugmentation:\n    \"\"\"Applies random shifting, scaling, and rotation transformations.\n\n    This node combines multiple geometric transformations with configurable ranges\n    and probability.\n\n    Args:\n        shift_limit (float): Maximum shift as fraction of image size (0.0-1.0)\n        scale_limit (float): Maximum scale factor change (0.0-1.0)\n        rotate_limit (int): Maximum rotation angle in degrees (0-180)\n        percent (float): Probability of applying transformations (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Shift moves image content within frame\n        - Scale changes overall image size\n        - Rotation angles are randomly sampled\n        - Can be chained with other augmentations\n        - All transformations are applied together when selected\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"shift_limit\": (\"FLOAT\", {\"default\": 0.1, \"min\": 0.0, \"max\": 1.0}),\n                \"scale_limit\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 1.0}),\n                \"rotate_limit\": (\"INT\", {\"default\": 45, \"min\": 0, \"max\": 180}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"\n    Applies random shifting, scaling, and rotation transformations to images.\n    Control transformation ranges and application frequency.\n    Chain with other augmentations for diverse geometric variations.\"\"\"\n\n    def execute(\n        self,\n        shift_limit: float = 0.1,\n        scale_limit: float = 0.2,\n        rotate_limit: int = 45,\n        percent: float = 0.3,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = shift_scale_augmentation(\n            shift_limit=shift_limit,\n            scale_limit=scale_limit,\n            rotate_limit=rotate_limit,\n            percent=percent,\n            augmentation=augmentation,\n        )\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#bluraugmentation","title":"BlurAugmentation","text":"<p>Applies various types of blur effects to images.</p> <p>This node provides multiple blur algorithms with configurable parameters for image softening effects.</p>"},{"location":"nodes/augmentation/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required blur_type <code>LIST</code> gaussian required blur_limit_min <code>INT</code> 3 min=3, step=3 required blur_limit_max <code>INT</code> 87 min=3, step=3 required percent <code>FLOAT</code> 0.3 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_6","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class BlurAugmentation:\n    \"\"\"Applies various types of blur effects to images.\n\n    This node provides multiple blur algorithms with configurable parameters for image\n    softening effects.\n\n    Args:\n        blur_type (str): Type of blur to apply:\n            - \"gaussian\": Gaussian blur\n            - \"motion\": Motion blur\n            - \"median\": Median filter blur\n        blur_limit_min (int): Minimum blur kernel size (must be multiple of 3)\n        blur_limit_max (int): Maximum blur kernel size (must be multiple of 3)\n        percent (float): Probability of applying the blur (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Kernel size is randomly selected between min and max limits\n        - Different blur types produce distinct softening effects\n        - Larger kernel sizes create stronger blur effects\n        - Can be chained with other augmentations\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"blur_type\": (\n                    [\"gaussian\", \"motion\", \"median\"],\n                    {\"default\": \"gaussian\"},\n                ),\n                \"blur_limit_min\": (\"INT\", {\"default\": 3, \"min\": 3, \"step\": 3}),\n                \"blur_limit_max\": (\"INT\", {\"default\": 87, \"min\": 3, \"step\": 3}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"Adds blur effects to images with gaussian, motion, or median styles.\n    Control blur strength and application frequency. Chain with other augmentations for creative transformations.\"\"\"\n\n    def execute(\n        self,\n        blur_type: str = \"gaussian\",\n        blur_limit_min: int = 3,\n        blur_limit_max: int = 87,\n        percent: float = 0.3,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        blur_limit = (blur_limit_min, blur_limit_max)\n        augmentation = blur_augmentation(\n            blur_type=blur_type,\n            blur_limit=blur_limit,\n            percent=percent,\n            augmentation=augmentation,\n        )\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#brightnesscontrastaugmentation","title":"BrightnessContrastAugmentation","text":"<p>Applies brightness and contrast adjustments to images.</p> <p>This node provides controls for adjusting image brightness and contrast with configurable limits and probability of application.</p>"},{"location":"nodes/augmentation/#inputs_7","title":"Inputs","text":"Group Name Type Default Extras required brightness_limit <code>FLOAT</code> 0.2 min=0.0, max=1.0 required contrast_limit <code>FLOAT</code> 0.2 min=0.0, max=1.0 required percent <code>FLOAT</code> 0.5 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_7","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class BrightnessContrastAugmentation:\n    \"\"\"Applies brightness and contrast adjustments to images.\n\n    This node provides controls for adjusting image brightness and contrast with configurable\n    limits and probability of application.\n\n    Args:\n        brightness_limit (float): Maximum brightness adjustment range (0.0-1.0)\n        contrast_limit (float): Maximum contrast adjustment range (0.0-1.0)\n        percent (float): Probability of applying the augmentation (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Brightness adjustments are applied as multiplicative factors\n        - Contrast adjustments modify image histogram spread\n        - Can be chained with other augmentations\n        - Actual adjustment values are randomly sampled within limits\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"brightness_limit\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 1.0}),\n                \"contrast_limit\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 1.0}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"Adjusts image brightness and contrast during generation.\n    Control modification ranges and application frequency.\n    Chain with other augmentations for creative image variations.\"\"\"\n\n    def execute(\n        self,\n        brightness_limit: float = 0.2,\n        contrast_limit: float = 0.2,\n        percent: float = 0.5,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = brightness_contrast_augmentation(\n            brightness_limit=brightness_limit,\n            contrast_limit=contrast_limit,\n            percent=percent,\n            augmentation=augmentation,\n        )\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#perspectiveaugmentation","title":"PerspectiveAugmentation","text":"<p>Applies perspective transformation effects to images.</p> <p>This node creates perspective distortion effects that simulate viewing angle changes, with configurable strength and probability.</p>"},{"location":"nodes/augmentation/#inputs_8","title":"Inputs","text":"Group Name Type Default Extras required scale <code>FLOAT</code> 0.05 min=0.01, max=0.5 required keep_size <code>BOOLEAN</code> True required percent <code>FLOAT</code> 0.3 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_8","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class PerspectiveAugmentation:\n    \"\"\"Applies perspective transformation effects to images.\n\n    This node creates perspective distortion effects that simulate viewing angle changes,\n    with configurable strength and probability.\n\n    Args:\n        scale (float): Strength of perspective effect (0.01-0.5)\n        keep_size (bool): Whether to maintain original image size\n        percent (float): Probability of applying the effect (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Scale controls the intensity of perspective change\n        - keep_size=True maintains original dimensions\n        - keep_size=False may change image size\n        - Can be chained with other augmentations\n        - Simulates realistic viewing angle variations\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"scale\": (\"FLOAT\", {\"default\": 0.05, \"min\": 0.01, \"max\": 0.5}),\n                \"keep_size\": (\"BOOLEAN\", {\"default\": True}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"\n    Applies perspective transformations to images, simulating viewing angle changes.\n    Control effect strength and application frequency.\n    Chain with other augmentations for realistic spatial variations.\"\"\"\n\n    def execute(\n        self,\n        scale: float = 0.05,\n        keep_size: bool = True,\n        percent: float = 0.3,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = perspective_augmentation(\n            scale=scale, keep_size=keep_size, percent=percent, augmentation=augmentation\n        )\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#flipaugmentation","title":"FlipAugmentation","text":"<p>Applies horizontal or vertical flip augmentation to images with configurable probability.</p> <p>This node performs random flip transformations on input images. It supports both horizontal and vertical flip operations with adjustable probability of application.</p>"},{"location":"nodes/augmentation/#inputs_9","title":"Inputs","text":"Group Name Type Default Extras required flip <code>LIST</code> horizontal required percent <code>FLOAT</code> 0.5 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_9","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class FlipAugmentation:\n    \"\"\"Applies horizontal or vertical flip augmentation to images with configurable probability.\n\n    This node performs random flip transformations on input images. It supports both horizontal and\n    vertical flip operations with adjustable probability of application.\n\n    Args:\n        flip (str): Direction of flip operation, either:\n            - \"horizontal\": Flips image left to right\n            - \"vertical\": Flips image top to bottom\n        percent (float): Probability of applying the flip, from 0.0 to 1.0.\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with. Defaults to None.\n\n    Returns:\n        tuple: Contains a single element:\n            augmentation (AUGMENTATION): The configured flip augmentation operation.\n\n    Raises:\n        ValueError: If flip direction is not \"horizontal\" or \"vertical\".\n        ValueError: If percent is not between 0.0 and 1.0.\n\n    Notes:\n        - Flip direction cannot be changed after initialization\n        - Setting percent to 0.5 applies the flip to approximately half of all samples\n        - Can be chained with other augmentations through the augmentation parameter\n        - Transformations are applied consistently when used with ComposeAugmentation\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"flip\": ([\"horizontal\", \"vertical\"], {\"default\": \"horizontal\"}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"\n    Applies horizontal or vertical flip transformations to images with adjustable probability.\n    Creates mirror-image variations of your content.\n    Chain with other augmentations for diverse image transformations.\"\"\"\n\n    def execute(\n        self,\n        flip: str = \"horizontal\",\n        percent: float = 0.5,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = flip_augmentation(flip, percent, augmentation)\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#rotationaugmentation","title":"RotationAugmentation","text":"<p>Rotates images by random angles within specified limits.</p> <p>This node performs random rotation augmentation with configurable angle limits and application probability.</p>"},{"location":"nodes/augmentation/#inputs_10","title":"Inputs","text":"Group Name Type Default Extras required limit <code>INT</code> 45 min=0, max=180 required percent <code>FLOAT</code> 0.5 min=0.0, max=1.0 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_10","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class RotationAugmentation:\n    \"\"\"Rotates images by random angles within specified limits.\n\n    This node performs random rotation augmentation with configurable angle limits and\n    application probability.\n\n    Args:\n        limit (int): Maximum rotation angle in degrees (0-180)\n        percent (float): Probability of applying the rotation (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Rotation angles are randomly sampled between -limit and +limit\n        - Empty areas after rotation are filled with black\n        - Can be chained with other augmentations\n        - Original aspect ratio is preserved\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"limit\": (\"INT\", {\"default\": 45, \"min\": 0, \"max\": 180}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"\n    Rotates images by random angles within specified limits.\n    Control rotation angle range and application frequency.\n    Chain with other augmentations for diverse orientation variations.\"\"\"\n\n    def execute(\n        self,\n        limit: int = 45,\n        percent: float = 0.5,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = rotation_augmentation(limit=limit, percent=percent, augmentation=augmentation)\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/augmentation/#cutoutaugmentation","title":"CutoutAugmentation","text":"<p>Creates random rectangular cutouts in images.</p> <p>This node randomly removes rectangular regions from images by filling them with black, useful for regularization and robustness training.</p>"},{"location":"nodes/augmentation/#inputs_11","title":"Inputs","text":"Group Name Type Default Extras required num_holes <code>INT</code> 8 min=1, max=20 required max_size <code>INT</code> 30 min=1, max=100 required percent <code>FLOAT</code> 0.3 min=0.0, max=1.0 optional min_num_holes <code>INT</code> 1 min=1, max=20 optional min_size <code>INT</code> 1 min=1, max=100 optional augmentation <code>AUGMENTATION</code> None"},{"location":"nodes/augmentation/#returns_11","title":"Returns","text":"Name Type augmentation <code>AUGMENTATION</code> Source code <pre><code>class CutoutAugmentation:\n    \"\"\"Creates random rectangular cutouts in images.\n\n    This node randomly removes rectangular regions from images by filling them with black,\n    useful for regularization and robustness training.\n\n    Args:\n        num_holes (int): Number of cutout regions to create (1-20)\n        max_size (int): Maximum size of cutout regions in pixels (1-100)\n        percent (float): Probability of applying cutouts (0.0-1.0)\n        augmentation (AUGMENTATION, optional): Existing augmentation to chain with\n\n    Returns:\n        tuple[AUGMENTATION]: Single-element tuple containing the configured augmentation\n\n    Notes:\n        - Cutout positions are randomly selected\n        - Each cutout region is independently sized\n        - Regions are filled with black (zero) values\n        - Can be chained with other augmentations\n        - Useful for preventing overfitting\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"num_holes\": (\"INT\", {\"default\": 8, \"min\": 1, \"max\": 20}),\n                \"max_size\": (\"INT\", {\"default\": 30, \"min\": 1, \"max\": 100}),\n                \"percent\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0}),\n            },\n            \"optional\": {\n                \"min_num_holes\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 20}),\n                \"min_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 100}),\n                \"augmentation\": (\"AUGMENTATION\", {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (\"AUGMENTATION\",)\n    FUNCTION = \"execute\"\n    CATEGORY = AUGMENTATION_CAT\n    DESCRIPTION = \"\"\"Creates random black rectangular cutouts in images.\n    Control number, size, and frequency of cutouts.\n    Useful for regularization and preventing overfitting.\n    Chain with other augmentations for creative effects.\"\"\"\n\n    def execute(\n        self,\n        num_holes: int = 8,\n        max_size: int = 30,\n        percent: float = 0.3,\n        min_num_holes: int = 1,\n        min_size: int = 1,\n        augmentation: Optional[list] = None,\n    ) -&gt; tuple[list]:\n        augmentation = cutout_augmentation(\n            num_holes=num_holes,\n            max_size=max_size,\n            percent=percent,\n            min_num_holes=min_num_holes,\n            min_size=min_size,\n            augmentation=augmentation,\n        )\n        return (augmentation,)\n</code></pre>"},{"location":"nodes/clearml/","title":"Clearml Nodes","text":""},{"location":"nodes/clearml/#reportscore","title":"ReportScore","text":"<p>Reports scores to ClearML.</p>"},{"location":"nodes/clearml/#inputs","title":"Inputs","text":"Group Name Type Default Extras required task <code>CLEARML_TASK</code> required score <code>FLOAT</code> required name <code>STRING</code> optional data <code>any_type</code>"},{"location":"nodes/clearml/#returns","title":"Returns","text":"Name Type clearml_task <code>CLEARML_TASK</code> Source code <pre><code>class ReportScore:\n    \"\"\"Reports scores to ClearML.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"task\": (\"CLEARML_TASK\", {}),\n                \"score\": (\"FLOAT\",),\n                \"name\": (\"STRING\",),\n            },\n            \"optional\": {\n                \"data\": (any_type,),\n            },\n        }\n\n    RETURN_TYPES = (\"CLEARML_TASK\",)\n    RETURN_NAMES = (\"clearml_task\",)\n    FUNCTION = \"process\"\n    CATEGORY = CLEARML_CAT\n    OUTPUT_NODE = True\n\n    def process(self, **kwargs):\n        task: Optional[Task] = kwargs.get(\"task\")\n        score: Optional[float] = kwargs.get(\"score\")\n        name: Optional[str] = kwargs.get(\"name\")\n        data: Optional[Any] = kwargs.get(\"data\")\n\n        if task is None or score is None or name is None:\n            raise ValueError(\"Task, score, and name are required\")\n\n        task.get_logger().report_single_value(name, score)\n\n        if data is not None:\n            df = pd.DataFrame(data)\n            task.register_artifact(\"data\", df)\n\n        return (task,)\n</code></pre>"},{"location":"nodes/data/","title":"Data Nodes","text":""},{"location":"nodes/data/#dict2yaml","title":"Dict2Yaml","text":"<p>Converts Python dictionaries to YAML strings for data interchange.</p> <p>A node that serializes Python dictionaries into YAML-formatted strings, facilitating data export and communication with external systems that require YAML format.</p>"},{"location":"nodes/data/#inputs","title":"Inputs","text":"Group Name Type Default Extras required dict <code>DICT</code>"},{"location":"nodes/data/#returns","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class Dict2Yaml:\n    \"\"\"Converts Python dictionaries to YAML strings for data interchange.\n\n    A node that serializes Python dictionaries into YAML-formatted strings, facilitating data\n    export and communication with external systems that require YAML format.\n\n    Args:\n        dict (dict): The Python dictionary to serialize.\n            Can contain nested dictionaries, lists, and primitive Python types.\n            All values must be JSON-serializable (dict, list, str, int, float, bool, None).\n\n    Returns:\n        tuple[str]: A single-element tuple containing:\n            - str: The YAML-formatted string representation of the input dictionary.\n\n    Raises:\n        ValueError: When dict is not a dictionary type.\n\n    Notes:\n        - All dictionary keys are converted to strings in the output YAML\n        - Complex Python objects (datetime, custom classes) must be pre-converted to basic types\n        - Handles nested structures of any depth\n        - Unicode characters are properly escaped in the output\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"dict\": (\"DICT\",),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CLASS_ID = \"dict_yaml\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Converts Python dictionaries to YAML-formatted strings. Handles nested structures and primitive data types.\n    Useful for data interchange with external systems that require YAML format.\"\"\"\n\n    def execute(self, dict: dict) -&gt; tuple[str]:\n        yaml_str = yaml.dump(dict)\n        return (yaml_str,)\n</code></pre>"},{"location":"nodes/data/#deletedictkey","title":"DeleteDictKey","text":"<p>Deletes a key from a dictionary.</p> <p>A node that provides key-based deletion of a dictionary while determining their Python type, enabling dynamic type handling and conditional processing in workflows.</p>"},{"location":"nodes/data/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required dict <code>DICT</code> required key <code>STRING</code>"},{"location":"nodes/data/#returns_1","title":"Returns","text":"Name Type dict <code>DICT</code> Source code <pre><code>class DeleteDictKey:\n    \"\"\"Deletes a key from a dictionary.\n\n    A node that provides key-based deletion of a dictionary while determining their Python\n    type, enabling dynamic type handling and conditional processing in workflows.\n\n    Args:\n        dict (dict): The source dictionary to delete values.\n            Must be a valid Python dictionary.\n            Can contain values of any type and nested structures.\n        key (str): The lookup key for value deletion.\n            Must be a string type.\n            Case-sensitive and must match exactly.\n            If the key starts with a dot it will be used as a path as in a jq query,\n            e.g. \".key1.key2[0]\" will delete the first value of array key2 in the dictionary key1.\n            Defaults to empty string.\n\n    Returns:\n        tuple[dict]: A tuple containing:\n            - dict: The updated dictionary with the key deleted.\n\n    Raises:\n        KeyError: When the specified key doesn't exist in the dictionary.\n        ValueError: When key is not a string or dict parameter is not a dictionary.\n\n    Notes:\n        - Supports dictionaries containing any Python type, including custom classes\n        - Thread-safe for concurrent access\n        - Preserves original data without modifications\n        - Handles nested data structures (dictionaries within dictionaries, lists, etc.)\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"dict\": (\"DICT\",),\n                \"key\": (\"STRING\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"DICT\",)\n    RETURN_NAMES = (\"new_dict\",)\n    FUNCTION = \"execute\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Removes a key from a dictionary.\n    Supports both direct key removal and path-based deletion using JQ syntax (with leading dot).\n    Returns the modified dictionary with the specified key removed.\"\"\"\n\n    def execute(self, dict: dict, key: str = \"\") -&gt; tuple[dict]:\n        if key.startswith(\".\"):\n            exists = jq.compile(key).input(dict).first()\n            if exists is None:\n                raise KeyError(f\"Key {key} not found in dictionary\")\n            delete_query = f\"del({key})\"\n            dict = jq.compile(delete_query).input(dict).first()\n        else:\n            if key not in dict:\n                raise KeyError(f\"Key {key} not found in dictionary\")\n            del dict[key]\n        return (dict,)\n</code></pre>"},{"location":"nodes/data/#getdictvalue","title":"GetDictValue","text":"<p>Retrieves and types dictionary values using string keys.</p> <p>A node that provides key-based access to dictionary values while determining their Python type, enabling dynamic type handling and conditional processing in workflows.</p>"},{"location":"nodes/data/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required dict <code>DICT</code> required key <code>STRING</code> Source code <pre><code>class GetDictValue:\n    \"\"\"Retrieves and types dictionary values using string keys.\n\n    A node that provides key-based access to dictionary values while determining their Python\n    type, enabling dynamic type handling and conditional processing in workflows.\n\n    Args:\n        dict (dict): The source dictionary to extract values from.\n            Must be a valid Python dictionary.\n            Can contain values of any type and nested structures.\n        key (str): The lookup key for value retrieval.\n            Must be a string type.\n            Case-sensitive and must match exactly.\n            If the key starts with a dot it will be used as a path as in a jq query,\n            e.g. \".key1.key2[0]\" will return the first value of array key2 in the dictionary key1.\n            Defaults to empty string.\n\n    Returns:\n        tuple[Any, str]: A tuple containing:\n            - Any: The value associated with the specified key.\n            - str: The Python type name of the retrieved value (e.g., 'str', 'int', 'dict').\n\n    Raises:\n        ValueError: When key is not a string or dict parameter is not a dictionary.\n        KeyError: When the specified key doesn't exist in the dictionary.\n\n    Notes:\n        - Supports dictionaries containing any Python type, including custom classes\n        - Type name is derived from the object's __class__.__name__\n        - Returns None for missing keys instead of raising KeyError\n        - Thread-safe for concurrent access\n        - Preserves original data without modifications\n        - Handles nested data structures (dictionaries within dictionaries, lists, etc.)\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"dict\": (\"DICT\",),\n                \"key\": (\"STRING\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (any_type, \"STRING\")\n    RETURN_NAMES = (\"value\", \"value_type\")\n    FUNCTION = \"execute\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Retrieves values from dictionaries using keys or JQ path expressions (with leading dot).\n    Returns both the value and its Python type name.\n    Supports nested data structures and complex queries.\"\"\"\n\n    def execute(self, dict: dict, key: str = \"\") -&gt; tuple[Any, str]:\n        if key.startswith(\".\"):\n            value = jq.compile(key).input(dict).first()\n            if value is None:\n                raise KeyError(f\"Key {key} not found in dictionary\")\n        else:\n            if key not in dict:\n                raise KeyError(f\"Key {key} not found in dictionary\")\n            value = dict.get(key)\n        value_type = type(value).__name__\n        return (value, value_type)\n</code></pre>"},{"location":"nodes/data/#setdictvalue","title":"SetDictValue","text":"<p>Sets a value in a dictionary using a string key.</p> <p>A node that provides key-based update of a dictionary while determining their Python type, enabling dynamic type handling and conditional processing in workflows.</p>"},{"location":"nodes/data/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required dict <code>DICT</code> required value <code>any_type</code> required key <code>STRING</code>"},{"location":"nodes/data/#returns_2","title":"Returns","text":"Name Type dict <code>DICT</code> Source code <pre><code>class SetDictValue:\n    \"\"\"Sets a value in a dictionary using a string key.\n\n    A node that provides key-based update of a dictionary while determining their Python\n    type, enabling dynamic type handling and conditional processing in workflows.\n\n    Args:\n        dict (dict): The source dictionary to set values.\n            Must be a valid Python dictionary.\n            Can contain values of any type and nested structures.\n        key (str): The lookup key for value setter.\n            Must be a string type.\n            Case-sensitive and must match exactly.\n            If the key starts with a dot it will be used as a path as in a jq query,\n            e.g. \".key1.key2[0]\" will set the first value of array key2 in the dictionary key1.\n            Defaults to empty string.\n        value (Any): The value to set.\n            Can be any type of value.\n\n    Returns:\n        tuple[dict]: A tuple containing:\n            - dict: The updated dictionary with the new value set.\n\n    Raises:\n        ValueError: When key is not a string or dict parameter is not a dictionary.\n\n    Notes:\n        - Supports dictionaries containing any Python type, including custom classes\n        - Thread-safe for concurrent access\n        - Preserves original data without modifications\n        - Handles nested data structures (dictionaries within dictionaries, lists, etc.)\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"dict\": (\"DICT\",),\n                \"value\": (any_type,),\n                \"key\": (\"STRING\", {\"default\": \"\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"DICT\",)\n    RETURN_NAMES = (\"new_dict\",)\n    FUNCTION = \"execute\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Sets or updates values in dictionaries using keys or JQ path expressions (with leading dot).\n    Supports any value type and nested data structures. Returns the modified dictionary.\"\"\"\n\n    def execute(self, dict: dict, value: Any, key: str = \"\") -&gt; tuple[dict]:\n        if key.startswith(\".\"):\n            update_query = f'{key} = \"{value}\"'\n            dict = jq.compile(update_query).input(dict).first()\n        else:\n            dict[key] = value\n        return (dict,)\n</code></pre>"},{"location":"nodes/data/#dict2json","title":"Dict2Json","text":"<p>Converts Python dictionaries to JSON strings for data interchange.</p> <p>A node that serializes Python dictionaries into JSON-formatted strings, facilitating data export and communication with external systems that require JSON format.</p>"},{"location":"nodes/data/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required dict <code>DICT</code>"},{"location":"nodes/data/#returns_3","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class Dict2Json:\n    \"\"\"Converts Python dictionaries to JSON strings for data interchange.\n\n    A node that serializes Python dictionaries into JSON-formatted strings, facilitating data\n    export and communication with external systems that require JSON format.\n\n    Args:\n        dict (dict): The Python dictionary to serialize.\n            Can contain nested dictionaries, lists, and primitive Python types.\n            All values must be JSON-serializable (dict, list, str, int, float, bool, None).\n\n    Returns:\n        tuple[str]: A single-element tuple containing:\n            - str: The JSON-formatted string representation of the input dictionary.\n                  Follows standard JSON syntax and escaping rules.\n\n    Raises:\n        TypeError: When dict contains values that cannot be serialized to JSON.\n        ValueError: When dict is not a dictionary type.\n\n    Notes:\n        - All dictionary keys are converted to strings in the output JSON\n        - Complex Python objects (datetime, custom classes) must be pre-converted to basic types\n        - Output is compact JSON without extra whitespace or formatting\n        - Handles nested structures of any depth\n        - Unicode characters are properly escaped in the output\n        - Circular references are not supported and will raise TypeError\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"dict\": (\"DICT\",),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CLASS_ID = \"dict_json\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Converts Python dictionaries to JSON-formatted strings. Handles nested structures and primitive data types.\n    Useful for data interchange with external systems that require JSON format.\"\"\"\n\n    def execute(self, dict: dict) -&gt; tuple[str]:\n        json_str = json.dumps(dict)\n        return (json_str,)\n</code></pre>"},{"location":"nodes/data/#toml2dict","title":"Toml2Dict","text":"<p>Converts TOML strings to Python dictionaries for workflow integration.</p> <p>A node that takes TOML-formatted strings and parses them into Python dictionaries, enabling seamless data integration within the workflow. Handles nested TOML structures and validates input format.</p>"},{"location":"nodes/data/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required toml_str <code>STRING</code> forceInput=True"},{"location":"nodes/data/#returns_4","title":"Returns","text":"Name Type dict <code>DICT</code> Source code <pre><code>class Toml2Dict:\n    \"\"\"Converts TOML strings to Python dictionaries for workflow integration.\n\n    A node that takes TOML-formatted strings and parses them into Python dictionaries, enabling\n    seamless data integration within the workflow. Handles nested TOML structures and validates\n    input format.\n\n    Args:\n        toml_str (str): The TOML-formatted input string to parse.\n            Must be a valid TOML string conforming to standard TOML syntax.\n            Can represent simple key-value pairs or complex nested structures.\n\n    Returns:\n        tuple[dict]: A single-element tuple containing:\n            - dict: The parsed Python dictionary representing the TOML structure.\n                   Preserves all nested objects, arrays, and primitive values.\n\n    Raises:\n        ValueError: When toml_str is not a string type.\n        toml.TomlDecodeError: When the input string contains invalid TOML syntax.\n\n    Notes:\n        - Accepts any valid TOML format including hash tables, arrays, and primitive values\n        - Empty TOML tables ('[table]') are valid inputs and return empty dictionaries\n        - Preserves all TOML data types: hash tables, arrays, strings, numbers, booleans, null\n        - Unicode characters are properly handled and preserved\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"toml_str\": (\"STRING\", {\"default\": \"\", \"forceInput\": True}),\n            },\n        }\n\n    RETURN_TYPES = (\"DICT\",)\n    FUNCTION = \"execute\"\n    CLASS_ID = \"toml_dict\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Converts TOML-formatted strings to Python dictionaries. Handles nested structures and validates input format.\n    Useful for importing configuration data into workflows.\"\"\"\n\n    def execute(self, toml_str: str = \"\") -&gt; tuple[dict]:\n        toml_dict = tomllib.loads(toml_str)\n        return (toml_dict,)\n</code></pre>"},{"location":"nodes/data/#dict2toml","title":"Dict2Toml","text":"<p>Converts Python dictionaries to TOML strings for data interchange.</p> <p>A node that serializes Python dictionaries into TOML-formatted strings, facilitating data export and communication with external systems that require TOML format.</p>"},{"location":"nodes/data/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required dict <code>DICT</code>"},{"location":"nodes/data/#returns_5","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class Dict2Toml:\n    \"\"\"Converts Python dictionaries to TOML strings for data interchange.\n\n    A node that serializes Python dictionaries into TOML-formatted strings, facilitating data\n    export and communication with external systems that require TOML format.\n\n    Args:\n        dict (dict): The Python dictionary to serialize.\n            Can contain nested dictionaries, lists, and primitive Python types.\n            All values must be JSON-serializable (dict, list, str, int, float, bool, None).\n\n    Returns:\n        tuple[str]: A single-element tuple containing:\n            - str: The TOML-formatted string representation of the input dictionary.\n\n    Raises:\n        ValueError: When dict is not a dictionary type.\n\n    Notes:\n        - All dictionary keys are converted to strings in the output TOML\n        - Complex Python objects (datetime, custom classes) must be pre-converted to basic types\n        - Output is compact TOML without extra whitespace or formatting\n        - Handles nested structures of any depth\n        - Unicode characters are properly escaped in the output\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"dict\": (\"DICT\",),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CLASS_ID = \"dict_toml\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Converts Python dictionaries to TOML-formatted strings. Handles nested structures and primitive data types.\n    Useful for data interchange with external systems that require TOML format.\"\"\"\n\n    def execute(self, dict: dict) -&gt; tuple[str]:\n        toml_str = tomli_w.dumps(dict)\n        return (toml_str,)\n</code></pre>"},{"location":"nodes/data/#getimagelistitem","title":"GetImageListItem","text":"<p>Extracts a single image from an image list by index.</p> <p>A node designed for batch image processing that allows selective access to individual images within a collection, enabling targeted processing of specific images in a sequence.</p>"},{"location":"nodes/data/#inputs_7","title":"Inputs","text":"Group Name Type Default Extras required images <code>LIST</code> required index <code>INT</code> 0"},{"location":"nodes/data/#returns_6","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class GetImageListItem:\n    \"\"\"Extracts a single image from an image list by index.\n\n    A node designed for batch image processing that allows selective access to individual images\n    within a collection, enabling targeted processing of specific images in a sequence.\n\n    Args:\n        images (list[Image]): The list of image objects to select from.\n            Must be a valid list containing compatible image objects.\n            Can be any length, but must not be empty.\n        index (int): The zero-based index of the desired image.\n            Must be a non-negative integer within the list bounds.\n            Defaults to 0 (first image).\n\n    Returns:\n        tuple[Image]: A single-element tuple containing:\n            - Image: The selected image object from the specified index position.\n\n    Raises:\n        ValueError: When index is not an integer or images is not a list.\n        IndexError: When index is outside the valid range for the image list.\n        TypeError: When images list contains invalid image objects.\n\n    Notes:\n        - Uses zero-based indexing (0 = first image)\n        - Does not support negative indices\n        - Returns a single image even from multi-image batches\n        - Preserves the original image data without modifications\n        - Thread-safe for concurrent access\n        - Memory efficient as it references rather than copies the image\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"images\": (\"LIST\",),\n                \"index\": (\"INT\", {\"default\": 0}),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Extracts a single image from a list of images by its index position.\n    Uses zero-based indexing (0 = first image).\n    Useful for selecting specific images from batches for individual processing.\"\"\"\n\n    def execute(self, images: list[torch.Tensor], index: int = 0) -&gt; tuple[torch.Tensor]:\n        image = images[index]\n        return (image,)\n</code></pre>"},{"location":"nodes/data/#getlistitem","title":"GetListItem","text":"<p>Retrieves and types items from any list by index position.</p> <p>A versatile node that provides access to list elements while also determining their Python type, enabling dynamic type handling and conditional processing in workflows.</p>"},{"location":"nodes/data/#inputs_8","title":"Inputs","text":"Group Name Type Default Extras required list <code>LIST</code> required index <code>INT</code> 0 Source code <pre><code>class GetListItem:\n    \"\"\"Retrieves and types items from any list by index position.\n\n    A versatile node that provides access to list elements while also determining their Python\n    type, enabling dynamic type handling and conditional processing in workflows.\n\n    Args:\n        list (list): The source list to extract items from.\n            Can contain elements of any type, including mixed types.\n            Must be a valid Python list, not empty.\n        index (int): The zero-based index of the desired item.\n            Must be a non-negative integer within the list bounds.\n            Defaults to 0 (first item).\n\n    Returns:\n        tuple[Any, str]: A tuple containing:\n            - Any: The retrieved item from the specified index position.\n            - str: The Python type name of the retrieved item (e.g., 'str', 'int', 'dict').\n\n    Raises:\n        ValueError: When index is not an integer or list parameter is not a list.\n        IndexError: When index is outside the valid range for the list.\n\n    Notes:\n        - Supports lists containing any Python type, including custom classes\n        - Type name is derived from the object's __class__.__name__\n        - Does not support negative indices\n        - Thread-safe for concurrent access\n        - Preserves original data without modifications\n        - Handles nested data structures (lists within lists, dictionaries, etc.)\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"list\": (\"LIST\",),\n                \"index\": (\"INT\", {\"default\": 0}),\n            },\n        }\n\n    RETURN_TYPES = (any_type, \"STRING\")\n    RETURN_NAMES = (\"item\", \"value_type\")\n    FUNCTION = \"execute\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Retrieves items from lists by index position. Returns both the item and its Python type name.\n    Uses zero-based indexing (0 = first item). Supports lists containing any data type.\"\"\"\n\n    def execute(self, list: list, index: int = 0) -&gt; tuple[Any, str]:\n        item = list[index]\n        item_type = type(item).__name__\n        return (item, item_type)\n</code></pre>"},{"location":"nodes/data/#json2dict","title":"Json2Dict","text":"<p>Converts JSON strings to Python dictionaries for workflow integration.</p> <p>A node that takes JSON-formatted strings and parses them into Python dictionaries, enabling seamless data integration within the workflow. Handles nested JSON structures and validates input format.</p>"},{"location":"nodes/data/#inputs_9","title":"Inputs","text":"Group Name Type Default Extras required json_str <code>STRING</code> { \"dict1\": {\"key1\": \"value1\", \"key2\": \"value2\"}, \"dict2\": {\"key1\": \"value3\", \"key2\": \"value4\"} } multiline=True"},{"location":"nodes/data/#returns_7","title":"Returns","text":"Name Type dict <code>DICT</code> Source code <pre><code>class Json2Dict:\n    \"\"\"Converts JSON strings to Python dictionaries for workflow integration.\n\n    A node that takes JSON-formatted strings and parses them into Python dictionaries, enabling\n    seamless data integration within the workflow. Handles nested JSON structures and validates\n    input format.\n\n    Args:\n        json_str (str): The JSON-formatted input string to parse.\n            Must be a valid JSON string conforming to standard JSON syntax.\n            Can represent simple key-value pairs or complex nested structures.\n\n    Returns:\n        tuple[dict]: A single-element tuple containing:\n            - dict: The parsed Python dictionary representing the JSON structure.\n                   Preserves all nested objects, arrays, and primitive values.\n\n    Raises:\n        ValueError: When json_str is not a string type.\n        json.JSONDecodeError: When the input string contains invalid JSON syntax.\n\n    Notes:\n        - Accepts any valid JSON format including objects, arrays, and primitive values\n        - Empty JSON objects ('{}') are valid inputs and return empty dictionaries\n        - Preserves all JSON data types: objects, arrays, strings, numbers, booleans, null\n        - Does not support JSON streaming or parsing multiple JSON objects\n        - Unicode characters are properly handled and preserved\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"json_str\": (\n                    \"STRING\",\n                    {\n                        \"default\": (\n                            '{\\n  \"dict1\": {\"key1\": \"value1\", \"key2\": \"value2\"},\\n  \"dict2\": {\"key1\": \"value3\", '\n                            '\"key2\": \"value4\"}\\n}'\n                        ),\n                        \"multiline\": True,\n                    },\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\"DICT\",)\n    FUNCTION = \"execute\"\n    CLASS_ID = \"json_dict\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Converts JSON-formatted strings to Python dictionaries. Handles nested structures and validates input format.\n    Useful for importing external data into workflows.\"\"\"\n\n    def execute(self, json_str: str = \"\") -&gt; tuple[dict]:\n        json_dict = json.loads(json_str)\n        return (json_dict,)\n</code></pre>"},{"location":"nodes/data/#yaml2dict","title":"Yaml2Dict","text":"<p>Converts YAML strings to Python dictionaries for data interchange.</p> <p>A node that takes YAML-formatted strings and parses them into Python dictionaries, enabling seamless data integration within the workflow. Handles nested YAML structures and validates input format.</p>"},{"location":"nodes/data/#inputs_10","title":"Inputs","text":"Group Name Type Default Extras required yaml_str <code>STRING</code> forceInput=True"},{"location":"nodes/data/#returns_8","title":"Returns","text":"Name Type dict <code>DICT</code> Source code <pre><code>class Yaml2Dict:\n    \"\"\"Converts YAML strings to Python dictionaries for data interchange.\n\n    A node that takes YAML-formatted strings and parses them into Python dictionaries,\n    enabling seamless data integration within the workflow. Handles nested YAML structures\n    and validates input format.\n\n    Args:\n        yaml_str (str): The YAML-formatted input string to parse.\n            Must be a valid YAML string conforming to standard YAML syntax.\n            Can represent simple key-value pairs or complex nested structures.\n\n    Returns:\n        tuple[dict]: A single-element tuple containing:\n            - dict: The parsed Python dictionary representing the YAML structure.\n\n    Raises:\n        ValueError: When yaml_str is not a string type.\n        yaml.YAMLError: When the input string contains invalid YAML syntax.\n\n    Notes:\n        - Accepts any valid YAML format including hash tables, arrays, and primitive values\n        - Preserves all YAML data types: hash tables, arrays, strings, numbers, booleans, null\n        - Unicode characters are properly handled and preserved\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\"yaml_str\": (\"STRING\", {\"default\": \"\", \"forceInput\": True})},\n        }\n\n    RETURN_TYPES = (\"DICT\",)\n    FUNCTION = \"execute\"\n    CLASS_ID = \"yaml_dict\"\n    CATEGORY = DATA_CAT\n    DESCRIPTION = \"\"\"\n    Converts YAML-formatted strings to Python dictionaries. Handles nested structures and validates input format.\n    Useful for importing configuration data into workflows.\"\"\"\n\n    def execute(self, yaml_str: str = \"\") -&gt; tuple[dict]:\n        yaml_dict = yaml.safe_load(yaml_str)\n        return (yaml_dict,)\n</code></pre>"},{"location":"nodes/dicts/","title":"Dicts Nodes","text":""},{"location":"nodes/dist/","title":"Dist Nodes","text":""},{"location":"nodes/dojo/","title":"Dojo Nodes","text":""},{"location":"nodes/embeddings/","title":"Embeddings Nodes","text":""},{"location":"nodes/evaluation/","title":"Evaluation Nodes","text":""},{"location":"nodes/evaluation/#agentevaluation","title":"AgentEvaluation","text":"<p>Evaluate a list of agents with a list of evaluators. You need to send a list of prompts and expected outputs. Optionally, the results can be reported to ClearML with a project name and task name. The node will return a list of scores and a summary of the scores.</p>"},{"location":"nodes/evaluation/#inputs","title":"Inputs","text":"Group Name Type Default Extras required agents <code>LIST</code> required evaluators <code>LIST</code> required prompts <code>LIST</code> required expected_output <code>LIST</code> required report_to_clearml <code>BOOLEAN</code> False required project_name <code>STRING</code> Agent Evaluation required task_name <code>STRING</code> Agent Evaluation Task"},{"location":"nodes/evaluation/#returns","title":"Returns","text":"Name Type list <code>LIST</code> dict <code>DICT</code> Source code <pre><code>class AgentEvaluation:\n    \"\"\"\n    Evaluate a list of agents with a list of evaluators. You need to send a list of prompts and expected outputs.\n    Optionally, the results can be reported to ClearML with a project name and task name. The node will return a list of\n    scores and a summary of the scores.\n\n    Args:\n        agents (list[Agent]): The list of agents to evaluate.\n        evaluators (list[BaseEvaluation]): The list of evaluators to use.\n        prompts (list[str]): The list of prompts to use for the agents.\n        expected_output (list[str]): The list of expected outputs.\n        report_to_clearml (bool): Whether to report the results to ClearML.\n        project_name (str): The name of the project to report to ClearML.\n        task_name (str): The name of the task to report to ClearML.\n\n    Returns:\n        tuple: Contains two elements:\n            scores (list[dict]): The list of scores.\n            scores_summary (dict): The summary of the scores, a row for each agent and a column for each evaluator with\n                the average score in the cells.\n\n    Raises:\n        ValueError: If the agents, evaluators, prompts, or expected output lists are empty.\n        ValueError: If the prompts and expected output lists have different lengths.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"agents\": (\"LIST\", {}),\n                \"evaluators\": (\"LIST\", {}),\n                \"prompts\": (\"LIST\", {}),\n                \"expected_output\": (\"LIST\", {}),\n                \"report_to_clearml\": (\"BOOLEAN\", {\"default\": False}),\n                \"project_name\": (\"STRING\", {\"default\": \"Agent Evaluation\"}),\n                \"task_name\": (\"STRING\", {\"default\": \"Agent Evaluation Task\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"LIST\", \"DICT\")\n    RETURN_NAMES = (\"scores\", \"scores_summary\")\n    FUNCTION = \"process\"\n    CATEGORY = EVALUATION_CAT\n    OUTPUT_NODE = True\n\n    def process(\n        self,\n        agents: list[Agent],\n        evaluators: list[BaseEvaluation],\n        prompts: list[str],\n        expected_output: list[str],\n        report_to_clearml: bool,\n        project_name: str,\n        task_name: str,\n    ):\n        if agents is None or len(agents) == 0:\n            raise ValueError(\"Agents list cannot be empty\")\n        if evaluators is None or len(evaluators) == 0:\n            raise ValueError(\"Evaluators list cannot be empty\")\n        if prompts is None or len(prompts) == 0:\n            raise ValueError(\"Prompts list cannot be empty\")\n        if expected_output is None or len(expected_output) == 0:\n            raise ValueError(\"Expected output list cannot be empty\")\n        if len(prompts) != len(expected_output):\n            raise ValueError(\"Prompts and expected output lists must have the same length\")\n\n        agent_evaluation = AgentEvaluationNeurochain(agents, evaluators, project_name, task_name)\n        scores = agent_evaluation.evaluate(prompts, expected_output, report_to_clearml)\n        scores_summary = agent_evaluation.scores_summary(scores)\n        return (scores, scores_summary)\n</code></pre>"},{"location":"nodes/file/","title":"File Nodes","text":""},{"location":"nodes/file/#base64fromimage","title":"Base64FromImage","text":"<p>Converts ComfyUI image tensors to base64-encoded strings.</p> <p>Transforms image tensors from ComfyUI's format into base64-encoded strings, suitable for web transmission or storage in text format.</p>"},{"location":"nodes/file/#inputs","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/file/#returns","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class Base64FromImage:\n    \"\"\"Converts ComfyUI image tensors to base64-encoded strings.\n\n    Transforms image tensors from ComfyUI's format into base64-encoded strings, suitable for web\n    transmission or storage in text format.\n\n    Args:\n        image (torch.Tensor): BWHC format tensor with values in [0,1] range.\n\n    Returns:\n        tuple[str]:\n            - base64_str: PNG-encoded image as base64 string without data URL prefix\n\n    Raises:\n        ValueError: If input is not a tensor or has invalid format\n        RuntimeError: If tensor conversion or encoding fails\n\n    Notes:\n        - Output is always PNG encoded\n        - Preserves alpha channel if present\n        - No data URL prefix in output\n        - Maintains original image quality\n        - Suitable for web APIs and storage\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\"required\": {\"image\": (\"IMAGE\",)}}\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = FILE_CAT\n    OUTPUT_NODE = True\n    DESCRIPTION = \"\"\"\n    Converts images to base64-encoded strings (PNG format).\n    Creates text representations of images suitable for web transmission, APIs,\n    or text-based storage without data URL prefix.\"\"\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[str]:\n        images = TensorImage.from_BWHC(image)\n        output = images.get_base64()\n        return (output,)\n</code></pre>"},{"location":"nodes/file/#fileloader","title":"FileLoader","text":"<p>Processes string input into ComfyUI-compatible file data.</p> <p>Converts JSON-formatted string data into file references with proper paths for ComfyUI processing. Handles both single files and multiple files separated by '&amp;&amp;'.</p>"},{"location":"nodes/file/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required value <code>STRING</code>"},{"location":"nodes/file/#returns_1","title":"Returns","text":"Name Type file <code>FILE</code> Source code <pre><code>class FileLoader:\n    \"\"\"Processes string input into ComfyUI-compatible file data.\n\n    Converts JSON-formatted string data into file references with proper paths for ComfyUI processing.\n    Handles both single files and multiple files separated by '&amp;&amp;'.\n\n    Args:\n        value (str): JSON-formatted string containing file data.\n\n    Returns:\n        tuple[list]:\n            - files: List of dictionaries with file data and updated paths\n\n    Raises:\n        ValueError: If input is not a string\n        json.JSONDecodeError: If JSON parsing fails\n        KeyError: If required file data fields are missing\n\n    Notes:\n        - Automatically prepends ComfyUI input folder path\n        - Supports multiple files via '&amp;&amp;' separator\n        - Preserves original file metadata\n        - Updates file paths for ComfyUI compatibility\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (\"STRING\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"FILE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = FILE_CAT\n    DESCRIPTION = \"\"\"\n    Converts JSON-formatted strings into ComfyUI-compatible file references.\n    Handles both single and multiple files (separated by '&amp;&amp;').\n    Automatically prepends proper input folder paths.\n    \"\"\"\n\n    def execute(self, value: str = \"\") -&gt; tuple[list]:\n        data = value.split(\"&amp;&amp;\") if \"&amp;&amp;\" in value else [value]\n        input_folder = os.path.join(BASE_COMFY_DIR, \"input\")\n        for i, _ in enumerate(data):\n            json_str = data[i]\n            data[i] = json.loads(json_str)\n            item = data[i]\n            if isinstance(item, dict):\n                name = item.get(\"name\", None)\n                if name is None:\n                    continue\n                item[\"name\"] = os.path.join(input_folder, name)\n                data[i] = item\n        return (data,)\n</code></pre>"},{"location":"nodes/file/#imagefromweb","title":"ImageFromWeb","text":"<p>Fetches and converts web images to ComfyUI-compatible tensors.</p> <p>Downloads an image from a URL and processes it into ComfyUI's expected tensor format. Handles both RGB and RGBA images with automatic mask generation for transparency.</p>"},{"location":"nodes/file/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required url <code>STRING</code> URL HERE"},{"location":"nodes/file/#returns_2","title":"Returns","text":"Name Type image <code>IMAGE</code> mask <code>MASK</code> Source code <pre><code>class ImageFromWeb:\n    \"\"\"Fetches and converts web images to ComfyUI-compatible tensors.\n\n    Downloads an image from a URL and processes it into ComfyUI's expected tensor format. Handles both RGB\n    and RGBA images with automatic mask generation for transparency.\n\n    Args:\n        url (str): Direct URL to the image file (PNG, JPG, JPEG, WebP).\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]:\n            - image: BWHC format tensor, normalized to [0,1] range\n            - mask: BWHC format tensor for transparency/alpha channel\n\n    Raises:\n        ValueError: If URL is invalid, inaccessible, or not a string\n        HTTPError: If image download fails\n        IOError: If image format is unsupported\n\n    Notes:\n        - Automatically converts images to float32 format\n        - RGB images get a mask of ones\n        - RGBA images use alpha channel as mask\n        - Supports standard web image formats\n        - Image dimensions are preserved\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\"required\": {\"url\": (\"STRING\", {\"default\": \"URL HERE\"})}}\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"execute\"\n    CATEGORY = FILE_CAT\n    DESCRIPTION = \"\"\"\n    Downloads and converts web images to ComfyUI-compatible tensors.\n    Fetches images from URLs and processes them with automatic mask generation for transparency.\n    Supports common web image formats.\n    \"\"\"\n\n    def execute(self, url: str = \"URL HERE\") -&gt; tuple[torch.Tensor, torch.Tensor]:\n        img_arr = TensorImage.from_web(url)\n        return image_array_to_tensor(img_arr)\n</code></pre>"},{"location":"nodes/file/#imagefrombase64","title":"ImageFromBase64","text":"<p>Converts base64 image strings to ComfyUI-compatible tensors.</p> <p>Processes base64-encoded image data into tensor format suitable for ComfyUI operations. Handles both RGB and RGBA images with proper mask generation.</p>"},{"location":"nodes/file/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required base64 <code>STRING</code> BASE64 HERE multiline=True"},{"location":"nodes/file/#returns_3","title":"Returns","text":"Name Type image <code>IMAGE</code> mask <code>MASK</code> Source code <pre><code>class ImageFromBase64:\n    \"\"\"Converts base64 image strings to ComfyUI-compatible tensors.\n\n    Processes base64-encoded image data into tensor format suitable for ComfyUI operations. Handles\n    both RGB and RGBA images with proper mask generation.\n\n    Args:\n        base64 (str): Raw base64-encoded image string without data URL prefix.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]:\n            - image: BWHC format tensor, normalized to [0,1] range\n            - mask: BWHC format tensor for transparency/alpha channel\n\n    Raises:\n        ValueError: If base64 string is invalid or not a string\n        IOError: If decoded image format is unsupported\n        binascii.Error: If base64 decoding fails\n\n    Notes:\n        - Converts decoded images to float32 format\n        - RGB images get a mask of ones\n        - RGBA images use alpha channel as mask\n        - Supports common image formats (PNG, JPG, JPEG)\n        - Original image dimensions are preserved\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\"required\": {\"base64\": (\"STRING\", {\"default\": \"BASE64 HERE\", \"multiline\": True})}}\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"execute\"\n    CATEGORY = FILE_CAT\n    DEPRECATED = True\n    DESCRIPTION = \"\"\"\n    Converts base64-encoded image strings to ComfyUI-compatible tensors with masks.\n    Handles both RGB and RGBA images, extracting transparency as mask.\n    Note: This node is deprecated.\n    \"\"\"\n\n    def execute(self, base64: str = \"BASE64 HERE\") -&gt; tuple[torch.Tensor, torch.Tensor]:\n        img_arr = TensorImage.from_base64(base64)\n        return image_array_to_tensor(img_arr)\n</code></pre>"},{"location":"nodes/file/#file2imagelist","title":"File2ImageList","text":"<p>Converts file references to a list of image tensors.</p> <p>Processes a list of file references, extracting and converting supported image files into ComfyUI-compatible tensor format.</p>"},{"location":"nodes/file/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required files <code>FILE</code>"},{"location":"nodes/file/#returns_4","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class File2ImageList:\n    \"\"\"Converts file references to a list of image tensors.\n\n    Processes a list of file references, extracting and converting supported image files into\n    ComfyUI-compatible tensor format.\n\n    Args:\n        files (list): List of file dictionaries with type and path information.\n\n    Returns:\n        tuple[list[torch.Tensor]]:\n            - images: List of BWHC format tensors from valid image files\n\n    Raises:\n        ValueError: If input is not a list\n        IOError: If image loading fails\n        RuntimeError: If tensor conversion fails\n\n    Notes:\n        - Supports PNG, JPG, JPEG, TIFF, BMP formats\n        - Skips non-image files\n        - Maintains original image properties\n        - Returns empty list if no valid images\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"files\": (\"FILE\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = FILE_CAT\n    CLASS_ID = \"file_image_list\"\n    OUTPUT_IS_LIST = (True,)\n    DESCRIPTION = \"\"\"\n    Converts file references to a list of image tensors. Processes multiple files,\n    extracting supported image formats (PNG, JPG, JPEG, TIFF, BMP) into ComfyUI-compatible format.\n    \"\"\"\n\n    def execute(self, files: list[dict]) -&gt; tuple[list[torch.Tensor]]:\n        images_list = []\n        for file in files:\n            mimetype = file[\"type\"]\n            extension = file[\"name\"].lower().split(\".\")[-1]\n            possible_extensions = [\"png\", \"jpg\", \"jpeg\", \"tiff\", \"tif\", \"bmp\"]\n            if mimetype.startswith(\"image\") and extension in possible_extensions:\n                images_list.append(TensorImage.from_local(file[\"name\"]).get_BWHC())\n\n        return (images_list,)\n</code></pre>"},{"location":"nodes/file/#file2list","title":"File2List","text":"<p>Converts file input to a standardized list format.</p> <p>Processes file input data into a consistent list format for further ComfyUI operations.</p>"},{"location":"nodes/file/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required files <code>FILE</code>"},{"location":"nodes/file/#returns_5","title":"Returns","text":"Name Type list <code>LIST</code> Source code <pre><code>class File2List:\n    \"\"\"Converts file input to a standardized list format.\n\n    Processes file input data into a consistent list format for further ComfyUI operations.\n\n    Args:\n        files (list): List of file dictionaries.\n\n    Returns:\n        tuple[list]:\n            - files: Processed list of file data\n\n    Raises:\n        ValueError: If input is not a list\n\n    Notes:\n        - Preserves original file metadata\n        - Maintains file order\n        - No file validation performed\n        - Suitable for further processing\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"files\": (\"FILE\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"LIST\",)\n    FUNCTION = \"execute\"\n    CLASS_ID = \"file_list\"\n    CATEGORY = FILE_CAT\n    DESCRIPTION = \"\"\"\n    Converts file input to a standardized list format.\n    Processes file data into a consistent structure while preserving metadata and original order.\n    Enables further list-based operations on file collections.\n    \"\"\"\n\n    def execute(self, files: list[dict]) -&gt; tuple[list[dict]]:\n        return (files,)\n</code></pre>"},{"location":"nodes/file/#folderloader","title":"FolderLoader","text":"<p>Processes folder paths into ComfyUI-compatible file data.</p> <p>Converts folder path information into properly formatted file references for ComfyUI processing. Supports both single and multiple folder paths.</p>"},{"location":"nodes/file/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required value <code>STRING</code>"},{"location":"nodes/file/#returns_6","title":"Returns","text":"Name Type file <code>FILE</code> Source code <pre><code>class FolderLoader:\n    \"\"\"Processes folder paths into ComfyUI-compatible file data.\n\n    Converts folder path information into properly formatted file references for ComfyUI processing.\n    Supports both single and multiple folder paths.\n\n    Args:\n        value (str): JSON-formatted string containing folder path data.\n\n    Returns:\n        tuple[list]:\n            - files: List of dictionaries with file data and updated paths\n\n    Raises:\n        ValueError: If input is not a string\n        json.JSONDecodeError: If JSON parsing fails\n        KeyError: If required folder data fields are missing\n\n    Notes:\n        - Automatically prepends ComfyUI input folder path\n        - Supports multiple folders via '&amp;&amp;' separator\n        - Maintains folder structure information\n        - Updates all paths for ComfyUI compatibility\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (\"STRING\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"FILE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = FILE_CAT\n    DESCRIPTION = \"\"\"\n    Converts folder path information into ComfyUI-compatible file references.\n    Handles both single and multiple folders (separated by '&amp;&amp;').\n    Automatically prepends proper input folder paths while maintaining folder structure.\n    \"\"\"\n\n    def execute(self, value: str = \"\") -&gt; tuple[list]:\n        data = value.split(\"&amp;&amp;\") if \"&amp;&amp;\" in value else [value]\n        input_folder = os.path.join(BASE_COMFY_DIR, \"input\")\n        for i, _ in enumerate(data):\n            json_str = data[i]\n            data[i] = json.loads(json_str)\n            item = data[i]\n            if isinstance(item, dict):\n                name = item.get(\"name\", None)\n                if name is None:\n                    continue\n                item[\"name\"] = os.path.join(input_folder, name)\n                data[i] = item\n        return (data,)\n</code></pre>"},{"location":"nodes/graph/","title":"Graph Nodes","text":""},{"location":"nodes/image/","title":"Image Nodes","text":""},{"location":"nodes/image/#imagesoftlight","title":"ImageSoftLight","text":"<p>Applies soft light blend mode between two images.</p> <p>Implements the soft light blending mode similar to photo editing software. The effect creates a subtle, soft lighting effect based on the interaction between the top and bottom layers.</p>"},{"location":"nodes/image/#inputs","title":"Inputs","text":"Group Name Type Default Extras required top <code>IMAGE</code> required bottom <code>IMAGE</code>"},{"location":"nodes/image/#returns","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class ImageSoftLight:\n    \"\"\"Applies soft light blend mode between two images.\n\n    Implements the soft light blending mode similar to photo editing software. The effect creates a\n    subtle, soft lighting effect based on the interaction between the top and bottom layers.\n\n    Args:\n        top (torch.Tensor): Top layer image in BWHC format, acts as the blend layer\n        bottom (torch.Tensor): Bottom layer image in BWHC format, acts as the base layer\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing:\n            - tensor: Blended image in BWHC format with same shape as inputs\n\n    Raises:\n        ValueError: If top is not a torch.Tensor\n        ValueError: If bottom is not a torch.Tensor\n        ValueError: If input tensors have different shapes\n\n    Notes:\n        - Both input images must have the same dimensions\n        - The blend preserves the original image dimensions and color range\n        - The effect is similar to soft light blend mode in photo editing software\n        - Processing is done on GPU if input tensors are on GPU\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"top\": (\"IMAGE\",),\n                \"bottom\": (\"IMAGE\",),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    DESCRIPTION = \"\"\"\n    Applies soft light blend mode between two images.\n    Creates subtle lighting effects based on the interaction between top and bottom layers.\n    Similar to soft light blending in photo editing software.\n    \"\"\"\n\n    def execute(self, top: torch.Tensor, bottom: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        top_tensor = TensorImage.from_BWHC(top)\n        bottom_tensor = TensorImage.from_BWHC(bottom)\n        output = image_soft_light(top_tensor, bottom_tensor).get_BWHC()\n\n        return (output,)\n</code></pre>"},{"location":"nodes/image/#imagelist2batch","title":"ImageList2Batch","text":"<p>Converts a list of individual images into a batched tensor.</p> <p>Combines multiple images into a single batched tensor, handling different input sizes through various resize modes. Supports multiple interpolation methods for optimal quality.</p>"},{"location":"nodes/image/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required images <code>IMAGE</code> required mode <code>LIST</code> required interpolation <code>LIST</code>"},{"location":"nodes/image/#returns_1","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class ImageList2Batch:\n    \"\"\"Converts a list of individual images into a batched tensor.\n\n    Combines multiple images into a single batched tensor, handling different input sizes through\n    various resize modes. Supports multiple interpolation methods for optimal quality.\n\n    Args:\n        images (list[torch.Tensor]): List of input images in BWHC format\n        mode (str): Resize mode for handling different image sizes:\n            - 'STRETCH': Stretches images to match largest dimensions\n            - 'FIT': Fits images within largest dimensions, maintaining aspect ratio\n            - 'FILL': Fills to largest dimensions, maintaining aspect ratio with cropping\n            - 'ASPECT': Preserves aspect ratio with padding\n        interpolation (str): Interpolation method for resizing:\n            - 'bilinear': Smooth interpolation suitable for most cases\n            - 'nearest': Nearest neighbor, best for pixel art\n            - 'bicubic': High-quality interpolation\n            - 'area': Best for downscaling\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing:\n            - tensor: Batched images in BWHC format\n\n    Raises:\n        ValueError: If images is not a list\n        ValueError: If mode is not a valid option\n        ValueError: If interpolation is not a valid option\n\n    Notes:\n        - All images in output batch will have same dimensions\n        - Original image qualities are preserved as much as possible\n        - Memory efficient processing for large batches\n        - GPU acceleration is automatically used when available\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                \"mode\": ([\"STRETCH\", \"FIT\", \"FILL\", \"ASPECT\"],),\n                \"interpolation\": ([\"bilinear\", \"nearest\", \"bicubic\", \"area\"],),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    INPUT_IS_LIST = True\n    CLASS_ID = \"image_list_batch\"\n    DESCRIPTION = \"\"\"\n    Combines multiple images into a single batched tensor.\n    Handles different input sizes through various resize modes (stretch, fit, fill, aspect)\n    with customizable interpolation methods. Useful for batch processing operations.\n    \"\"\"\n\n    def execute(\n        self,\n        images: list[torch.Tensor],\n        mode: str = \"STRETCH\",\n        interpolation: str = \"bilinear\",\n    ) -&gt; tuple[torch.Tensor]:\n        # Check if all images have the same shape\n        shapes = [img.shape for img in images]\n        if len(set(shapes)) == 1:\n            # All images have the same shape, no need to resize\n            return (torch.stack(images),)\n\n        # Images have different shapes, proceed with resizing\n        max_height = max(img.shape[1] for img in images)\n        max_width = max(img.shape[2] for img in images)\n\n        resized_images = []\n        for img in images:\n            tensor_img = TensorImage.from_BWHC(img)\n            resized_img = resize(\n                tensor_img,\n                max_width,\n                max_height,\n                mode=mode,\n                interpolation=interpolation,\n            )\n            resized_images.append(resized_img.get_BWHC().squeeze(0))\n\n        return (torch.stack(resized_images),)\n</code></pre>"},{"location":"nodes/image/#getimageshape","title":"GetImageShape","text":"<p>Analyzes and returns the dimensions of an input image.</p> <p>Extracts and returns detailed shape information from an input image tensor, providing both individual dimensions and a formatted string representation.</p>"},{"location":"nodes/image/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/image/#returns_2","title":"Returns","text":"Name Type int <code>INT</code> int <code>INT</code> int <code>INT</code> int <code>INT</code> string <code>STRING</code> Source code <pre><code>class GetImageShape:\n    \"\"\"Analyzes and returns the dimensions of an input image.\n\n    Extracts and returns detailed shape information from an input image tensor, providing both\n    individual dimensions and a formatted string representation.\n\n    Args:\n        image (torch.Tensor): Input image in BWHC format to analyze\n\n    Returns:\n        tuple[int, int, int, int, str]: Five-element tuple containing:\n            - int: Batch size (B dimension)\n            - int: Width in pixels\n            - int: Height in pixels\n            - int: Number of channels (typically 3 for RGB, 4 for RGBA)\n            - str: Formatted string showing complete shape (B,W,H,C)\n\n    Raises:\n        ValueError: If image is not a torch.Tensor\n        ValueError: If image does not have exactly 4 dimensions\n\n    Notes:\n        - Useful for debugging and dynamic processing\n        - Shape string provides human-readable format\n        - Can handle both RGB and RGBA images\n        - Validates correct tensor format\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            },\n        }\n\n    RETURN_TYPES = (\"INT\", \"INT\", \"INT\", \"INT\", \"STRING\")\n    RETURN_NAMES = (\"batch\", \"width\", \"height\", \"channels\", \"debug\")\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    CLASS_ID = \"get_image_size\"\n    DESCRIPTION = \"\"\"\n    Analyzes and returns the dimensions of an input image.\n    Extracts batch size, width, height, channels, and a formatted shape string.\n    Useful for debugging and dynamic image processing workflows.\n    \"\"\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[int, int, int, int, str]:\n        return (\n            image.shape[0],\n            image.shape[2],\n            image.shape[1],\n            image.shape[3],\n            str(image.shape),\n        )\n</code></pre>"},{"location":"nodes/image/#imagesubtract","title":"ImageSubtract","text":"<p>Computes the absolute difference between two images.</p> <p>Performs pixel-wise subtraction between two images and takes the absolute value of the result, useful for comparing images or creating difference maps.</p>"},{"location":"nodes/image/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required image_0 <code>IMAGE</code> required image_1 <code>IMAGE</code>"},{"location":"nodes/image/#returns_3","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class ImageSubtract:\n    \"\"\"Computes the absolute difference between two images.\n\n    Performs pixel-wise subtraction between two images and takes the absolute value of the result,\n    useful for comparing images or creating difference maps.\n\n    Args:\n        image_0 (torch.Tensor): First image in BWHC format\n        image_1 (torch.Tensor): Second image in BWHC format to subtract from first image\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing:\n            - tensor: Difference image in BWHC format with same shape as inputs\n\n    Raises:\n        ValueError: If image_0 is not a torch.Tensor\n        ValueError: If image_1 is not a torch.Tensor\n        ValueError: If input tensors have different shapes\n\n    Notes:\n        - Both input images must have the same dimensions\n        - Output values represent absolute differences between corresponding pixels\n        - Useful for change detection or image comparison\n        - Result is always positive due to absolute value operation\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image_0\": (\"IMAGE\",),\n                \"image_1\": (\"IMAGE\",),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    DESCRIPTION = \"\"\"\n    Computes the absolute difference between two images.\n    Performs pixel-wise subtraction and takes the absolute value of the result.\n    Useful for comparing images, detecting changes, or creating difference maps.\n    \"\"\"\n\n    def execute(self, image_0: torch.Tensor, image_1: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        image_0_tensor = TensorImage.from_BWHC(image_0)\n        image_1_tensor = TensorImage.from_BWHC(image_1)\n        image_tensor = torch.abs(image_0_tensor - image_1_tensor)\n        output = TensorImage(image_tensor).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/image/#imageaverage","title":"ImageAverage","text":"<p>Calculates the average color of an input image.</p> <p>Computes the mean color values across all pixels in the image, resulting in a uniform color image representing the average color of the input.</p>"},{"location":"nodes/image/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> optional focus_mask <code>MASK</code>"},{"location":"nodes/image/#returns_4","title":"Returns","text":"Name Type image <code>IMAGE</code> string <code>STRING</code> Source code <pre><code>class ImageAverage:\n    \"\"\"Calculates the average color of an input image.\n\n    Computes the mean color values across all pixels in the image, resulting in a uniform color\n    image representing the average color of the input.\n\n    Args:\n        image (torch.Tensor): Input image in BWHC format to calculate average from\n        focus_mask (torch.Tensor, optional): Mask to focus calculation on specific areas\n\n    Returns:\n        tuple[torch.Tensor, str]: Two-element tuple containing:\n            - tensor: Uniform color image in BWHC format with same shape as input\n            - str: Hexadecimal color code of the average color\n\n    Raises:\n        ValueError: If image is not a torch.Tensor\n\n    Notes:\n        - Output maintains the same dimensions as input but with uniform color\n        - Calculation is performed per color channel\n        - Useful for color analysis or creating color-matched solid backgrounds\n        - Preserves the original batch size\n        - When focus_mask is provided, only calculates average from masked areas\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            },\n            \"optional\": {\n                \"focus_mask\": (\"MASK\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"STRING\")\n    RETURN_NAMES = (\"color\", \"hex_color\")\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    DESCRIPTION = \"\"\"\n    Calculates the average color of an input image, creating a uniform color image.\n    Optionally uses a mask to focus on specific areas. Returns both the color image\n    and hexadecimal color code.\n    \"\"\"\n\n    def execute(self, image: torch.Tensor, focus_mask: Optional[torch.Tensor] = None) -&gt; tuple[torch.Tensor, str]:\n        step = TensorImage.from_BWHC(image)\n        if focus_mask is not None:\n            mask = TensorImage.from_BWHC(focus_mask)\n            masked_image = step * mask\n            step = masked_image.sum(dim=[2, 3], keepdim=True) / (mask.sum(dim=[2, 3], keepdim=True) + 1e-8)\n        else:\n            step = step.mean(dim=[2, 3], keepdim=True)\n        step = step.expand(-1, -1, image.shape[1], image.shape[2])\n        output = TensorImage(step).get_BWHC()\n\n        avg_color = step[0, :, 0, 0] * 255\n        hex_color = \"#{:02x}{:02x}{:02x}\".format(\n            int(avg_color[0].item()), int(avg_color[1].item()), int(avg_color[2].item())\n        )\n\n        return (output, hex_color)\n</code></pre>"},{"location":"nodes/image/#imagegaussianblur","title":"ImageGaussianBlur","text":"<p>Applies Gaussian blur filter to an input image.</p> <p>This node performs Gaussian blur using a configurable kernel size and sigma value. Multiple passes can be applied for stronger blur effects. The blur is applied uniformly across all color channels.</p>"},{"location":"nodes/image/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required radius <code>INT</code> 13 required sigma <code>FLOAT</code> 10.5 required iterations <code>INT</code> 1"},{"location":"nodes/image/#returns_5","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class ImageGaussianBlur:\n    \"\"\"Applies Gaussian blur filter to an input image.\n\n    This node performs Gaussian blur using a configurable kernel size and sigma value. Multiple passes\n    can be applied for stronger blur effects. The blur is applied uniformly across all color channels.\n\n    Args:\n        image (torch.Tensor): Input image tensor in BWHC format\n        radius (int): Blur kernel radius in pixels (kernel size = 2 * radius + 1)\n        sigma (float): Standard deviation for Gaussian kernel, controls blur strength\n        iterations (int): Number of times to apply the blur filter sequentially\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing:\n            - tensor: Blurred image in BWHC format with same shape as input\n\n    Raises:\n        ValueError: If image is not a torch.Tensor\n        ValueError: If radius is not an integer\n        ValueError: If sigma is not a float\n        ValueError: If iterations is not an integer\n\n    Notes:\n        - Larger radius and sigma values produce stronger blur effects\n        - Multiple iterations can create smoother results but increase processing time\n        - Input image dimensions and batch size are preserved in output\n        - Processing is done on GPU if input tensor is on GPU\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"radius\": (\"INT\", {\"default\": 13}),\n                \"sigma\": (\"FLOAT\", {\"default\": 10.5}),\n                \"iterations\": (\"INT\", {\"default\": 1}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    DESCRIPTION = \"\"\"\n    Applies Gaussian blur filter to images with controllable strength.\n    Adjust blur intensity through radius, sigma, and iteration count.\n    Creates smooth, natural-looking blur effects for softening details or creating depth.\n    \"\"\"\n\n    def execute(\n        self,\n        image: torch.Tensor,\n        radius: int = 13,\n        sigma: float = 10.5,\n        iterations: int = 1,\n    ) -&gt; tuple[torch.Tensor]:\n        tensor_image = TensorImage.from_BWHC(image)\n        output = gaussian_blur2d(tensor_image, radius, sigma, iterations).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/image/#imagebatch2list","title":"ImageBatch2List","text":"<p>Splits a batched tensor of images into individual images.</p> <p>Converts a batch of images stored in a single tensor into a list of separate image tensors, useful for processing images individually after batch operations.</p>"},{"location":"nodes/image/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/image/#returns_6","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class ImageBatch2List:\n    \"\"\"Splits a batched tensor of images into individual images.\n\n    Converts a batch of images stored in a single tensor into a list of separate image tensors,\n    useful for processing images individually after batch operations.\n\n    Args:\n        image (torch.Tensor): Batched input images in BWHC format\n\n    Returns:\n        tuple[list[torch.Tensor]]: Single-element tuple containing:\n            - list: Individual images, each in BWHC format with batch size 1\n\n    Raises:\n        ValueError: If image is not a torch.Tensor\n\n    Notes:\n        - Each output image maintains original dimensions and channels\n        - Output images have batch dimension of 1\n        - Useful for post-processing individual images after batch operations\n        - Memory efficient as it uses views when possible\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\"required\": {\"image\": (\"IMAGE\",)}}\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    CLASS_ID = \"image_batch_list\"\n    OUTPUT_IS_LIST = (True,)\n    DESCRIPTION = \"\"\"\n    Splits a batched tensor of images into a list of individual images.\n    Converts a single tensor containing multiple images into separate tensors, each with batch size 1.\n    Useful for processing images individually after batch operations.\n    \"\"\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[list[torch.Tensor]]:\n        image_list = [img.unsqueeze(0) for img in image]\n        return (image_list,)\n</code></pre>"},{"location":"nodes/image/#imageunsharpmask","title":"ImageUnsharpMask","text":"<p>Enhances image sharpness using unsharp mask technique.</p> <p>This node applies an unsharp mask filter to enhance edge details in the image. It works by subtracting a blurred version of the image from the original, creating a sharpening effect.</p>"},{"location":"nodes/image/#inputs_7","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required radius <code>INT</code> 3 required sigma <code>FLOAT</code> 1.5 required iterations <code>INT</code> 1"},{"location":"nodes/image/#returns_7","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class ImageUnsharpMask:\n    \"\"\"Enhances image sharpness using unsharp mask technique.\n\n    This node applies an unsharp mask filter to enhance edge details in the image. It works by\n    subtracting a blurred version of the image from the original, creating a sharpening effect.\n\n    Args:\n        image (torch.Tensor): Input image in BWHC format\n        radius (int): Size of the blur kernel used in the unsharp mask\n        sigma (float): Strength of the blur in the unsharp mask calculation\n        iterations (int): Number of times to apply the sharpening effect\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing:\n            - tensor: Sharpened image in BWHC format with same shape as input\n\n    Raises:\n        ValueError: If image is not a torch.Tensor\n        ValueError: If radius is not an integer\n        ValueError: If sigma is not a float\n        ValueError: If iterations is not an integer\n\n    Notes:\n        - Higher sigma values create stronger sharpening effects\n        - Multiple iterations can create more pronounced sharpening but may introduce artifacts\n        - The process preserves the original image dimensions and color range\n        - Works on all color channels independently\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"radius\": (\"INT\", {\"default\": 3}),\n                \"sigma\": (\"FLOAT\", {\"default\": 1.5}),\n                \"iterations\": (\"INT\", {\"default\": 1}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    DESCRIPTION = \"\"\"\n    Enhances image sharpness using unsharp mask technique.\n    Works by subtracting a blurred version from the original image to enhance edge details.\n    Control strength with radius, sigma, and iteration count.\n    \"\"\"\n\n    def execute(\n        self,\n        image: torch.Tensor,\n        radius: int = 3,\n        sigma: float = 1.5,\n        iterations: int = 1,\n    ) -&gt; tuple[torch.Tensor]:\n        tensor_image = TensorImage.from_BWHC(image)\n        output = unsharp_mask(tensor_image, radius, sigma, iterations).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/image/#imagetranspose","title":"ImageTranspose","text":"<p>Transforms and composites an overlay image onto a base image.</p> <p>Provides comprehensive image composition capabilities including resizing, positioning, rotation, and edge feathering of an overlay image onto a base image.</p>"},{"location":"nodes/image/#inputs_8","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required image_overlay <code>IMAGE</code> required width <code>INT</code> max=48000, step=1 required height <code>INT</code> max=48000, step=1 required x <code>INT</code> 0 min=0, max=48000, step=1 required y <code>INT</code> 0 min=0, max=48000, step=1 required rotation <code>INT</code> 0 max=360, step=1 required feathering <code>INT</code> 0 min=0, max=100, step=1"},{"location":"nodes/image/#returns_8","title":"Returns","text":"Name Type image <code>IMAGE</code> image <code>IMAGE</code> Source code <pre><code>class ImageTranspose:\n    \"\"\"Transforms and composites an overlay image onto a base image.\n\n    Provides comprehensive image composition capabilities including resizing, positioning, rotation,\n    and edge feathering of an overlay image onto a base image.\n\n    Args:\n        image (torch.Tensor): Base image in BWHC format\n        image_overlay (torch.Tensor): Overlay image in BWHC format\n        width (int): Target width for overlay (-1 for original size)\n        height (int): Target height for overlay (-1 for original size)\n        X (int): Horizontal offset in pixels from left edge\n        Y (int): Vertical offset in pixels from top edge\n        rotation (int): Rotation angle in degrees (-360 to 360)\n        feathering (int): Edge feathering radius in pixels (0-100)\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: Two-element tuple containing:\n            - tensor: Composited image in RGB format\n            - tensor: Composited image in RGBA format with transparency\n\n    Raises:\n        ValueError: If any input parameters are not of correct type\n        ValueError: If rotation is outside valid range\n        ValueError: If feathering is outside valid range\n\n    Notes:\n        - Supports both RGB and RGBA overlay images\n        - Automatically handles padding and cropping\n        - Feathering creates smooth edges around the overlay\n        - All transformations preserve aspect ratio when specified\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"image_overlay\": (\"IMAGE\",),\n                \"width\": (\"INT\", {\"default\": -1, \"min\": -1, \"max\": 48000, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": -1, \"min\": -1, \"max\": 48000, \"step\": 1}),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 48000, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 48000, \"step\": 1}),\n                \"rotation\": (\"INT\", {\"default\": 0, \"min\": -360, \"max\": 360, \"step\": 1}),\n                \"feathering\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 100, \"step\": 1}),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"IMAGE\",\n        \"IMAGE\",\n    )\n    RETURN_NAMES = (\n        \"rgb\",\n        \"rgba\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    DESCRIPTION = \"\"\"\n    Transforms and composites an overlay image onto a base image.\n    Provides comprehensive composition capabilities including resizing, positioning, rotation, and edge feathering.\n    Returns both RGB and RGBA versions.\n    \"\"\"\n\n    def execute(\n        self,\n        image: torch.Tensor,\n        image_overlay: torch.Tensor,\n        width: int = -1,\n        height: int = -1,\n        x: int = 0,\n        y: int = 0,\n        rotation: int = 0,\n        feathering: int = 0,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        base_image = TensorImage.from_BWHC(image)\n        overlay_image = TensorImage.from_BWHC(image_overlay)\n\n        if width == -1:\n            width = overlay_image.shape[3]\n        if height == -1:\n            height = overlay_image.shape[2]\n\n        device = base_image.device\n        overlay_image = overlay_image.to(device)\n\n        # Resize overlay image\n        overlay_image = transform.resize(overlay_image, (height, width))\n\n        if rotation != 0:\n            angle = torch.tensor(rotation, dtype=torch.float32, device=device)\n            center = torch.tensor([width / 2, height / 2], dtype=torch.float32, device=device)\n            overlay_image = transform.rotate(overlay_image, angle, center=center)\n\n        # Create mask (handle both RGB and RGBA cases)\n        if overlay_image.shape[1] == 4:\n            mask = overlay_image[:, 3:4, :, :]\n        else:\n            mask = torch.ones((1, 1, height, width), device=device)\n\n        # Pad overlay image and mask\n        pad_left = x\n        pad_top = y\n        pad_right = max(0, base_image.shape[3] - overlay_image.shape[3] - x)\n        pad_bottom = max(0, base_image.shape[2] - overlay_image.shape[2] - y)\n\n        overlay_image = torch.nn.functional.pad(overlay_image, (pad_left, pad_right, pad_top, pad_bottom))\n        mask = torch.nn.functional.pad(mask, (pad_left, pad_right, pad_top, pad_bottom))\n\n        # Resize to match base image\n        overlay_image = transform.resize(overlay_image, base_image.shape[2:])\n        mask = transform.resize(mask, base_image.shape[2:])\n\n        if feathering &gt; 0:\n            kernel_size = 2 * feathering + 1\n            feather_kernel = torch.ones((1, 1, kernel_size, kernel_size), device=device) / (kernel_size**2)\n            mask = torch.nn.functional.conv2d(mask, feather_kernel, padding=feathering)\n\n        # Blend images\n        result = base_image * (1 - mask) + overlay_image[:, :3, :, :] * mask\n\n        result = TensorImage(result).get_BWHC()\n\n        rgb = result\n        rgba = torch.cat([rgb, mask.permute(0, 2, 3, 1)], dim=3)\n\n        return (rgb, rgba)\n</code></pre>"},{"location":"nodes/image/#imagebasecolor","title":"ImageBaseColor","text":"<p>Creates a solid color image with specified dimensions.</p> <p>This node generates a uniform color image using a hex color code. The output is a tensor in BWHC format (Batch, Width, Height, Channels) with the specified dimensions.</p>"},{"location":"nodes/image/#inputs_9","title":"Inputs","text":"Group Name Type Default Extras required hex_color <code>STRING</code> #FFFFFF required width <code>INT</code> 1024 required height <code>INT</code> 1024"},{"location":"nodes/image/#returns_9","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class ImageBaseColor:\n    \"\"\"Creates a solid color image with specified dimensions.\n\n    This node generates a uniform color image using a hex color code. The output is a tensor in BWHC\n    format (Batch, Width, Height, Channels) with the specified dimensions.\n\n    Args:\n        hex_color (str): Hex color code in format \"#RRGGBB\" (e.g., \"#FFFFFF\" for white)\n        width (int): Width of the output image in pixels\n        height (int): Height of the output image in pixels\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing:\n            - tensor: Image in BWHC format with shape (1, height, width, 3)\n\n    Raises:\n        ValueError: If width or height are not integers\n        ValueError: If hex_color is not a string\n        ValueError: If hex_color is not in valid \"#RRGGBB\" format\n\n    Notes:\n        - The output tensor values are normalized to range [0, 1]\n        - Alpha channel is not supported\n        - The batch dimension is always 1\n        - RGB values are extracted from hex color and converted to float32\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"hex_color\": (\"STRING\", {\"default\": \"#FFFFFF\"}),\n                \"width\": (\"INT\", {\"default\": 1024}),\n                \"height\": (\"INT\", {\"default\": 1024}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_CAT\n    CLASS_ID = \"image_base_color\"\n    DESCRIPTION = \"\"\"\n    Creates a solid color image with specified dimensions.\n    Generates a uniform color image using a hex color code (#RRGGBB format).\n    Useful for backgrounds, color testing, or as base layers for compositing.\n    \"\"\"\n\n    def execute(self, hex_color: str = \"#FFFFFF\", width: int = 1024, height: int = 1024) -&gt; tuple[torch.Tensor]:\n        hex_color = hex_color.lstrip(\"#\")\n        r, g, b = tuple(int(hex_color[i : i + 2], 16) for i in (0, 2, 4))\n\n        # Create a tensor with the specified color\n        color_tensor = torch.tensor([r, g, b], dtype=torch.float32) / 255.0\n\n        # Reshape to (3, 1, 1) and expand to (3, H, W)\n        color_tensor = color_tensor.view(3, 1, 1).expand(3, height, width)\n\n        # Repeat for the batch size\n        batch_tensor = color_tensor.unsqueeze(0).expand(1, -1, -1, -1)\n\n        output = TensorImage(batch_tensor).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/image_processing/","title":"Image Processing Nodes","text":""},{"location":"nodes/image_processing/#rescale","title":"Rescale","text":"<p>Rescales images and masks by a specified factor while preserving aspect ratio.</p> <p>Provides flexible rescaling of images and masks with support for various interpolation methods and optional antialiasing. Useful for uniform scaling operations where maintaining aspect ratio is important.</p>"},{"location":"nodes/image_processing/#inputs","title":"Inputs","text":"Group Name Type Default Extras optional image <code>IMAGE</code> None optional mask <code>MASK</code> None optional factor <code>FLOAT</code> 2.0 min=0.01, max=100.0, step=0.01 optional interpolation <code>LIST</code> optional antialias <code>BOOLEAN</code> True"},{"location":"nodes/image_processing/#returns","title":"Returns","text":"Name Type image <code>IMAGE</code> mask <code>MASK</code> Source code <pre><code>class Rescale:\n    \"\"\"Rescales images and masks by a specified factor while preserving aspect ratio.\n\n    Provides flexible rescaling of images and masks with support for various interpolation\n    methods and optional antialiasing. Useful for uniform scaling operations where\n    maintaining aspect ratio is important.\n\n    Args:\n        image (torch.Tensor, optional): Input image in BWHC format with values in range [0, 1]\n        mask (torch.Tensor, optional): Input mask in BWHC format with values in range [0, 1]\n        factor (float): Scale multiplier (0.01-100.0)\n        interpolation (str): Resampling method to use:\n            - \"nearest\": Nearest neighbor (sharp, blocky)\n            - \"nearest-exact\": Nearest neighbor without rounding\n            - \"bilinear\": Linear interpolation (smooth)\n            - \"bicubic\": Cubic interpolation (smoother)\n            - \"box\": Box sampling (good for downscaling)\n            - \"hamming\": Hamming windowed sampling\n            - \"lanczos\": Lanczos resampling (sharp, fewer artifacts)\n        antialias (bool): Whether to apply antialiasing when downscaling\n\n    Returns:\n        tuple:\n            - image (torch.Tensor): Rescaled image in BWHC format\n            - mask (torch.Tensor): Rescaled mask in BWHC format\n\n    Raises:\n        ValueError: If neither image nor mask is provided\n        ValueError: If invalid interpolation method specified\n        RuntimeError: If input tensors have invalid dimensions\n\n    Notes:\n        - At least one of image or mask must be provided\n        - Output maintains the same number of channels as input\n        - Antialiasing is recommended when downscaling to prevent artifacts\n        - All interpolation methods preserve the value range [0, 1]\n        - Memory usage scales quadratically with factor\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {},\n            \"optional\": {\n                \"image\": (\"IMAGE\", {\"default\": None}),\n                \"mask\": (\"MASK\", {\"default\": None}),\n                \"factor\": (\n                    \"FLOAT\",\n                    {\"default\": 2.0, \"min\": 0.01, \"max\": 100.0, \"step\": 0.01},\n                ),\n                \"interpolation\": (\n                    [\n                        \"nearest\",\n                        \"nearest-exact\",\n                        \"bilinear\",\n                        \"bicubic\",\n                        \"box\",\n                        \"hamming\",\n                        \"lanczos\",\n                    ],\n                ),\n                \"antialias\": (\"BOOLEAN\", {\"default\": True}),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"IMAGE\",\n        \"MASK\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_PROCESSING_CAT\n    DESCRIPTION = \"\"\"\n    Rescales images and masks by a specified factor while preserving aspect ratio.\n    Supports various interpolation methods and optional antialiasing.\n    Useful for uniform scaling operations.\n    \"\"\"\n\n    def execute(\n        self,\n        image: Optional[torch.Tensor] = None,\n        mask: Optional[torch.Tensor] = None,\n        factor: float = 2.0,\n        interpolation: str = \"nearest\",\n        antialias: bool = True,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        if not isinstance(image, torch.Tensor) and not isinstance(mask, torch.Tensor):\n            raise ValueError(\"Either image or mask must be provided\")\n        input_image = (\n            TensorImage.from_BWHC(image) if isinstance(image, torch.Tensor) else TensorImage(torch.zeros((1, 3, 1, 1)))\n        )\n        input_mask = (\n            TensorImage.from_BWHC(mask) if isinstance(mask, torch.Tensor) else TensorImage(torch.zeros((1, 1, 1, 1)))\n        )\n        output_image = rescale(\n            input_image,\n            factor,\n            interpolation,\n            antialias,\n        ).get_BWHC()\n        output_mask = rescale(\n            input_mask,\n            factor,\n            interpolation,\n            antialias,\n        ).get_BWHC()\n\n        return (\n            output_image,\n            output_mask,\n        )\n</code></pre>"},{"location":"nodes/image_processing/#cutout","title":"Cutout","text":"<p>Creates masked cutouts from images with both RGB and RGBA outputs.</p> <p>Extracts portions of an image based on a mask, providing both RGB and RGBA versions of the result. Useful for isolating subjects or creating transparent cutouts for compositing.</p>"},{"location":"nodes/image_processing/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required mask <code>MASK</code>"},{"location":"nodes/image_processing/#returns_1","title":"Returns","text":"Name Type image <code>IMAGE</code> image <code>IMAGE</code> Source code <pre><code>class Cutout:\n    \"\"\"Creates masked cutouts from images with both RGB and RGBA outputs.\n\n    Extracts portions of an image based on a mask, providing both RGB and RGBA\n    versions of the result. Useful for isolating subjects or creating transparent\n    cutouts for compositing.\n\n    Args:\n        image (torch.Tensor): Input image in BWHC format with values in range [0, 1]\n        mask (torch.Tensor): Binary or continuous mask in BWHC format with values in range [0, 1]\n\n    Returns:\n        tuple:\n            - rgb (torch.Tensor): Masked image in RGB format (BWHC)\n            - rgba (torch.Tensor): Masked image in RGBA format (BWHC)\n\n    Raises:\n        ValueError: If either image or mask is not provided\n        ValueError: If input tensors have mismatched dimensions\n        RuntimeError: If input tensors have invalid dimensions\n\n    Notes:\n        - Mask values determine transparency in RGBA output\n        - RGB output has masked areas filled with black\n        - RGBA output preserves partial mask values as alpha\n        - Input image must be 3 channels (RGB)\n        - Input mask must be 1 channel\n        - Output maintains original image resolution\n        - All non-zero mask values are considered for cutout\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"mask\": (\"MASK\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"IMAGE\")\n    RETURN_NAMES = (\"rgb\", \"rgba\")\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_PROCESSING_CAT\n    DESCRIPTION = \"\"\"\n    Creates masked cutouts from images with both RGB and RGBA outputs.\n    Extracts portions of an image based on a mask,\n    useful for isolating subjects or creating transparent cutouts for compositing.\n    \"\"\"\n\n    def execute(\n        self,\n        image: Optional[torch.Tensor],\n        mask: Optional[torch.Tensor],\n    ):\n        if not isinstance(image, torch.Tensor) or not isinstance(mask, torch.Tensor):\n            raise ValueError(\"Either image or mask must be provided\")\n\n        tensor_image = TensorImage.from_BWHC(image)\n        tensor_mask = TensorImage.from_BWHC(mask, image.device)\n\n        image_rgb, image_rgba = cutout(tensor_image, tensor_mask)\n\n        out_image_rgb = TensorImage(image_rgb).get_BWHC()\n        out_image_rgba = TensorImage(image_rgba).get_BWHC()\n\n        return (\n            out_image_rgb,\n            out_image_rgba,\n        )\n</code></pre>"},{"location":"nodes/image_processing/#autocrop","title":"AutoCrop","text":"<p>Automatically crops an image based on a mask content.</p> <p>This node detects non-zero regions in a mask and crops both the image and mask to those regions, with optional padding. Useful for removing empty space around subjects or focusing on specific masked areas.</p>"},{"location":"nodes/image_processing/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required mask <code>MASK</code> required mask_threshold <code>FLOAT</code> 0.1 min=0.0, max=1.0, step=0.01 required left_padding <code>INT</code> 0 required right_padding <code>INT</code> 0 required top_padding <code>INT</code> 0 required bottom_padding <code>INT</code> 0"},{"location":"nodes/image_processing/#returns_2","title":"Returns","text":"Name Type image <code>IMAGE</code> mask <code>MASK</code> int <code>INT</code> int <code>INT</code> int <code>INT</code> int <code>INT</code> Source code <pre><code>class AutoCrop:\n    \"\"\"Automatically crops an image based on a mask content.\n\n    This node detects non-zero regions in a mask and crops both the image and mask\n    to those regions, with optional padding. Useful for removing empty space around\n    subjects or focusing on specific masked areas.\n\n    Args:\n        image (torch.Tensor): Input image tensor in BWHC format with values in range [0, 1]\n        mask (torch.Tensor): Input mask tensor in BWHC format with values in range [0, 1]\n        mask_threshold (float): Minimum mask value to consider as content (0.0-1.0)\n        left_padding (int): Additional pixels to include on the left side\n        right_padding (int): Additional pixels to include on the right side\n        top_padding (int): Additional pixels to include on the top\n        bottom_padding (int): Additional pixels to include on the bottom\n\n    Returns:\n        tuple:\n            - cropped_image (torch.Tensor): Cropped image in BWHC format\n            - cropped_mask (torch.Tensor): Cropped mask in BWHC format\n            - x (int): X-coordinate of crop start in original image\n            - y (int): Y-coordinate of crop start in original image\n            - width (int): Width of cropped region\n            - height (int): Height of cropped region\n\n    Raises:\n        ValueError: If mask and image dimensions don't match\n        RuntimeError: If no content is found in mask above threshold\n\n    Notes:\n        - Input tensors should be in BWHC format (Batch, Width, Height, Channels)\n        - Mask should be single-channel\n        - All padding values must be non-negative\n        - If mask is empty above threshold, may return minimal crop\n        - Coordinates are returned relative to original image\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"mask\": (\"MASK\",),\n                \"mask_threshold\": (\n                    \"FLOAT\",\n                    {\"default\": 0.1, \"min\": 0.00, \"max\": 1.00, \"step\": 0.01},\n                ),\n                \"left_padding\": (\"INT\", {\"default\": 0}),\n                \"right_padding\": (\"INT\", {\"default\": 0}),\n                \"top_padding\": (\"INT\", {\"default\": 0}),\n                \"bottom_padding\": (\"INT\", {\"default\": 0}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\", \"INT\", \"INT\", \"INT\", \"INT\")\n    RETURN_NAMES = (\"cropped_image\", \"cropped_mask\", \"x\", \"y\", \"width\", \"height\")\n\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_PROCESSING_CAT\n    DESCRIPTION = \"\"\"\n    Automatically crops images based on mask content.\n    Detects non-zero regions in a mask and crops both image and mask to those regions with optional padding.\n    Returns crop coordinates for further processing.\n    \"\"\"\n\n    def execute(\n        self,\n        image: torch.Tensor,\n        mask: torch.Tensor,\n        mask_threshold: float = 0.1,\n        left_padding: int = 0,\n        right_padding: int = 0,\n        top_padding: int = 0,\n        bottom_padding: int = 0,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, int, int, int, int]:\n        img_tensor = TensorImage.from_BWHC(image)\n        mask_tensor = TensorImage.from_BWHC(mask)\n        if img_tensor.shape[1] != 3:\n            img_tensor = rgba_to_rgb(img_tensor)\n\n        padding = (\n            left_padding,\n            right_padding,\n            top_padding,\n            bottom_padding,\n        )\n        img_result, mask_result, min_x, min_y, width, height = auto_crop(\n            img_tensor, mask_tensor, mask_threshold=mask_threshold, padding=padding\n        )\n        output_img = TensorImage(img_result).get_BWHC()\n        output_mask = TensorImage(mask_result).get_BWHC()\n\n        return (output_img, output_mask, min_x, min_y, width, height)\n</code></pre>"},{"location":"nodes/image_processing/#resize","title":"Resize","text":"<p>Resizes images and masks to specific dimensions with multiple sizing modes.</p> <p>A versatile resizing node that supports multiple modes for handling aspect ratio and provides fine control over interpolation methods. Suitable for preparing images for specific size requirements while maintaining quality.</p>"},{"location":"nodes/image_processing/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras optional image <code>IMAGE</code> None optional mask <code>MASK</code> None optional width <code>INT</code> 1024 min=32, step=2, max=40960 optional height <code>INT</code> 1024 min=32, step=2, max=40960 optional mode <code>LIST</code> optional interpolation <code>LIST</code> optional antialias <code>BOOLEAN</code> True optional multiple_of <code>INT</code> 1 min=1, step=1, max=1024"},{"location":"nodes/image_processing/#returns_3","title":"Returns","text":"Name Type image <code>IMAGE</code> mask <code>MASK</code> Source code <pre><code>class Resize:\n    \"\"\"Resizes images and masks to specific dimensions with multiple sizing modes.\n\n    A versatile resizing node that supports multiple modes for handling aspect ratio\n    and provides fine control over interpolation methods. Suitable for preparing\n    images for specific size requirements while maintaining quality.\n\n    Args:\n        image (torch.Tensor, optional): Input image in BWHC format with values in range [0, 1]\n        mask (torch.Tensor, optional): Input mask in BWHC format with values in range [0, 1]\n        width (int): Target width in pixels (32-40960)\n        height (int): Target height in pixels (32-40960)\n        mode (str): How to handle aspect ratio:\n            - \"STRETCH\": Force to exact dimensions, may distort\n            - \"FIT\": Fit within dimensions, may be smaller\n            - \"FILL\": Fill dimensions, may crop\n            - \"ASPECT\": Preserve aspect ratio, fit longest side\n        interpolation (str): Resampling method:\n            - \"bilinear\": Linear interpolation (smooth)\n            - \"nearest\": Nearest neighbor (sharp)\n            - \"bicubic\": Cubic interpolation (smoother)\n            - \"area\": Area averaging (good for downscaling)\n        antialias (bool): Whether to apply antialiasing when downscaling\n        multiple_of (int, optional): Ensure output dimensions are multiples of this value.\n            If provided, final dimensions will be adjusted to nearest multiple.\n\n    Returns:\n        tuple:\n            - image (torch.Tensor): Resized image in BWHC format\n            - mask (torch.Tensor): Resized mask in BWHC format\n\n    Raises:\n        ValueError: If neither image nor mask is provided\n        ValueError: If dimensions are out of valid range\n        ValueError: If invalid mode or interpolation method\n        RuntimeError: If input tensors have invalid dimensions\n\n    Notes:\n        - At least one of image or mask must be provided\n        - Output maintains the same number of channels as input\n        - STRETCH mode may distort image proportions\n        - FIT mode ensures no cropping but may not fill target size\n        - FILL mode ensures target size but may crop content\n        - ASPECT mode preserves proportions using longest edge\n        - Antialiasing recommended when downscaling\n        - When multiple_of is set, final dimensions will be adjusted to nearest multiple\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {},\n            \"optional\": {\n                \"image\": (\"IMAGE\", {\"default\": None}),\n                \"mask\": (\"MASK\", {\"default\": None}),\n                \"width\": (\"INT\", {\"default\": 1024, \"min\": 32, \"step\": 2, \"max\": 40960}),\n                \"height\": (\n                    \"INT\",\n                    {\"default\": 1024, \"min\": 32, \"step\": 2, \"max\": 40960},\n                ),\n                \"mode\": ([\"STRETCH\", \"FIT\", \"FILL\", \"ASPECT\"],),\n                \"interpolation\": ([\"lanczos\", \"bilinear\", \"nearest\", \"bicubic\", \"area\"],),\n                \"antialias\": (\n                    \"BOOLEAN\",\n                    {\"default\": True},\n                ),\n                \"multiple_of\": (\n                    \"INT\",\n                    {\"default\": 1, \"min\": 1, \"step\": 1, \"max\": 1024},\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"IMAGE\",\n        \"MASK\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_PROCESSING_CAT\n    DESCRIPTION = \"\"\"\n    Resizes images and masks to specified dimensions with flexible options.\n    Supports various modes (stretch, fit, fill, aspect), interpolation methods,\n    and dimension constraints. Handles both RGB and grayscale inputs.\n    \"\"\"\n\n    def execute(\n        self,\n        image: Optional[torch.Tensor] = None,\n        mask: Optional[torch.Tensor] = None,\n        width: int = 1024,\n        height: int = 1024,\n        mode: str = \"default\",\n        interpolation: str = \"lanczos\",\n        antialias: bool = True,\n        multiple_of: int = 1,\n    ):\n        input_image = (\n            TensorImage.from_BWHC(image)\n            if isinstance(image, torch.Tensor)\n            else TensorImage(torch.zeros((1, 3, width, height)))\n        )\n        input_mask = (\n            TensorImage.from_BWHC(mask)\n            if isinstance(mask, torch.Tensor)\n            else TensorImage(torch.zeros((1, 1, width, height)))\n        )\n\n        output_image = resize(input_image, width, height, mode, interpolation, antialias, multiple_of).get_BWHC()\n        output_mask = resize(input_mask, width, height, mode, interpolation, antialias, multiple_of).get_BWHC()\n\n        return (\n            output_image,\n            output_mask,\n        )\n</code></pre>"},{"location":"nodes/image_processing/#upscaleimage","title":"UpscaleImage","text":"<p>AI-powered image upscaling with tiled processing and flexible scaling modes.</p> <p>A comprehensive image upscaling node that leverages AI models for high-quality image enlargement. Supports both factor-based rescaling and target size resizing while efficiently managing GPU memory through tiled processing. Compatible with various AI upscaling models and includes multiple resampling methods for final adjustments.</p>"},{"location":"nodes/image_processing/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required upscale_model <code>&lt;ast.Call object at 0x1007d3730&gt;</code> required mode <code>LIST</code> required rescale_factor <code>FLOAT</code> 2 min=0.01, max=100.0, step=0.01 required resize_size <code>INT</code> 1024 min=1, max=48000, step=1 required resampling_method <code>resampling_methods</code> required tiled_size <code>INT</code> 512 min=128, max=2048, step=128"},{"location":"nodes/image_processing/#returns_4","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class UpscaleImage:\n    \"\"\"AI-powered image upscaling with tiled processing and flexible scaling modes.\n\n    A comprehensive image upscaling node that leverages AI models for high-quality image enlargement.\n    Supports both factor-based rescaling and target size resizing while efficiently managing GPU\n    memory through tiled processing. Compatible with various AI upscaling models and includes\n    multiple resampling methods for final adjustments.\n\n    Args:\n        image (torch.Tensor): Input image tensor in BCHW format with values in range [0, 1].\n        upscale_model (str): Filename of the AI upscaling model to use.\n        mode (str): Scaling mode, either:\n            - \"rescale\": Scale relative to original size by a factor\n            - \"resize\": Scale to a specific target size\n        rescale_factor (float, optional): Scaling multiplier when using \"rescale\" mode.\n            Defaults to 2.0.\n        resize_size (int, optional): Target size in pixels for longest edge when using \"resize\" mode.\n            Defaults to 1024.\n        resampling_method (str, optional): Final resampling method for precise size adjustment.\n            Options: \"bilinear\", \"nearest\", \"bicubic\", \"area\". Defaults to \"bilinear\".\n        tiled_size (int, optional): Size of processing tiles in pixels. Larger tiles use more GPU memory.\n            Defaults to 512.\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing:\n            - image (torch.Tensor): Upscaled image in BCHW format with values in range [0, 1]\n\n    Raises:\n        ValueError: If the upscale model is invalid or incompatible\n        RuntimeError: If GPU memory is insufficient even with minimum tile size\n        TypeError: If input tensors are of incorrect type\n\n    Notes:\n        - Models are loaded from the \"upscale_models\" directory\n        - Processing is done in tiles to manage GPU memory efficiently\n        - For large upscaling factors, multiple passes may be performed\n        - The aspect ratio is always preserved in \"resize\" mode\n        - If GPU memory is insufficient, tile size is automatically reduced\n        - Tiled processing may show slight seams with some models\n        - Final output is always clamped to [0, 1] range\n        - Model scale factor is automatically detected and respected\n        - Progress bar shows processing status for large images\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        resampling_methods = [\"bilinear\", \"nearest\", \"bicubic\", \"area\"]\n\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"upscale_model\": (folder_paths.get_filename_list(\"upscale_models\"),),\n                \"mode\": ([\"rescale\", \"resize\"],),\n                \"rescale_factor\": (\n                    \"FLOAT\",\n                    {\"default\": 2, \"min\": 0.01, \"max\": 100.0, \"step\": 0.01},\n                ),\n                \"resize_size\": (\n                    \"INT\",\n                    {\"default\": 1024, \"min\": 1, \"max\": 48000, \"step\": 1},\n                ),\n                \"resampling_method\": (resampling_methods,),\n                \"tiled_size\": (\n                    \"INT\",\n                    {\"default\": 512, \"min\": 128, \"max\": 2048, \"step\": 128},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_PROCESSING_CAT\n    DESCRIPTION = \"\"\"\n    AI-powered image upscaling with tiled processing and flexible scaling modes.\n    Leverages AI models for high-quality enlargement with options for factor-based rescaling or target size resizing.\n    Efficiently manages GPU memory through tiled processing.\n    \"\"\"\n\n    def load_model(self, model_name):\n        model_path = folder_paths.get_full_path(\"upscale_models\", model_name)\n        sd = comfy.utils.load_torch_file(model_path, safe_load=True)\n        if \"module.layers.0.residual_group.blocks.0.norm1.weight\" in sd:\n            sd = comfy.utils.state_dict_prefix_replace(sd, {\"module.\": \"\"})\n        out = ModelLoader().load_from_state_dict(sd)\n\n        if not isinstance(out, ImageModelDescriptor):\n            raise ValueError(\"Upscale model must be a single-image model.\")\n\n        return out\n\n    def upscale_with_model(\n        self,\n        image: torch.Tensor,\n        upscale_model: Optional[ImageModelDescriptor],\n        device: Optional[torch.device],\n        tile: int = 512,\n        overlap: int = 32,\n    ) -&gt; torch.Tensor:\n        if upscale_model is None:\n            raise ValueError(\"upscale_model is required\")\n        if device is None:\n            raise ValueError(\"device is required\")\n        if not hasattr(upscale_model, \"model\"):\n            raise ValueError(\"upscale_model must have a model attribute\")\n        if not hasattr(upscale_model, \"scale\"):\n            raise ValueError(\"upscale_model must have a scale attribute\")\n\n        memory_required = comfy.model_management.module_size(upscale_model.model)\n        memory_required += (tile * tile * 3) * image.element_size() * max(upscale_model.scale, 1.0) * 384.0\n        memory_required += image.nelement() * image.element_size()\n        comfy.model_management.free_memory(memory_required, device)\n        in_img = image.movedim(-1, -3).to(device)\n\n        s = None\n        oom = True\n        while oom:\n            try:\n                steps = in_img.shape[0] * comfy.utils.get_tiled_scale_steps(\n                    in_img.shape[3],\n                    in_img.shape[2],\n                    tile_x=tile,\n                    tile_y=tile,\n                    overlap=overlap,\n                )\n                pbar = comfy.utils.ProgressBar(steps)\n                s = comfy.utils.tiled_scale(\n                    in_img,\n                    lambda a: upscale_model(a),\n                    tile_x=tile,\n                    tile_y=tile,\n                    overlap=overlap,\n                    upscale_amount=upscale_model.scale,\n                    pbar=pbar,\n                )\n                oom = False\n            except comfy.model_management.OOM_EXCEPTION as e:\n                tile //= 2\n                if tile &lt; 128:\n                    raise e\n\n        if not isinstance(s, torch.Tensor):\n            raise ValueError(\"Upscaling failed\")\n        s = torch.clamp(s.movedim(-3, -1), min=0, max=1.0)  # type: ignore\n        return s\n\n    def execute(\n        self,\n        image: torch.Tensor,\n        upscale_model: str,\n        mode: str = \"rescale\",\n        rescale_factor: float = 2,\n        resize_size: int = 1024,\n        resampling_method: str = \"bilinear\",\n        tiled_size: int = 512,\n    ):\n        # Load upscale model\n        up_model = self.load_model(upscale_model)\n        device = comfy.model_management.get_torch_device()\n        up_model.to(device)\n\n        # target size\n        _, H, W, _ = image.shape\n        target_size = resize_size if mode == \"resize\" else max(H, W) * rescale_factor\n        current_size = max(H, W)\n        up_image = image\n        while current_size &lt; target_size:\n            step = self.upscale_with_model(upscale_model=up_model, image=up_image, device=device, tile=tiled_size)\n            del up_image\n            up_image = step.to(\"cpu\")\n            _, H, W, _ = up_image.shape\n            current_size = max(H, W)\n\n        up_model.to(\"cpu\")\n        tensor_image = TensorImage.from_BWHC(up_image)\n\n        if mode == \"resize\":\n            up_image = resize(\n                tensor_image,\n                resize_size,\n                resize_size,\n                \"ASPECT\",\n                resampling_method,\n                True,\n            ).get_BWHC()\n        else:\n            # get the max size of the upscaled image\n            _, _, H, W = tensor_image.shape\n            upscaled_max_size = max(H, W)\n\n            original_image = TensorImage.from_BWHC(image)\n            _, _, ori_H, ori_W = original_image.shape\n            original_max_size = max(ori_H, ori_W)\n\n            # rescale_factor is the factor to multiply the original max size\n            original_target_size = rescale_factor * original_max_size\n            scale_factor = original_target_size / upscaled_max_size\n\n            up_image = rescale(tensor_image, scale_factor, resampling_method, True).get_BWHC()\n\n        return (up_image,)\n</code></pre>"},{"location":"nodes/image_processing/#rotate","title":"Rotate","text":"<p>Rotates images and masks by a specified angle with optional zoom adjustment.</p> <p>Performs rotation of images and masks with control over whether to zoom to fit the entire rotated content. Useful for reorienting content while managing the trade-off between content preservation and output size.</p>"},{"location":"nodes/image_processing/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras optional image <code>IMAGE</code> None optional mask <code>MASK</code> None optional angle <code>FLOAT</code> 0.0 min=0, max=360.0, step=1.0 optional zoom_to_fit <code>BOOLEAN</code> False"},{"location":"nodes/image_processing/#returns_5","title":"Returns","text":"Name Type image <code>IMAGE</code> mask <code>MASK</code> Source code <pre><code>class Rotate:\n    \"\"\"Rotates images and masks by a specified angle with optional zoom adjustment.\n\n    Performs rotation of images and masks with control over whether to zoom to fit\n    the entire rotated content. Useful for reorienting content while managing the\n    trade-off between content preservation and output size.\n\n    Args:\n        image (torch.Tensor, optional): Input image in BWHC format with values in range [0, 1]\n        mask (torch.Tensor, optional): Input mask in BWHC format with values in range [0, 1]\n        angle (float): Rotation angle in degrees (0-360)\n        zoom_to_fit (bool): Whether to zoom out to show all rotated content\n\n    Returns:\n        tuple:\n            - image (torch.Tensor): Rotated image in BWHC format\n            - mask (torch.Tensor): Rotated mask in BWHC format\n\n    Raises:\n        ValueError: If neither image nor mask is provided\n        ValueError: If angle is outside valid range\n        RuntimeError: If input tensors have invalid dimensions\n\n    Notes:\n        - At least one of image or mask must be provided\n        - Rotation is performed counterclockwise\n        - When zoom_to_fit is False, corners may be clipped\n        - When zoom_to_fit is True, output may be larger\n        - Interpolation is bilinear for smooth results\n        - Empty areas after rotation are filled with black\n        - Maintains aspect ratio of input\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {},\n            \"optional\": {\n                \"image\": (\"IMAGE\", {\"default\": None}),\n                \"mask\": (\"MASK\", {\"default\": None}),\n                \"angle\": (\n                    \"FLOAT\",\n                    {\"default\": 0.0, \"min\": 0, \"max\": 360.0, \"step\": 1.0},\n                ),\n                \"zoom_to_fit\": (\"BOOLEAN\", {\"default\": False}),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"IMAGE\",\n        \"MASK\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_PROCESSING_CAT\n    DESCRIPTION = \"\"\"\n    Rotates images and masks by a specified angle with optional zoom adjustment.\n    Controls whether to zoom out to show all rotated content or maintain original dimensions.\n    Useful for reorienting content while managing content preservation.\n    \"\"\"\n\n    def execute(\n        self,\n        image: Optional[torch.Tensor] = None,\n        mask: Optional[torch.Tensor] = None,\n        angle: float = 0.0,\n        zoom_to_fit: bool = False,\n    ):\n        input_image = (\n            TensorImage.from_BWHC(image) if isinstance(image, torch.Tensor) else TensorImage(torch.zeros((1, 3, 1, 1)))\n        )\n        input_mask = (\n            TensorImage.from_BWHC(mask) if isinstance(mask, torch.Tensor) else TensorImage(torch.zeros((1, 1, 1, 1)))\n        )\n        output_image = rotate(input_image, angle, zoom_to_fit).get_BWHC()\n        output_mask = rotate(input_mask, angle, zoom_to_fit).get_BWHC()\n\n        return (\n            output_image,\n            output_mask,\n        )\n</code></pre>"},{"location":"nodes/image_processing/#resizewithmegapixels","title":"ResizeWithMegapixels","text":"<p>Resizes images and masks to a target megapixel count while preserving aspect ratio.</p> <p>A specialized resizing node that targets a specific image size in megapixels rather than exact dimensions. This is useful for batch processing images to a consistent size while maintaining their original proportions and managing memory usage.</p>"},{"location":"nodes/image_processing/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required megapixels <code>FLOAT</code> 1.0 min=0.01, max=100.0, step=0.1 optional image <code>IMAGE</code> None optional mask <code>MASK</code> None optional interpolation <code>LIST</code> optional antialias <code>BOOLEAN</code> True optional multiple_of <code>INT</code> 1 min=1, step=1, max=1024"},{"location":"nodes/image_processing/#returns_6","title":"Returns","text":"Name Type image <code>IMAGE</code> mask <code>MASK</code> Source code <pre><code>class ResizeWithMegapixels:\n    \"\"\"Resizes images and masks to a target megapixel count while preserving aspect ratio.\n\n    A specialized resizing node that targets a specific image size in megapixels rather than\n    exact dimensions. This is useful for batch processing images to a consistent size while\n    maintaining their original proportions and managing memory usage.\n\n    Args:\n        megapixels (float): Target size in megapixels (0.01-100.0)\n        image (torch.Tensor, optional): Input image in BWHC format with values in range [0, 1]\n        mask (torch.Tensor, optional): Input mask in BWHC format with values in range [0, 1]\n        interpolation (str): Resampling method:\n            - \"bilinear\": Linear interpolation (smooth)\n            - \"nearest\": Nearest neighbor (sharp)\n            - \"bicubic\": Cubic interpolation (smoother)\n            - \"area\": Area averaging (good for downscaling)\n        antialias (bool): Whether to apply antialiasing when downscaling\n\n    Returns:\n        tuple:\n            - image (torch.Tensor): Resized image in BWHC format\n            - mask (torch.Tensor): Resized mask in BWHC format\n\n    Raises:\n        ValueError: If neither image nor mask is provided\n        ValueError: If invalid interpolation method\n        RuntimeError: If input tensors have invalid dimensions\n\n    Notes:\n        - At least one of image or mask must be provided\n        - Output maintains the same number of channels as input\n        - Aspect ratio is always preserved\n        - Target dimensions are calculated as sqrt(megapixels * 1M * aspect_ratio)\n        - Actual output size may vary slightly due to rounding\n        - Memory usage scales linearly with megapixel count\n        - Antialiasing recommended when downscaling\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"megapixels\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.01, \"max\": 100.0, \"step\": 0.1},\n                ),\n            },\n            \"optional\": {\n                \"image\": (\"IMAGE\", {\"default\": None}),\n                \"mask\": (\"MASK\", {\"default\": None}),\n                \"interpolation\": ([\"lanczos\", \"bilinear\", \"nearest\", \"bicubic\", \"area\"],),\n                \"antialias\": (\n                    \"BOOLEAN\",\n                    {\"default\": True},\n                ),\n                \"multiple_of\": (\n                    \"INT\",\n                    {\"default\": 1, \"min\": 1, \"step\": 1, \"max\": 1024},\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"IMAGE\",\n        \"MASK\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = IMAGE_PROCESSING_CAT\n    DESCRIPTION = \"\"\"\n    Resizes images based on total pixel count (megapixels) while preserving aspect ratio.\n    Instead of specifying exact dimensions, target a specific image size in megapixels.\n    Useful for batch processing to consistent quality levels or reducing memory usage.\n    \"\"\"\n\n    def get_dimensions(self, megapixels: float, original_width: int = 0, original_height: int = 0) -&gt; tuple[int, int]:\n        \"\"\"Calculate target dimensions based on megapixels while preserving aspect ratio.\n\n        Args:\n            megapixels (float): Target size in megapixels\n            original_width (int): Original image width (optional)\n            original_height (int): Original image height (optional)\n\n        Returns:\n            tuple[int, int]: Target (width, height)\n        \"\"\"\n        total_pixels = megapixels * 1000000\n\n        if original_width &gt; 0 and original_height &gt; 0:\n            # Preserve aspect ratio if original dimensions are provided\n            aspect_ratio = original_width / original_height\n            height = math.sqrt(total_pixels / aspect_ratio)\n            width = height * aspect_ratio\n        else:\n            # Default to square dimensions if no original dimensions\n            width = height = math.sqrt(total_pixels)\n\n        return (round(width), round(height))\n\n    def execute(\n        self,\n        megapixels: float = 1.0,\n        image: Optional[torch.Tensor] = None,\n        mask: Optional[torch.Tensor] = None,\n        interpolation: str = \"lanczos\",\n        antialias: bool = True,\n        multiple_of: int = 1,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        if isinstance(image, torch.Tensor):\n            shape = TensorImage.from_BWHC(image).shape\n\n        elif isinstance(mask, torch.Tensor):\n            shape = TensorImage.from_BWHC(mask).shape\n\n        else:\n            raise ValueError(\"Either image or mask must be provided\")\n\n        _, _, orig_height, orig_width = shape\n\n        target_width, target_height = self.get_dimensions(megapixels, orig_width, orig_height)\n\n        # Create a Resize instance and call its execute method\n        resize_node = Resize()\n        output_image, output_mask = resize_node.execute(\n            image=image,\n            mask=mask,\n            width=target_width,\n            height=target_height,\n            mode=\"ASPECT\",\n            interpolation=interpolation,\n            antialias=antialias,\n            multiple_of=multiple_of,\n        )\n\n        return (output_image, output_mask)\n</code></pre>"},{"location":"nodes/labs/","title":"Labs Nodes","text":""},{"location":"nodes/labs/#loopstart","title":"LoopStart","text":"<p>Initiates a loop with optional initial values for each iteration.</p> <p>This node is deprecated. Use the \"Do While Loop Start\" node instead.</p> <p>A control node that starts a loop, allowing for a specified number of iterations. It can accept optional initial values for each iteration, which can be used within the loop. This node is useful for creating iterative workflows where the same set of operations is performed multiple times.</p> Source code <pre><code>class LoopStart:\n    \"\"\"Initiates a loop with optional initial values for each iteration.\n\n    This node is deprecated. Use the \"Do While Loop Start\" node instead.\n\n    A control node that starts a loop, allowing for a specified number of iterations. It can accept\n    optional initial values for each iteration, which can be used within the loop. This node is useful\n    for creating iterative workflows where the same set of operations is performed multiple times.\n\n    Args:\n        init_value (Any): The initial value for the first iteration. Can be of any type.\n\n    Returns:\n        tuple[tuple]: A tuple containing a flow control signal and the initial values for each iteration.\n\n    Notes:\n        - The number of initial values can be adjusted by changing the MAX_FLOW_NUM constant.\n        - Each initial value can be of any type, providing flexibility for different workflows.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"optional\": {},\n        }\n        for i in range(MAX_FLOW_NUM):\n            inputs[\"optional\"][f\"init_value_{i}\"] = (any_type,)\n        return inputs\n\n    RETURN_TYPES = ByPassTypeTuple(tuple([\"FLOW_CONTROL\"] + [any_type] * MAX_FLOW_NUM))\n    RETURN_NAMES = ByPassTypeTuple(tuple([\"flow\"] + [f\"value_{i}\" for i in range(MAX_FLOW_NUM)]))\n    FUNCTION = \"execute\"\n    DEPRECATED = True\n\n    CATEGORY = LABS_CAT + \"/Loops\"\n\n    def execute(self, **kwargs):\n        values = []\n        for i in range(MAX_FLOW_NUM):\n            values.append(kwargs.get(f\"init_value_{i}\", None))\n        return tuple([\"stub\"] + values)\n</code></pre>"},{"location":"nodes/labs/#loopend","title":"LoopEnd","text":"<p>Ends a loop and returns the final values after the loop execution.</p> <p>This node is deprecated. Use the \"Do While Loop End\" node instead.</p> <p>A control node that signifies the end of a loop initiated by a <code>LoopStart</code> node. It processes the flow control signal and can return the final values from the loop iterations. This node is useful for managing the completion of iterative workflows and retrieving results after looping.</p> Source code <pre><code>class LoopEnd:\n    \"\"\"Ends a loop and returns the final values after the loop execution.\n\n    This node is deprecated. Use the \"Do While Loop End\" node instead.\n\n    A control node that signifies the end of a loop initiated by a `LoopStart` node. It processes the\n    flow control signal and can return the final values from the loop iterations. This node is useful\n    for managing the completion of iterative workflows and retrieving results after looping.\n\n    Args:\n        flow (FLOW_CONTROL): The flow control signal indicating the current state of the loop.\n        end_loop (bool): A boolean flag that indicates whether to end the loop. If True, the loop will terminate.\n        dynprompt (DYNPROMPT, optional): Dynamic prompt information for the node.\n        unique_id (UNIQUE_ID, optional): A unique identifier for the loop instance.\n\n    Returns:\n        tuple: A tuple containing the final values from the loop iterations.\n\n    Notes:\n        - The loop can be terminated based on the `end_loop` flag,\n          allowing for flexible control over the iteration process.\n        - The number of returned values corresponds to the number of initial values provided in the `LoopStart`.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"required\": {\n                \"flow\": (\"FLOW_CONTROL\", {\"rawLink\": True}),\n                \"end_loop\": (\"BOOLEAN\", {}),\n            },\n            \"optional\": {},\n            \"hidden\": {\n                \"dynprompt\": \"DYNPROMPT\",\n                \"unique_id\": \"UNIQUE_ID\",\n            },\n        }\n        for i in range(MAX_FLOW_NUM):\n            inputs[\"optional\"][f\"init_value_{i}\"] = (any_type,)\n        return inputs\n\n    RETURN_TYPES = ByPassTypeTuple(tuple([any_type] * MAX_FLOW_NUM))\n    RETURN_NAMES = ByPassTypeTuple(tuple(f\"value_{i}\" for i in range(MAX_FLOW_NUM)))\n    FUNCTION = \"execute\"\n    DEPRECATED = True\n    CATEGORY = LABS_CAT + \"/Loops\"\n\n    def explore_dependencies(self, node_id, dynprompt, upstream):\n        node_info = dynprompt.get_node(node_id)\n        if \"inputs\" not in node_info:\n            return\n        for _, v in node_info[\"inputs\"].items():\n            if is_link(v):\n                parent_id = v[0]\n                if parent_id not in upstream:\n                    upstream[parent_id] = []\n                    self.explore_dependencies(parent_id, dynprompt, upstream)\n                upstream[parent_id].append(node_id)\n\n    def collect_contained(self, node_id, upstream, contained):\n        if node_id not in upstream:\n            return\n        for child_id in upstream[node_id]:\n            if child_id not in contained:\n                contained[child_id] = True\n                self.collect_contained(child_id, upstream, contained)\n\n    def execute(self, flow: tuple[str], end_loop: bool, dynprompt=None, unique_id=None, **kwargs):\n        if end_loop:\n            # We're done with the loop\n            values = []\n            for i in range(MAX_FLOW_NUM):\n                values.append(kwargs.get(f\"init_value_{i}\", None))\n            return tuple(values)\n\n        # We want to loop\n        if dynprompt is not None:\n            _ = dynprompt.get_node(unique_id)\n        upstream = {}\n        # Get the list of all nodes between the open and close nodes\n        self.explore_dependencies(unique_id, dynprompt, upstream)\n\n        contained = {}\n        open_node = flow[0]\n        self.collect_contained(open_node, upstream, contained)\n        contained[unique_id] = True\n        contained[open_node] = True\n\n        graph = GraphBuilder()\n        for node_id in contained:\n            if dynprompt is not None:\n                original_node = dynprompt.get_node(node_id)\n                node = graph.node(\n                    original_node[\"class_type\"],\n                    \"Recurse\" if node_id == unique_id else node_id,\n                )\n                node.set_override_display_id(node_id)\n        for node_id in contained:\n            if dynprompt is not None:\n                original_node = dynprompt.get_node(node_id)\n                node = graph.lookup_node(\"Recurse\" if node_id == unique_id else node_id)\n                for k, v in original_node[\"inputs\"].items():\n                    if is_link(v) and v[0] in contained:\n                        parent = graph.lookup_node(v[0])\n                        node.set_input(k, parent.out(v[1]))\n                    else:\n                        node.set_input(k, v)\n\n        new_open = graph.lookup_node(open_node)\n        for i in range(MAX_FLOW_NUM):\n            key = f\"init_value_{i}\"\n            new_open.set_input(key, kwargs.get(key, None))\n        my_clone = graph.lookup_node(\"Recurse\")\n        result = map(lambda x: my_clone.out(x), range(MAX_FLOW_NUM))\n        return {\n            \"result\": tuple(result),\n            \"expand\": graph.finalize(),\n        }\n</code></pre>"},{"location":"nodes/labs/#otsuthreshold","title":"OTSUThreshold","text":"<p>A node that performs Otsu's thresholding on an input image.</p> <p>This node implements Otsu's method, which automatically determines an optimal threshold value by minimizing intra-class intensity variance. It converts the input image to grayscale and applies binary thresholding using the computed optimal threshold.</p>"},{"location":"nodes/labs/#inputs","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/labs/#returns","title":"Returns","text":"Name Type float <code>FLOAT</code> image <code>IMAGE</code> Source code <pre><code>class OTSUThreshold:\n    \"\"\"A node that performs Otsu's thresholding on an input image.\n\n    This node implements Otsu's method, which automatically determines an optimal threshold\n    value by minimizing intra-class intensity variance. It converts the input image to\n    grayscale and applies binary thresholding using the computed optimal threshold.\n\n    Args:\n        image (torch.Tensor): The input image tensor to threshold.\n                            Expected shape: (B, H, W, C)\n\n    Returns:\n        tuple[float, torch.Tensor]: A tuple containing:\n            - The computed Otsu threshold value\n            - The thresholded binary image as a tensor with shape (B, H, W, C)\n\n    Notes:\n        - The input image is automatically converted to grayscale before thresholding\n        - The output binary image contains values of 0 and 255\n        - The threshold computation is performed using OpenCV's implementation\n    \"\"\"\n\n    CLASS_ID = \"otsu_threshold\"\n    CATEGORY = LABS_CAT\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            }\n        }\n\n    RETURN_TYPES = (\"FLOAT\", \"IMAGE\")\n    FUNCTION = \"execute\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[float, torch.Tensor]:\n        img_transformed = v2.Compose(\n            [\n                ops.Permute(dims=[0, 3, 1, 2]),\n                v2.Grayscale(),\n                v2.ToDtype(torch.uint8, scale=True),\n            ]\n        )(image)\n        img_np = img_transformed.squeeze().cpu().numpy()\n\n        thresh, out = cv.threshold(img_np, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n\n        out_torch = torch.from_numpy(out).to(image.device).unsqueeze(0).unsqueeze(0)\n        out_transformed = v2.Compose([v2.RGB(), ops.Permute(dims=[0, 2, 3, 1])])(out_torch)\n        return (thresh, out_transformed)\n</code></pre>"},{"location":"nodes/labs/#forloopend","title":"ForLoopEnd","text":"<p>Ends a for loop and returns final values after all iterations.     Works with ForLoopStart to create iterative workflows with a fixed number of iterations.     Manages loop state and termination based on the iteration count.</p> Source code <pre><code>class ForLoopEnd:\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"required\": {\n                \"flow\": (\"FLOW_CONTROL\", {\"rawLink\": True, \"forceInput\": True}),\n                \"num_slots\": ([str(i) for i in range(1, MAX_FLOW_NUM)], {\"default\": \"1\"}),\n            },\n            \"optional\": {},\n            \"hidden\": {\n                \"dynprompt\": \"DYNPROMPT\",\n                \"unique_id\": \"UNIQUE_ID\",\n            },\n        }\n        for i in range(1, MAX_FLOW_NUM):\n            inputs[\"optional\"][f\"init_value_{i}\"] = (any_type, {\"rawLink\": True, \"forceInput\": True})\n        return inputs\n\n    RETURN_TYPES = ByPassTypeTuple(tuple([any_type] * (MAX_FLOW_NUM - 1)))\n    RETURN_NAMES = ByPassTypeTuple(tuple(f\"value_{i}\" for i in range(1, MAX_FLOW_NUM)))\n    FUNCTION = \"execute\"\n    CATEGORY = LABS_CAT + \"/Loops\"\n    DESCRIPTION = \"\"\"\n    Ends a for loop and returns final values after all iterations.\n    Works with ForLoopStart to create iterative workflows with a fixed number of iterations.\n    Manages loop state and termination based on the iteration count.\n    \"\"\"\n\n    def execute(self, flow: tuple[str], dynprompt=None, unique_id=None, **kwargs):\n        graph = GraphBuilder()\n        while_open = flow[0]\n        iterations = 0\n\n        if dynprompt is not None:\n            forstart_node = dynprompt.get_node(while_open)\n            inputs = forstart_node[\"inputs\"]\n            iterations = inputs[\"iterations\"]\n\n        mathAddOne = graph.node(\"signature_math_operator\", value=\"a+1\", a=[while_open, 1])\n        condition = graph.node(\"signature_compare\", a=mathAddOne.out(0), b=iterations, comparison=\"a &gt;= b\")\n\n        input_values = {(f\"init_value_{i}\"): kwargs.get(f\"init_value_{i}\", None) for i in range(1, MAX_FLOW_NUM)}\n        while_close = graph.node(\n            \"signature_do_while_loop_end\",\n            flow=flow,\n            end_loop=condition.out(0),\n            init_value_0=mathAddOne.out(0),\n            **input_values,\n        )\n\n        return {\n            \"result\": tuple([while_close.out(i) for i in range(1, MAX_FLOW_NUM)]),\n            \"expand\": graph.finalize(),\n        }\n</code></pre>"},{"location":"nodes/labs/#dowhileloopend","title":"DoWhileLoopEnd","text":"<p>Ends a loop and returns the final values after the loop execution.</p> <p>A control node that signifies the end of a loop initiated by a <code>LoopStart</code> node. It processes the flow control signal and can return the final values from the loop iterations. This node is useful for managing the completion of iterative workflows and retrieving results after looping.</p> Source code <pre><code>class DoWhileLoopEnd:\n    \"\"\"Ends a loop and returns the final values after the loop execution.\n\n    A control node that signifies the end of a loop initiated by a `LoopStart` node. It processes the\n    flow control signal and can return the final values from the loop iterations. This node is useful\n    for managing the completion of iterative workflows and retrieving results after looping.\n\n    Args:\n        flow (FLOW_CONTROL): The flow control signal indicating the current state of the loop.\n        end_loop (bool): A boolean flag that indicates whether to end the loop. If True, the loop will terminate.\n        dynprompt (DYNPROMPT, optional): Dynamic prompt information for the node.\n        unique_id (UNIQUE_ID, optional): A unique identifier for the loop instance.\n\n    Returns:\n        tuple: A tuple containing the final values from the loop iterations.\n\n    Notes:\n        - The loop can be terminated based on the `end_loop` flag,\n          allowing for flexible control over the iteration process.\n        - The number of returned values corresponds to the number of initial values provided in the `LoopStart`.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"required\": {\n                \"flow\": (\"FLOW_CONTROL\", {\"rawLink\": True, \"forceInput\": True}),\n                \"end_loop\": (\"BOOLEAN\", {\"forceInput\": True}),\n                \"num_slots\": ([str(i) for i in range(1, MAX_FLOW_NUM + 1)], {\"default\": \"1\"}),\n            },\n            \"optional\": {},\n            \"hidden\": {\"dynprompt\": \"DYNPROMPT\", \"unique_id\": \"UNIQUE_ID\"},\n        }\n        for i in range(MAX_FLOW_NUM):\n            inputs[\"optional\"][f\"init_value_{i}\"] = (any_type, {\"forceInput\": True})\n        return inputs\n\n    RETURN_TYPES = ByPassTypeTuple(tuple([any_type] * MAX_FLOW_NUM))\n    RETURN_NAMES = ByPassTypeTuple(tuple(f\"value_{i}\" for i in range(MAX_FLOW_NUM)))\n    FUNCTION = \"execute\"\n    CATEGORY = LABS_CAT + \"/Loops\"\n    DESCRIPTION = \"\"\"\n    Ends a do-while loop and returns final values after execution.\n    Controls loop termination based on the 'end_loop' condition.\n    Works with DoWhileLoopStart to create iterative workflows that execute at least once before checking the condition.\n    \"\"\"\n\n    def explore_dependencies(self, node_id, dynprompt, upstream, parent_ids):\n        node_info = dynprompt.get_node(node_id)\n        if \"inputs\" not in node_info:\n            return\n        for _, v in node_info[\"inputs\"].items():\n            if is_link(v):\n                parent_id = v[0]\n                display_id = dynprompt.get_display_node_id(parent_id)\n                display_node = dynprompt.get_node(display_id)\n                class_type = display_node[\"class_type\"]\n                if class_type not in [\"signature_for_loop_end\", \"signature_do_while_loop_end\"]:\n                    parent_ids.append(display_id)\n                if parent_id not in upstream:\n                    upstream[parent_id] = []\n                    self.explore_dependencies(parent_id, dynprompt, upstream, parent_ids)\n\n                upstream[parent_id].append(node_id)\n\n    def explore_output_nodes(self, dynprompt, upstream, output_nodes, parent_ids):\n        for parent_id in upstream:\n            display_id = dynprompt.get_display_node_id(parent_id)\n            for output_id in output_nodes:\n                id = output_nodes[output_id][0]\n                if id in parent_ids and display_id == id and output_id not in upstream[parent_id]:\n                    if \".\" in parent_id:\n                        arr = parent_id.split(\".\")\n                        arr[len(arr) - 1] = output_id\n                        upstream[parent_id].append(\".\".join(arr))\n                    else:\n                        upstream[parent_id].append(output_id)\n\n    def collect_contained(self, node_id, upstream, contained):\n        if node_id not in upstream:\n            return\n        for child_id in upstream[node_id]:\n            if child_id not in contained:\n                contained[child_id] = True\n                self.collect_contained(child_id, upstream, contained)\n\n    def execute(self, flow: tuple[str], end_loop: bool, dynprompt=None, unique_id=None, **kwargs):\n        if end_loop:\n            # We're done with the loop\n            values = []\n            for i in range(MAX_FLOW_NUM):\n                values.append(kwargs.get(f\"init_value_{i}\", None))\n            return tuple(values)\n\n        # We want to loop\n        if dynprompt is not None:\n            dynprompt.get_node(unique_id)\n        upstream = {}\n        # Get the list of all nodes between the open and close nodes\n        parent_ids = []\n        self.explore_dependencies(unique_id, dynprompt, upstream, parent_ids)\n        parent_ids = list(set(parent_ids))\n\n        if dynprompt is not None:\n            prompts = dynprompt.get_original_prompt()\n        output_nodes = {}\n        for id in prompts:\n            node = prompts[id]\n            if \"inputs\" not in node:\n                continue\n            class_type = node[\"class_type\"]\n            class_def = ALL_NODE_CLASS_MAPPINGS[class_type]\n            if hasattr(class_def, \"OUTPUT_NODE\") and class_def.OUTPUT_NODE:\n                for k, v in node[\"inputs\"].items():\n                    if is_link(v):\n                        output_nodes[id] = v\n\n        graph = GraphBuilder()\n        self.explore_output_nodes(dynprompt, upstream, output_nodes, parent_ids)\n        contained = {}\n        open_node = flow[0]\n        self.collect_contained(open_node, upstream, contained)\n        contained[unique_id] = True\n        contained[open_node] = True\n\n        for node_id in contained:\n            if dynprompt is not None:\n                original_node = dynprompt.get_node(node_id)\n                node = graph.node(\n                    original_node[\"class_type\"],\n                    \"Recurse\" if node_id == unique_id else node_id,\n                )\n                node.set_override_display_id(node_id)\n        for node_id in contained:\n            if dynprompt is not None:\n                original_node = dynprompt.get_node(node_id)\n                node = graph.lookup_node(\"Recurse\" if node_id == unique_id else node_id)\n                for k, v in original_node[\"inputs\"].items():\n                    if is_link(v) and v[0] in contained:\n                        parent = graph.lookup_node(v[0])\n                        node.set_input(k, parent.out(v[1]))\n                    else:\n                        node.set_input(k, v)\n\n        new_open = graph.lookup_node(open_node)\n        for i in range(MAX_FLOW_NUM):\n            key = f\"init_value_{i}\"\n            new_open.set_input(key, kwargs.get(key, None))\n        my_clone = graph.lookup_node(\"Recurse\")\n        result = map(lambda x: my_clone.out(x), range(MAX_FLOW_NUM))\n        return {\n            \"result\": tuple(result),\n            \"expand\": graph.finalize(),\n        }\n</code></pre>"},{"location":"nodes/labs/#blocker","title":"Blocker","text":"<p>Controls flow execution based on a boolean condition.</p> <p>A utility node that blocks or allows execution flow based on a boolean flag. When the continue flag is False, it blocks execution by returning an ExecutionBlocker. When True, it passes through the input value unchanged.</p>"},{"location":"nodes/labs/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required should_continue <code>BOOLEAN</code> False required input <code>any_type</code> None Source code <pre><code>class Blocker:\n    \"\"\"Controls flow execution based on a boolean condition.\n\n    A utility node that blocks or allows execution flow based on a boolean flag. When the continue\n    flag is False, it blocks execution by returning an ExecutionBlocker. When True, it passes through\n    the input value unchanged.\n\n    Args:\n        continue (bool): Flag to control execution flow. When False, blocks execution.\n        in (Any): The input value to pass through when execution is allowed.\n\n    Returns:\n        tuple[Any]: A single-element tuple containing either:\n            - The input value if continue is True\n            - An ExecutionBlocker if continue is False\n\n    Notes:\n        - Useful for conditional workflow execution\n        - Can be used to create branches in execution flow\n        - The ExecutionBlocker prevents downstream nodes from executing\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"should_continue\": (\"BOOLEAN\", {\"default\": False}),\n                \"input\": (any_type, {\"default\": None}),\n            },\n        }\n\n    RETURN_TYPES = (any_type,)\n    RETURN_NAMES = (\"out\",)\n    CATEGORY = LABS_CAT\n    FUNCTION = \"execute\"\n    DESCRIPTION = \"\"\"\n    Controls workflow execution based on a boolean condition.\n    When 'should_continue' is False, blocks downstream execution by returning an ExecutionBlocker.\n    When True, passes through the input value unchanged.\n    Useful for conditional branches.\n    \"\"\"\n\n    def execute(self, should_continue: bool = False, input: Any = None) -&gt; tuple[Any]:\n        return (input if should_continue else ExecutionBlocker(None),)\n</code></pre>"},{"location":"nodes/labs/#forloopstart","title":"ForLoopStart","text":"<p>Initiates a for loop with a specified number of iterations.     Creates a loop that executes a fixed number of times, tracking the current iteration index.     Works with ForLoopEnd to create iterative workflows with predictable execution counts.</p> Source code <pre><code>class ForLoopStart:\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"required\": {\n                \"iterations\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 100000, \"step\": 1}),\n                \"num_slots\": ([str(i) for i in range(1, MAX_FLOW_NUM)], {\"default\": \"1\"}),\n            },\n            \"optional\": {},\n            \"hidden\": {\n                \"init_value_0\": (any_type,),\n            },\n        }\n        for i in range(1, MAX_FLOW_NUM):\n            inputs[\"optional\"][f\"init_value_{i}\"] = (any_type, {\"forceInput\": True})\n        return inputs\n\n    RETURN_TYPES = ByPassTypeTuple(tuple([\"FLOW_CONTROL\", \"INT\"] + [any_type] * (MAX_FLOW_NUM - 1)))\n    RETURN_NAMES = ByPassTypeTuple(tuple([\"flow\", \"current_index\"] + [f\"value_{i}\" for i in range(1, MAX_FLOW_NUM)]))\n    FUNCTION = \"execute\"\n\n    CATEGORY = LABS_CAT + \"/Loops\"\n    DESCRIPTION = \"\"\"\n    Initiates a for loop with a specified number of iterations.\n    Creates a loop that executes a fixed number of times, tracking the current iteration index.\n    Works with ForLoopEnd to create iterative workflows with predictable execution counts.\n    \"\"\"\n\n    def execute(self, **kwargs):\n        current_index = 0\n        if \"init_value_0\" in kwargs:\n            current_index = kwargs[\"init_value_0\"]\n\n        graph = GraphBuilder()\n        initial_values = {(f\"init_value_{i}\"): kwargs.get(f\"init_value_{i}\", None) for i in range(1, MAX_FLOW_NUM)}\n        # TODO: check if signature_do_while_loop_start is the correct name, or if it should be signature_for_loop_start\n        graph.node(\"signature_do_while_loop_start\", init_value_0=current_index, **initial_values)\n        outputs = [kwargs.get(f\"init_value_{i}\", None) for i in range(1, MAX_FLOW_NUM)]\n        return {\n            \"result\": tuple([\"stub\", current_index] + outputs),\n            \"expand\": graph.finalize(),\n        }\n</code></pre>"},{"location":"nodes/labs/#dowhileloopstart","title":"DoWhileLoopStart","text":"<p>Initiates a loop with optional initial values for each iteration.</p> <p>A control node that starts a loop, allowing for a specified number of iterations. It can accept optional initial values for each iteration, which can be used within the loop. This node is useful for creating iterative workflows where the same set of operations is performed multiple times. The \"Do While Loop End\" node checks if the loop should continue, so this is a \"do while\" loop.</p> Source code <pre><code>class DoWhileLoopStart:\n    \"\"\"Initiates a loop with optional initial values for each iteration.\n\n    A control node that starts a loop, allowing for a specified number of iterations. It can accept\n    optional initial values for each iteration, which can be used within the loop. This node is useful\n    for creating iterative workflows where the same set of operations is performed multiple times. The\n    \"Do While Loop End\" node checks if the loop should continue, so this is a \"do while\" loop.\n\n    Args:\n        init_value (Any): The initial value for the first iteration. Can be of any type.\n\n    Returns:\n        tuple[tuple]: A tuple containing a flow control signal and the initial values for each iteration.\n\n    Notes:\n        - The number of initial values can be adjusted by changing the MAX_FLOW_NUM constant.\n        - Each initial value can be of any type, providing flexibility for different workflows.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"required\": {\n                \"num_slots\": ([str(i) for i in range(1, MAX_FLOW_NUM + 1)], {\"default\": \"1\"}),\n            },\n            \"optional\": {},\n        }\n        for i in range(MAX_FLOW_NUM):\n            inputs[\"optional\"][f\"init_value_{i}\"] = (any_type, {\"forceInput\": True})\n        return inputs\n\n    RETURN_TYPES = ByPassTypeTuple(tuple([\"FLOW_CONTROL\"] + [any_type] * MAX_FLOW_NUM))\n    RETURN_NAMES = ByPassTypeTuple(tuple([\"flow\"] + [f\"value_{i}\" for i in range(MAX_FLOW_NUM)]))\n    FUNCTION = \"execute\"\n\n    CATEGORY = LABS_CAT + \"/Loops\"\n    DESCRIPTION = \"\"\"\n    Initiates a do-while loop with optional initial values.\n    Starts an iterative workflow that executes at least once before checking a condition.\n    Works with DoWhileLoopEnd to create loops that process the same operations multiple times.\n    \"\"\"\n\n    def execute(self, **kwargs):\n        values = []\n        for i in range(MAX_FLOW_NUM):\n            values.append(kwargs.get(f\"init_value_{i}\", None))\n        return tuple([\"stub\"] + values)\n</code></pre>"},{"location":"nodes/labs/#lorastacker","title":"LoraStacker","text":"<p>Manages multiple LoRA models with configurable weights and modes.</p> <p>This node provides an interface for stacking multiple LoRA models with independent weight controls and two operating modes for simple or advanced weight management.</p>"},{"location":"nodes/labs/#returns_1","title":"Returns","text":"Name Type lora_stack <code>LORA_STACK</code> Source code <pre><code>class LoraStacker:\n    \"\"\"Manages multiple LoRA models with configurable weights and modes.\n\n    This node provides an interface for stacking multiple LoRA models with independent\n    weight controls and two operating modes for simple or advanced weight management.\n\n    Args:\n        num_slots (str): Number of LoRA slots to enable (1-10)\n        mode (str): Weight control mode:\n            - \"Simple\": Single weight per LoRA\n            - \"Advanced\": Separate model and CLIP weights\n        switch_1..10 (str): \"On\"/\"Off\" toggle for each LoRA slot\n        lora_name_1..10 (str): Name of LoRA model for each slot\n        weight_1..10 (float): Weight value for simple mode (-10.0 to 10.0)\n        model_weight_1..10 (float): Model weight for advanced mode (-10.0 to 10.0)\n        clip_weight_1..10 (float): CLIP weight for advanced mode (-10.0 to 10.0)\n        lora_stack (LORA_STACK, optional): Existing stack to extend\n\n    Returns:\n        tuple[LORA_STACK]: Single-element tuple containing list of configured LoRAs\n\n    Notes:\n        - Each slot can be independently enabled/disabled\n        - Simple mode uses same weight for model and CLIP\n        - Advanced mode allows separate model and CLIP weights\n        - Weights can be negative for inverse effects\n        - Can extend existing LORA_STACK\n        - Disabled or empty slots are skipped\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        loras = [\"None\"] + folder_paths.get_filename_list(\"loras\")\n\n        inputs = {\n            \"required\": {\n                \"num_slots\": ([str(i) for i in range(1, 11)], {\"default\": \"1\"}),\n                \"mode\": ([\"Simple\", \"Advanced\"],),\n            },\n            \"optional\": {\"lora_stack\": (\"LORA_STACK\",)},\n        }\n\n        for i in range(1, 11):\n            inputs[\"optional\"].update(\n                {\n                    f\"switch_{i}\": ([\"On\", \"Off\"],),\n                    f\"lora_name_{i}\": (loras,),\n                    f\"weight_{i}\": (\n                        \"FLOAT\",\n                        {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                    ),\n                    f\"model_weight_{i}\": (\n                        \"FLOAT\",\n                        {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                    ),\n                    f\"clip_weight_{i}\": (\n                        \"FLOAT\",\n                        {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                    ),\n                }\n            )\n\n        return inputs\n\n    RETURN_TYPES = (\"LORA_STACK\",)\n    RETURN_NAMES = (\"lora_stack\",)\n    FUNCTION = \"execute\"\n    CATEGORY = LABS_CAT\n    CLASS_ID = \"lora_stacker\"\n    DESCRIPTION = \"\"\"\n    Manages multiple LoRA models with configurable weights and modes.\n    Provides an interface for stacking up to 10 LoRA models with independent weight controls.\n    Supports simple mode (single weight) or advanced mode (separate model and CLIP weights).\n    \"\"\"\n\n    def execute(self, **kwargs):\n        num_slots = int(kwargs.get(\"num_slots\", 1))\n        mode = kwargs.get(\"mode\", \"Simple\")\n        lora_stack = kwargs.get(\"lora_stack\")\n\n        lora_list: list = []\n        if lora_stack is not None:\n            lora_list.extend([lora for lora in lora_stack if lora[0] is not None and lora[0] != \"\"])\n\n        for i in range(1, num_slots + 1):\n            switch = kwargs.get(f\"switch_{i}\")\n            lora_name = kwargs.get(f\"lora_name_{i}\") or \"None\"\n\n            if lora_name is not None and lora_name != \"None\" and switch == \"On\":\n                if mode == \"Simple\":\n                    weight = float(kwargs.get(f\"weight_{i}\") or 1.0)\n                    lora_list.extend([(lora_name, weight, weight)])\n                else:\n                    model_weight = float(kwargs.get(f\"model_weight_{i}\") or 1.0)\n                    clip_weight = float(kwargs.get(f\"clip_weight_{i}\") or 1.0)\n                    lora_list.extend([(lora_name, model_weight, clip_weight)])\n\n        return (lora_list,)\n</code></pre>"},{"location":"nodes/labs/#dinoheatmap","title":"DINOHeatmap","text":"<p>A ComfyUI node that generates similarity heatmaps using DINO Vision Transformer.</p> <p>This node takes an input image and a template image, optionally with a mask, and produces a heatmap showing regions similar to the template using DINO embeddings.</p>"},{"location":"nodes/labs/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required template <code>IMAGE</code> optional mask <code>MASK</code>"},{"location":"nodes/labs/#returns_2","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class DINOHeatmap:\n    \"\"\"A ComfyUI node that generates similarity heatmaps using DINO Vision Transformer.\n\n    This node takes an input image and a template image, optionally with a mask,\n    and produces a heatmap showing regions similar to the template using DINO embeddings.\n    \"\"\"\n\n    CLASS_ID = \"dino_heatmap\"\n    CATEGORY = LABS_CAT\n    DESCRIPTION = \"\"\"Generates a similarity heatmap between two images using DINO Vision Transformer.\n\n    This node helps you find regions in an image that are similar to a template image. It uses Facebook's DINOv2\n    Vision Transformer model to analyze the images and create a heatmap highlighting matching areas.\n\n    Inputs:\n    - Image: The main image to analyze\n    - Template: The reference image to search for\n    - Mask (optional): A mask to focus the search on specific areas of the template. If provided, mask dimensions (H, W)\n      should be equal to those of the template.\n\n    Output:\n    - A heatmap image where brighter areas indicate stronger similarity to the template\n\n    When a mask is provided, the node will direct attention to specific regions of the template defined by the mask.\n    If no mask is provided, the entire template will be considered for similarity matching.\n\n    This is useful for:\n    - Finding specific objects or patterns in images\n    - Analyzing image composition and recurring elements\n    - Visual pattern matching and comparison\n    - Object localization without traditional object detection\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"template\": (\"IMAGE\",),\n            },\n            \"optional\": {\n                \"mask\": (\"MASK\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n\n    def execute(self, image: Tensor, template: Tensor, mask: Optional[Tensor] = None) -&gt; tuple[Tensor,]:\n        image = TensorImage.from_BWHC(image)\n        template = TensorImage.from_BWHC(template)\n        mask = TensorImage.from_BWHC(mask) if mask is not None else None\n\n        model = DINOSimilarity()\n        output = model.predict(image, template, mask)\n\n        output = TensorImage(output).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/lists/","title":"Lists Nodes","text":""},{"location":"nodes/llm/","title":"Llm Nodes","text":""},{"location":"nodes/loaders/","title":"Loaders Nodes","text":""},{"location":"nodes/logic/","title":"Logic Nodes","text":""},{"location":"nodes/logic/#switch","title":"Switch","text":"<p>Switches between two input values based on a boolean condition.</p> <p>A logic gate that selects between two inputs of any type based on a boolean condition. When the condition is True, it returns the 'true' value; otherwise, it returns the 'false' value. This node is useful for creating conditional workflows and dynamic value selection.</p>"},{"location":"nodes/logic/#inputs","title":"Inputs","text":"Group Name Type Default Extras required on_true <code>any_type</code> lazy=True required on_false <code>any_type</code> lazy=True required condition <code>BOOLEAN</code> True Source code <pre><code>class Switch:\n    \"\"\"Switches between two input values based on a boolean condition.\n\n    A logic gate that selects between two inputs of any type based on a boolean condition. When the\n    condition is True, it returns the 'true' value; otherwise, it returns the 'false' value. This node\n    is useful for creating conditional workflows and dynamic value selection.\n\n    Args:\n        condition (bool): The boolean condition that determines which value to return.\n            Defaults to False if not provided.\n        on_true (Any): The value to return when the condition is True. Can be of any type.\n        on_false (Any): The value to return when the condition is False. Can be of any type.\n\n    Returns:\n        tuple[Any]: A single-element tuple containing either the 'true' or 'false' value based on\n            the condition.\n\n    Notes:\n        - The node accepts inputs of any type, making it versatile for different data types\n        - Both 'on_true' and 'on_false' values must be provided\n        - The condition is automatically cast to boolean, with None being treated as False\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"on_true\": (any_type, {\"lazy\": True}),\n                \"on_false\": (any_type, {\"lazy\": True}),\n                \"condition\": (\"BOOLEAN\", {\"default\": True}),\n            }\n        }\n\n    RETURN_TYPES = (any_type,)\n    RETURN_NAMES = (\"output\",)\n    FUNCTION = \"execute\"\n    CATEGORY = LOGIC_CAT\n    DESCRIPTION = \"\"\"\n    Switches between two input values based on a boolean condition.\n    When the condition is True, returns the 'true' value; otherwise, returns the 'false' value.\n    Useful for creating conditional workflows and dynamic value selection.\n    \"\"\"\n\n    def check_lazy_status(self, condition: bool, on_true: Any = None, on_false: Any = None) -&gt; Any:\n        if condition and on_true is None:\n            return [\"on_true\"]\n        if not condition and on_false is None:\n            return [\"on_false\"]\n        return None\n\n    def execute(self, on_true: Any, on_false: Any, condition: bool = True) -&gt; tuple[Any]:\n        return (on_true if condition else on_false,)\n</code></pre>"},{"location":"nodes/logic/#compare","title":"Compare","text":"<p>Compares two input values based on a specified comparison operation.</p> <p>A logic gate that evaluates a comparison between two inputs of any type. The comparison is determined by the specified operation, which can include equality, inequality, and relational comparisons. This node is useful for implementing conditional logic based on the relationship between two values.</p>"},{"location":"nodes/logic/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required a <code>any_type</code> required b <code>any_type</code> required comparison <code>compare_functions</code> a == b"},{"location":"nodes/logic/#returns","title":"Returns","text":"Name Type boolean <code>BOOLEAN</code> Source code <pre><code>class Compare:\n    \"\"\"Compares two input values based on a specified comparison operation.\n\n    A logic gate that evaluates a comparison between two inputs of any type. The comparison is determined\n    by the specified operation, which can include equality, inequality, and relational comparisons. This\n    node is useful for implementing conditional logic based on the relationship between two values.\n\n    Args:\n        a (Any): The first value to compare. Can be of any type.\n        b (Any): The second value to compare. Can be of any type.\n        comparison (str): The comparison operation to perform. Defaults to \"a == b\".\n            Available options include:\n            - \"a == b\": Checks if a is equal to b.\n            - \"a != b\": Checks if a is not equal to b.\n            - \"a &lt; b\": Checks if a is less than b.\n            - \"a &gt; b\": Checks if a is greater than b.\n            - \"a &lt;= b\": Checks if a is less than or equal to b.\n            - \"a &gt;= b\": Checks if a is greater than or equal to b.\n\n    Returns:\n        tuple[bool]: A single-element tuple containing the result of the comparison as a boolean value.\n\n    Notes:\n        - The node accepts inputs of any type, making it versatile for different data types.\n        - If the inputs are tensors, lists, or tuples,\n          the comparison will be evaluated based on their shapes or lengths.\n        - The output will be cast to a boolean value.\n    \"\"\"\n\n    COMPARE_FUNCTIONS = {\n        \"a == b\": lambda a, b: a == b,\n        \"a != b\": lambda a, b: a != b,\n        \"a &lt; b\": lambda a, b: a &lt; b,\n        \"a &gt; b\": lambda a, b: a &gt; b,\n        \"a &lt;= b\": lambda a, b: a &lt;= b,\n        \"a &gt;= b\": lambda a, b: a &gt;= b,\n    }\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        compare_functions = list(cls.COMPARE_FUNCTIONS.keys())\n        return {\n            \"required\": {\n                \"a\": (any_type,),\n                \"b\": (any_type,),\n                \"comparison\": (compare_functions, {\"default\": \"a == b\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"BOOLEAN\",)\n    RETURN_NAMES = (\"result\",)\n    FUNCTION = \"execute\"\n    CATEGORY = LOGIC_CAT\n    DESCRIPTION = \"\"\"\n    Compares two input values based on a specified comparison operation.\n    Evaluates equality, inequality, and relational comparisons between inputs of any type.\n    Handles special cases for tensors, lists, and tuples.\n    \"\"\"\n\n    def execute(self, a: Any, b: Any, comparison: str = \"a == b\") -&gt; tuple[bool]:\n        try:\n            output = self.COMPARE_FUNCTIONS[comparison](a, b)\n        except Exception as e:\n            if isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):\n                output = self.COMPARE_FUNCTIONS[comparison](a.shape, b.shape)\n            elif isinstance(a, (list, tuple)) and isinstance(b, (list, tuple)):\n                output = self.COMPARE_FUNCTIONS[comparison](len(a), len(b))\n            else:\n                raise e\n\n        if isinstance(output, torch.Tensor):\n            output = output.all().item()\n        elif isinstance(output, (list, tuple)):\n            output = all(output)\n\n        return (bool(output),)\n</code></pre>"},{"location":"nodes/lora/","title":"Lora Nodes","text":""},{"location":"nodes/lora/#applylorastack","title":"ApplyLoraStack","text":"<p>Applies multiple LoRA models sequentially to a base model and CLIP in ComfyUI.</p> <p>This node takes a base model, CLIP, and a stack of LoRA models as input. It applies each LoRA in the stack sequentially using specified weights for both model and CLIP components.</p>"},{"location":"nodes/lora/#inputs","title":"Inputs","text":"Group Name Type Default Extras required model <code>MODEL</code> required clip <code>CLIP</code> required lora_stack <code>LORA_STACK</code>"},{"location":"nodes/lora/#returns","title":"Returns","text":"Name Type model <code>MODEL</code> clip <code>CLIP</code> Source code <pre><code>class ApplyLoraStack:\n    \"\"\"Applies multiple LoRA models sequentially to a base model and CLIP in ComfyUI.\n\n    This node takes a base model, CLIP, and a stack of LoRA models as input. It applies each LoRA\n    in the stack sequentially using specified weights for both model and CLIP components.\n\n    Args:\n        model (MODEL): The base Stable Diffusion model to modify\n        clip (CLIP): The CLIP model to modify\n        lora_stack (LORA_STACK): A list of tuples containing (lora_name, model_weight, clip_weight)\n\n    Returns:\n        tuple:\n            - MODEL: The modified Stable Diffusion model with all LoRAs applied\n            - CLIP: The modified CLIP model with all LoRAs applied\n\n    Notes:\n        - LoRAs are applied in sequence, with each modification building on previous changes\n        - If lora_stack is None, returns the original model and CLIP unchanged\n        - Uses ComfyUI's built-in LoRA loading and application mechanisms\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"model\": (\"MODEL\",),\n                \"clip\": (\"CLIP\",),\n                \"lora_stack\": (\"LORA_STACK\",),\n            }\n        }\n\n    RETURN_TYPES = (\n        \"MODEL\",\n        \"CLIP\",\n    )\n    RETURN_NAMES = (\n        \"MODEL\",\n        \"CLIP\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = LORA_CAT\n    DESCRIPTION = \"\"\"\n    Applies multiple LoRA models sequentially to a base model and CLIP.\n    Processes each LoRA in the stack with specified weights for both model and CLIP components.\n    Useful for combining multiple style or concept adaptations in a single workflow.\n    \"\"\"\n\n    def execute(\n        self,\n        model: Any,\n        clip: Any,\n        lora_stack: Optional[list] = None,\n    ):\n        if lora_stack is None:\n            return (\n                model,\n                clip,\n            )\n\n        loras = []\n        model_lora = model\n        clip_lora = clip\n        loras.extend(lora_stack)\n\n        for lora in loras:\n            lora_name, strength_model, strength_clip = lora\n\n            lora_path = folder_paths.get_full_path(\"loras\", lora_name)\n            lora = utils.load_torch_file(lora_path, safe_load=True)\n\n            model_lora, clip_lora = sd.load_lora_for_models(model_lora, clip_lora, lora, strength_model, strength_clip)\n\n        return (\n            model_lora,\n            clip_lora,\n        )\n</code></pre>"},{"location":"nodes/lora/#dict2lorastack","title":"Dict2LoraStack","text":"<p>Converts a list of LoRA configuration dictionaries into a LORA_STACK format.</p> <p>Transforms a list of dictionaries containing LoRA configurations into the tuple format required for LORA_STACK operations. Can optionally extend an existing stack.</p>"},{"location":"nodes/lora/#returns_1","title":"Returns","text":"Name Type lora_stack <code>LORA_STACK</code> Source code <pre><code>class Dict2LoraStack:\n    \"\"\"Converts a list of LoRA configuration dictionaries into a LORA_STACK format.\n\n    Transforms a list of dictionaries containing LoRA configurations into the tuple format required\n    for LORA_STACK operations. Can optionally extend an existing stack.\n\n    Args:\n        lora_dicts (LIST): List of dictionaries, each containing:\n            - lora_name (str): Name of the LoRA model\n            - lora_weight (float): Weight to apply to both model and CLIP\n        lora_stack (LORA_STACK, optional): Existing stack to extend\n\n    Returns:\n        tuple:\n            - LORA_STACK: List of tuples (lora_name, model_weight, clip_weight)\n\n    Raises:\n        ValueError: If lora_dicts is not a list\n\n    Notes:\n        - Uses same weight for both model and CLIP components\n        - Filters out any \"None\" entries when extending existing stack\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"required\": {\n                \"lora_dicts\": (\"LIST\",),\n            },\n            \"optional\": {\"lora_stack\": (\"LORA_STACK\",)},\n        }\n        return inputs\n\n    RETURN_TYPES = (\"LORA_STACK\",)\n    FUNCTION = \"execute\"\n    CATEGORY = LORA_CAT\n    CLASS_ID = \"dict_to_lora_stack\"\n    DESCRIPTION = \"\"\"\n    Converts a list of LoRA configuration dictionaries into a LORA_STACK format.\n    Transforms dictionaries with lora_name and lora_weight into tuples for LORA_STACK operations.\n    Can optionally extend an existing stack.\n    \"\"\"\n\n    def execute(self, lora_dicts: list, lora_stack: Optional[list] = None):\n        loras: list[Optional[tuple]] = [None for _ in lora_dicts]\n\n        for idx, lora_dict in enumerate(lora_dicts):\n            loras[idx] = (\n                lora_dict[\"lora_name\"],\n                lora_dict[\"lora_weight\"],\n                lora_dict[\"lora_weight\"],\n            )  # type: ignore\n\n        # If lora_stack is not None, extend the loras list with lora_stack\n        if lora_stack is not None:\n            loras.extend([lora for lora in lora_stack if lora[0] != \"None\"])\n\n        return (loras,)\n</code></pre>"},{"location":"nodes/lora/#lorastack","title":"LoraStack","text":"<p>Creates a configurable stack of up to 3 LoRA models with adjustable weights.</p> <p>Provides a user interface to enable/disable and configure up to three LoRA models with independent weights for both model and CLIP components. Can extend an existing LORA_STACK.</p>"},{"location":"nodes/lora/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required switch_1 <code>LIST</code> required lora_name_1 <code>loras</code> required model_weight_1 <code>FLOAT</code> 1.0 max=10.0, step=0.01 required clip_weight_1 <code>FLOAT</code> 1.0 max=10.0, step=0.01 required switch_2 <code>LIST</code> required lora_name_2 <code>loras</code> required model_weight_2 <code>FLOAT</code> 1.0 max=10.0, step=0.01 required clip_weight_2 <code>FLOAT</code> 1.0 max=10.0, step=0.01 required switch_3 <code>LIST</code> required lora_name_3 <code>loras</code> required model_weight_3 <code>FLOAT</code> 1.0 max=10.0, step=0.01 required clip_weight_3 <code>FLOAT</code> 1.0 max=10.0, step=0.01 optional lora_stack <code>LORA_STACK</code>"},{"location":"nodes/lora/#returns_2","title":"Returns","text":"Name Type lora_stack <code>LORA_STACK</code> Source code <pre><code>class LoraStack:\n    \"\"\"Creates a configurable stack of up to 3 LoRA models with adjustable weights.\n\n    Provides a user interface to enable/disable and configure up to three LoRA models with independent\n    weights for both model and CLIP components. Can extend an existing LORA_STACK.\n\n    Args:\n        switch_1/2/3 (str): \"On\" or \"Off\" to enable/disable each LoRA\n        lora_name_1/2/3 (str): Names of LoRA models to use\n        model_weight_1/2/3 (float): Weight multipliers for model component (-10.0 to 10.0)\n        clip_weight_1/2/3 (float): Weight multipliers for CLIP component (-10.0 to 10.0)\n        lora_stack (LORA_STACK, optional): Existing stack to extend\n\n    Returns:\n        tuple:\n            - LORA_STACK: List of tuples (lora_name, model_weight, clip_weight)\n\n    Notes:\n        - Each LoRA can be independently enabled/disabled\n        - Weights can be negative for inverse effects\n        - Only enabled LoRAs with valid names (not \"None\") are included in output\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        loras = [\"None\"] + folder_paths.get_filename_list(\"loras\")\n\n        return {\n            \"required\": {\n                \"switch_1\": ([\"Off\", \"On\"],),\n                \"lora_name_1\": (loras,),\n                \"model_weight_1\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                ),\n                \"clip_weight_1\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                ),\n                \"switch_2\": ([\"Off\", \"On\"],),\n                \"lora_name_2\": (loras,),\n                \"model_weight_2\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                ),\n                \"clip_weight_2\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                ),\n                \"switch_3\": ([\"Off\", \"On\"],),\n                \"lora_name_3\": (loras,),\n                \"model_weight_3\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                ),\n                \"clip_weight_3\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01},\n                ),\n            },\n            \"optional\": {\"lora_stack\": (\"LORA_STACK\",)},\n        }\n\n    RETURN_TYPES = (\"LORA_STACK\",)\n    RETURN_NAMES = (\"lora_stack\",)\n    FUNCTION = \"execute\"\n    CATEGORY = LORA_CAT\n    CLASS_NAME = \"LoraStack(OLD)\"\n    DEPRECATED = True\n    DESCRIPTION = \"\"\"\n    Creates a configurable stack of up to 3 LoRA models with adjustable weights.\n    Provides controls to enable/disable and configure each LoRA with independent weights for model and CLIP components.\n    Can extend an existing stack.\n    \"\"\"\n\n    def execute(\n        self,\n        **kwargs,\n    ):\n        switch_1 = kwargs.get(\"switch_1\")\n        lora_name_1 = kwargs.get(\"lora_name_1\")\n        model_weight_1 = kwargs.get(\"model_weight_1\")\n        clip_weight_1 = kwargs.get(\"clip_weight_1\")\n        switch_2 = kwargs.get(\"switch_2\")\n        lora_name_2 = kwargs.get(\"lora_name_2\")\n        model_weight_2 = kwargs.get(\"model_weight_2\")\n        clip_weight_2 = kwargs.get(\"clip_weight_2\")\n        switch_3 = kwargs.get(\"switch_3\")\n        lora_name_3 = kwargs.get(\"lora_name_3\")\n        model_weight_3 = kwargs.get(\"model_weight_3\")\n        clip_weight_3 = kwargs.get(\"clip_weight_3\")\n        lora_stack = kwargs.get(\"lora_stack\")\n\n        lora_list: list = []\n        if lora_stack is not None:\n            lora_list.extend([lora for lora in lora_stack if lora[0] != \"None\"])\n\n        if lora_name_1 != \"None\" and switch_1 == \"On\":\n            (lora_list.extend([(lora_name_1, model_weight_1, clip_weight_1)]),)  # type: ignore\n\n        if lora_name_2 != \"None\" and switch_2 == \"On\":\n            (lora_list.extend([(lora_name_2, model_weight_2, clip_weight_2)]),)  # type: ignore\n\n        if lora_name_3 != \"None\" and switch_3 == \"On\":\n            (lora_list.extend([(lora_name_3, model_weight_3, clip_weight_3)]),)  # type: ignore\n\n        return (lora_list,)\n</code></pre>"},{"location":"nodes/lora/#buildloradataset","title":"BuildLoraDataset","text":"<p>Saves images and captions in a format suitable for LoRA training.</p> <p>Creates a structured dataset directory containing images and their corresponding caption files, with support for multiple captions and optional text modifications.</p>"},{"location":"nodes/lora/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required images <code>IMAGE</code> required labels <code>LIST</code> forceInput=True required dataset_name <code>STRING</code> dataset required repeats <code>INT</code> 5 min=1 required training_backend <code>LIST</code> optional prefix <code>STRING</code> optional suffix <code>STRING</code>"},{"location":"nodes/lora/#returns_3","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class BuildLoraDataset:\n    \"\"\"Saves images and captions in a format suitable for LoRA training.\n\n    Creates a structured dataset directory containing images and their corresponding caption files,\n    with support for multiple captions and optional text modifications.\n\n    Args:\n        dataset_name (str): Name for the dataset folder\n        repeats (int): Number of times to repeat the dataset (min: 1)\n        images (IMAGE): Tensor containing the images to save\n        labels (str): Caption text, multiple captions separated by newlines\n        prefix (str, optional): Text to add before each caption\n        suffix (str, optional): Text to add after each caption\n\n    Returns:\n        tuple:\n            - str: Path to the created dataset folder\n\n    Raises:\n        ValueError: If any input parameters are of incorrect type\n\n    Notes:\n        - Creates folder structure: comfy/loras_datasets/dataset_name_uuid/repeats_dataset_name/\n        - Saves images as PNG files with corresponding .txt caption files\n        - Supports multiple captions via newline separation\n        - Includes UUID in folder name for uniqueness\n        - Creates parent directories if they don't exist\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                \"labels\": (\"LIST\", {\"forceInput\": True}),\n                \"dataset_name\": (\"STRING\", {\"default\": \"dataset\"}),\n                \"repeats\": (\"INT\", {\"default\": 5, \"min\": 1}),\n                \"training_backend\": ([\"ai-toolkit\", \"simpletuner\"],),\n            },\n            \"optional\": {\n                \"prefix\": (\"STRING\", {\"default\": \"\"}),\n                \"suffix\": (\"STRING\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    RETURN_NAMES = (\"folder_path\",)\n    OUTPUT_NODE = True\n    FUNCTION = \"execute\"\n    CATEGORY = LORA_CAT\n    DESCRIPTION = \"\"\"\n    Saves images and captions in a format suitable for LoRA training.\n    Creates a structured dataset directory with images and corresponding caption files.\n    Supports multiple captions, repeats, and optional text modifications with prefix/suffix.\n    \"\"\"\n\n    # Add a class variable to track the counter across function calls\n    _counter = 0\n\n    def execute(\n        self,\n        images: torch.Tensor,\n        labels: list[str],\n        dataset_name: str = \"dataset\",\n        repeats: int = 5,\n        prefix: Optional[str] = \"\",\n        suffix: Optional[str] = \"\",\n        training_backend: str = \"ai-toolkit\",\n    ) -&gt; tuple[str]:\n        print(\"dataset_name\", dataset_name, type(dataset_name))\n        if len(dataset_name) == 0:\n            dataset_name = \"dataset\"\n        root_folder = os.path.join(BASE_COMFY_DIR, \"loras_datasets\")\n        if not os.path.exists(root_folder):\n            os.mkdir(root_folder)\n\n        dataset_folder = None\n        if training_backend == \"ai-toolkit\":\n            uuid = uuid7str()\n            dataset_folder = os.path.join(root_folder, f\"{dataset_name}_{uuid}\")\n            if not os.path.exists(dataset_folder):\n                os.mkdir(dataset_folder)\n            images_folder = os.path.join(dataset_folder, f\"{repeats}_{dataset_name}\")\n            if not os.path.exists(images_folder):\n                os.mkdir(images_folder)\n\n            tensor_images = TensorImage.from_BWHC(images)\n            for i, img in enumerate(tensor_images):\n                # timestamp to be added to the image name\n\n                TensorImage(img).save(os.path.join(images_folder, f\"{dataset_name}_{i}.png\"))\n                # write txt label with the same name of the image\n                with open(os.path.join(images_folder, f\"{dataset_name}_{i}.txt\"), \"w\") as f:\n                    label = prefix + labels[self._counter % len(labels)] + suffix  # type: ignore\n                    f.write(label)\n                self._counter += 1\n        elif training_backend == \"simpletuner\":\n            dataset_folder = os.path.join(root_folder, dataset_name)\n            if not os.path.exists(dataset_folder):\n                os.mkdir(dataset_folder)\n\n            tensor_images = TensorImage.from_BWHC(images)\n            for i, img in enumerate(tensor_images):\n                uuid = uuid7str()\n                # Save images directly to the dataset folder\n                TensorImage(img).save(os.path.join(dataset_folder, f\"{dataset_name}_{uuid}.png\"))\n\n                # Write txt label with the same name of the image\n                with open(os.path.join(dataset_folder, f\"{dataset_name}_{uuid}.txt\"), \"w\") as f:\n                    # Use modulo to cycle through labels if needed\n                    label = str(prefix) + labels[self._counter % len(labels)] + str(suffix)\n                    f.write(label)\n\n                self._counter += 1\n\n        return (str(dataset_folder),)\n</code></pre>"},{"location":"nodes/mask/","title":"Mask Nodes","text":""},{"location":"nodes/mask/#maskmorphology","title":"MaskMorphology","text":"<p>Applies morphological operations to transform mask shapes and boundaries.</p> <p>Provides various morphological operations to modify mask shapes through kernel-based transformations. Supports multiple iterations for stronger effects.</p>"},{"location":"nodes/mask/#inputs","title":"Inputs","text":"Group Name Type Default Extras required mask <code>MASK</code> required operation <code>LIST</code> required kernel_size <code>INT</code> 1 min=1, step=2 required iterations <code>INT</code> 5 min=1, step=1"},{"location":"nodes/mask/#returns","title":"Returns","text":"Name Type mask <code>MASK</code> Source code <pre><code>class MaskMorphology:\n    \"\"\"Applies morphological operations to transform mask shapes and boundaries.\n\n    Provides various morphological operations to modify mask shapes through kernel-based transformations.\n    Supports multiple iterations for stronger effects.\n\n    Args:\n        mask (torch.Tensor): Input mask tensor in BWHC format\n        operation (str): Morphological operation to apply. Options:\n            - \"dilation\": Expands mask regions\n            - \"erosion\": Shrinks mask regions\n            - \"opening\": Erosion followed by dilation\n            - \"closing\": Dilation followed by erosion\n            - \"gradient\": Difference between dilation and erosion\n            - \"top_hat\": Difference between input and opening\n            - \"bottom_hat\": Difference between closing and input\n        kernel_size (int): Size of the morphological kernel. Default: 1\n        iterations (int): Number of times to apply the operation. Default: 5\n\n    Returns:\n        tuple[torch.Tensor]: A single-element tuple containing the processed mask in BWHC format\n\n    Raises:\n        ValueError: If mask is not a valid torch.Tensor or if operation is invalid\n\n    Notes:\n        - Larger kernel sizes and more iterations result in stronger morphological effects\n        - Operations are performed using the TensorImage wrapper class for format consistency\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"operation\": (\n                    [\n                        \"dilation\",\n                        \"erosion\",\n                        \"opening\",\n                        \"closing\",\n                        \"gradient\",\n                        \"top_hat\",\n                        \"bottom_hat\",\n                    ],\n                ),\n                \"kernel_size\": (\n                    \"INT\",\n                    {\"default\": 1, \"min\": 1, \"max\": MAX_INT, \"step\": 2},\n                ),\n                \"iterations\": (\n                    \"INT\",\n                    {\"default\": 5, \"min\": 1, \"max\": MAX_INT, \"step\": 1},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Applies morphological operations to transform mask shapes and boundaries.\n    Provides operations like dilation (expand), erosion (shrink), opening, closing, gradient, and more.\n    Useful for refining mask edges and shapes.\n    \"\"\"\n\n    def execute(\n        self,\n        mask: torch.Tensor,\n        operation: str = \"dilation\",\n        kernel_size: int = 1,\n        iterations: int = 5,\n    ) -&gt; tuple[torch.Tensor]:\n        step = TensorImage.from_BWHC(mask)\n\n        operations = {\n            \"dilation\": dilation,\n            \"erosion\": erosion,\n            \"opening\": opening,\n            \"closing\": closing,\n            \"gradient\": gradient,\n            \"top_hat\": top_hat,\n            \"bottom_hat\": bottom_hat,\n        }\n\n        if operation not in operations:\n            raise ValueError(f\"Invalid operation: {operation}\")\n\n        try:\n            output = operations[operation](image=step, kernel_size=kernel_size, iterations=iterations)\n        except KeyError:\n            raise ValueError(f\"Invalid operation: {operation}\")\n\n        return (output.get_BWHC(),)\n</code></pre>"},{"location":"nodes/mask/#mask2trimap","title":"Mask2Trimap","text":"<p>Converts a binary mask into a trimap representation with three distinct regions.</p> <p>Creates a trimap by identifying definite foreground, definite background, and uncertain regions using threshold values and morphological operations.</p>"},{"location":"nodes/mask/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required mask <code>MASK</code> required inner_min_threshold <code>INT</code> 200 min=0, max=255 required inner_max_threshold <code>INT</code> 255 min=0, max=255 required outer_min_threshold <code>INT</code> 15 min=0, max=255 required outer_max_threshold <code>INT</code> 240 min=0, max=255 required kernel_size <code>INT</code> 10 min=1, max=100"},{"location":"nodes/mask/#returns_1","title":"Returns","text":"Name Type mask <code>MASK</code> trimap <code>TRIMAP</code> Source code <pre><code>class Mask2Trimap:\n    \"\"\"Converts a binary mask into a trimap representation with three distinct regions.\n\n    Creates a trimap by identifying definite foreground, definite background, and uncertain regions\n    using threshold values and morphological operations.\n\n    Args:\n        mask (torch.Tensor): Input binary mask in BWHC format\n        inner_min_threshold (int): Minimum threshold for inner/foreground region. Default: 200\n        inner_max_threshold (int): Maximum threshold for inner/foreground region. Default: 255\n        outer_min_threshold (int): Minimum threshold for outer/background region. Default: 15\n        outer_max_threshold (int): Maximum threshold for outer/background region. Default: 240\n        kernel_size (int): Size of morphological kernel for region processing. Default: 10\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: Tuple containing:\n            - Processed mask in BWHC format\n            - Trimap tensor with foreground, background, and uncertain regions\n\n    Raises:\n        ValueError: If mask is not a valid torch.Tensor\n\n    Notes:\n        - Output trimap has values: 0 (background), 0.5 (uncertain), 1 (foreground)\n        - Kernel size affects the smoothness of region boundaries\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"inner_min_threshold\": (\"INT\", {\"default\": 200, \"min\": 0, \"max\": 255}),\n                \"inner_max_threshold\": (\"INT\", {\"default\": 255, \"min\": 0, \"max\": 255}),\n                \"outer_min_threshold\": (\"INT\", {\"default\": 15, \"min\": 0, \"max\": 255}),\n                \"outer_max_threshold\": (\"INT\", {\"default\": 240, \"min\": 0, \"max\": 255}),\n                \"kernel_size\": (\"INT\", {\"default\": 10, \"min\": 1, \"max\": 100}),\n            }\n        }\n\n    RETURN_TYPES = (\"MASK\", \"TRIMAP\")\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    CLASS_ID = \"mask_trimap\"\n    DESCRIPTION = \"\"\"\n    Converts a binary mask into a trimap representation with three distinct regions.\n    Creates a trimap by identifying definite foreground, definite background,\n    and uncertain regions using thresholds and morphological operations.\n    \"\"\"\n\n    def execute(\n        self,\n        mask: torch.Tensor,\n        inner_min_threshold: int = 200,\n        inner_max_threshold: int = 255,\n        outer_min_threshold: int = 15,\n        outer_max_threshold: int = 240,\n        kernel_size: int = 10,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        step = TensorImage.from_BWHC(mask)\n        inner_mask = TensorImage(step.clone())\n        inner_mask[inner_mask &gt; (inner_max_threshold / 255.0)] = 1.0\n        inner_mask[inner_mask &lt;= (inner_min_threshold / 255.0)] = 0.0\n\n        step = TensorImage.from_BWHC(mask)\n        inner_mask = erosion(image=inner_mask, kernel_size=kernel_size, iterations=1)\n\n        inner_mask[inner_mask != 0.0] = 1.0\n\n        outter_mask = step.clone()\n        outter_mask[outter_mask &gt; (outer_max_threshold / 255.0)] = 1.0\n        outter_mask[outter_mask &lt;= (outer_min_threshold / 255.0)] = 0.0\n        outter_mask = dilation(image=inner_mask, kernel_size=kernel_size, iterations=5)\n\n        outter_mask[outter_mask != 0.0] = 1.0\n\n        trimap_im = torch.zeros_like(step)\n        trimap_im[outter_mask == 1.0] = 0.5\n        trimap_im[inner_mask == 1.0] = 1.0\n        batch_size = step.shape[0]\n\n        trimap = torch.zeros(\n            batch_size,\n            2,\n            step.shape[2],\n            step.shape[3],\n            dtype=step.dtype,\n            device=step.device,\n        )\n        for i in range(batch_size):\n            tar_trimap = trimap_im[i][0]\n            trimap[i][1][tar_trimap == 1] = 1\n            trimap[i][0][tar_trimap == 0] = 1\n\n        output_0 = TensorImage(trimap_im).get_BWHC()\n        output_1 = trimap.permute(0, 2, 3, 1)\n\n        return (\n            output_0,\n            output_1,\n        )\n</code></pre>"},{"location":"nodes/mask/#basemask","title":"BaseMask","text":"<p>Creates a basic binary mask with specified dimensions.</p> <p>A utility class that generates a simple binary mask (black or white) with user-defined dimensions. The mask is returned in BWHC (Batch, Width, Height, Channel) format.</p>"},{"location":"nodes/mask/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required color <code>LIST</code> white required width <code>INT</code> 1024 min=1, step=1 required height <code>INT</code> 1024 min=1, step=1"},{"location":"nodes/mask/#returns_2","title":"Returns","text":"Name Type mask <code>MASK</code> Source code <pre><code>class BaseMask:\n    \"\"\"Creates a basic binary mask with specified dimensions.\n\n    A utility class that generates a simple binary mask (black or white) with user-defined dimensions.\n    The mask is returned in BWHC (Batch, Width, Height, Channel) format.\n\n    Args:\n        color (str): The mask color. Options:\n            - \"white\": Creates a mask filled with ones\n            - \"black\": Creates a mask filled with zeros\n        width (int): Width of the output mask in pixels. Default: 1024\n        height (int): Height of the output mask in pixels. Default: 1024\n\n    Returns:\n        tuple[torch.Tensor]: A single-element tuple containing the binary mask tensor in BWHC format\n\n    Raises:\n        None\n\n    Notes:\n        - The output mask will have dimensions (1, 1, height, width) before BWHC conversion\n        - All values in the mask are either 0 (black) or 1 (white)\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"color\": ([\"white\", \"black\"], {\"default\": \"white\"}),\n                \"width\": (\n                    \"INT\",\n                    {\"default\": 1024, \"min\": 1, \"max\": MAX_INT, \"step\": 1},\n                ),\n                \"height\": (\n                    \"INT\",\n                    {\"default\": 1024, \"min\": 1, \"max\": MAX_INT, \"step\": 1},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Creates a basic binary mask with specified dimensions.\n    Generates a simple black or white mask with user-defined width and height.\n    Useful as a starting point for more complex mask operations.\n    \"\"\"\n\n    def execute(self, color: str = \"white\", width: int = 1024, height: int = 1024) -&gt; tuple[torch.Tensor]:\n        if color == \"white\":\n            mask = torch.ones(1, 1, height, width)\n        else:\n            mask = torch.zeros(1, 1, height, width)\n        mask = TensorImage(mask).get_BWHC()\n        return (mask,)\n</code></pre>"},{"location":"nodes/mask/#maskinvert","title":"MaskInvert","text":"<p>Inverts a binary mask by flipping all values.</p> <p>Creates a negative version of the input mask where white becomes black and vice versa.</p>"},{"location":"nodes/mask/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required mask <code>MASK</code>"},{"location":"nodes/mask/#returns_3","title":"Returns","text":"Name Type mask <code>MASK</code> Source code <pre><code>class MaskInvert:\n    \"\"\"Inverts a binary mask by flipping all values.\n\n    Creates a negative version of the input mask where white becomes black and vice versa.\n\n    Args:\n        mask (torch.Tensor): Input mask in BWHC format\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing the inverted mask\n\n    Raises:\n        ValueError: If mask is not a valid torch.Tensor\n\n    Notes:\n        - Each pixel value is subtracted from 1.0\n        - Useful for creating negative space masks\n        - Preserves the mask's dimensions and format\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Inverts a binary mask by flipping all values.\n    Creates a negative version of the input mask where white becomes black and vice versa.\n    Useful for creating negative space masks or reversing selection areas.\n    \"\"\"\n\n    def execute(self, mask: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        step = TensorImage.from_BWHC(mask)\n        step = 1.0 - step\n        output = TensorImage(step).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/mask/#getmaskshape","title":"GetMaskShape","text":"<p>Analyzes and returns the dimensional information of a mask tensor.</p> <p>Extracts and returns the shape parameters of the input mask for analysis or debugging.</p>"},{"location":"nodes/mask/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required mask <code>MASK</code>"},{"location":"nodes/mask/#returns_4","title":"Returns","text":"Name Type int <code>INT</code> int <code>INT</code> int <code>INT</code> int <code>INT</code> string <code>STRING</code> Source code <pre><code>class GetMaskShape:\n    \"\"\"Analyzes and returns the dimensional information of a mask tensor.\n\n    Extracts and returns the shape parameters of the input mask for analysis or debugging.\n\n    Args:\n        mask (torch.Tensor): Input mask in BWHC format\n\n    Returns:\n        tuple[int, int, int, int, str]: Tuple containing:\n            - Batch size\n            - Width\n            - Height\n            - Number of channels\n            - String representation of shape\n\n    Raises:\n        ValueError: If mask is not a valid torch.Tensor\n\n    Notes:\n        - Handles both 3D and 4D tensor inputs\n        - Useful for debugging and validation\n        - Returns dimensions in a consistent order regardless of input format\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n            },\n        }\n\n    RETURN_TYPES = (\"INT\", \"INT\", \"INT\", \"INT\", \"STRING\")\n    RETURN_NAMES = (\"batch\", \"width\", \"height\", \"channels\", \"debug\")\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Analyzes and returns the dimensional information of a mask tensor.\n    Extracts shape parameters (batch size, width, height, channels) for analysis or debugging.\n    Handles both 3D and 4D tensor inputs.\n    \"\"\"\n\n    def execute(self, mask: torch.Tensor) -&gt; tuple[int, int, int, int, str]:\n        if len(mask.shape) == 3:\n            return (mask.shape[0], mask.shape[2], mask.shape[1], 1, str(mask.shape))\n        return (\n            mask.shape[0],\n            mask.shape[2],\n            mask.shape[1],\n            mask.shape[3],\n            str(mask.shape),\n        )\n</code></pre>"},{"location":"nodes/mask/#maskdistance","title":"MaskDistance","text":"<p>Calculates the Euclidean distance between two binary masks.</p> <p>Computes the average pixel-wise Euclidean distance between two masks, useful for comparing mask similarity or differences.</p>"},{"location":"nodes/mask/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required mask_0 <code>MASK</code> required mask_1 <code>MASK</code>"},{"location":"nodes/mask/#returns_5","title":"Returns","text":"Name Type float <code>FLOAT</code> Source code <pre><code>class MaskDistance:\n    \"\"\"Calculates the Euclidean distance between two binary masks.\n\n    Computes the average pixel-wise Euclidean distance between two masks, useful for comparing\n    mask similarity or differences.\n\n    Args:\n        mask_0 (torch.Tensor): First input mask in BWHC format\n        mask_1 (torch.Tensor): Second input mask in BWHC format\n\n    Returns:\n        tuple[float]: A single-element tuple containing the computed distance value\n\n    Raises:\n        ValueError: If either mask_0 or mask_1 is not a valid torch.Tensor\n\n    Notes:\n        - Distance is calculated as the root mean square difference between mask pixels\n        - Output is normalized and returned as a single float value\n        - Smaller values indicate more similar masks\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\"required\": {\"mask_0\": (\"MASK\",), \"mask_1\": (\"MASK\",)}}\n\n    RETURN_TYPES = (\"FLOAT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Calculates the Euclidean distance between two binary masks.\n    Computes the average pixel-wise difference between masks, providing a numerical measure of similarity.\n    Smaller values indicate more similar masks.\n    \"\"\"\n\n    def execute(self, mask_0: torch.Tensor, mask_1: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        tensor1 = TensorImage.from_BWHC(mask_0)\n        tensor2 = TensorImage.from_BWHC(mask_1)\n\n        try:\n            dist = torch.Tensor((tensor1 - tensor2).pow(2).sum(3).sqrt().mean())\n        except RuntimeError:\n            raise ValueError(\"Invalid mask dimensions\")\n        return (dist,)\n</code></pre>"},{"location":"nodes/mask/#maskbinaryfilter","title":"MaskBinaryFilter","text":"<p>Applies binary thresholding to convert a grayscale mask into a binary mask.</p> <p>Converts all values above threshold to 1 and below threshold to 0, creating a strict binary mask.</p>"},{"location":"nodes/mask/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required mask <code>MASK</code> required threshold <code>FLOAT</code> 0.01 min=0.0, max=1.0, step=0.01"},{"location":"nodes/mask/#returns_6","title":"Returns","text":"Name Type mask <code>MASK</code> Source code <pre><code>class MaskBinaryFilter:\n    \"\"\"Applies binary thresholding to convert a grayscale mask into a binary mask.\n\n    Converts all values above threshold to 1 and below threshold to 0, creating a strict\n    binary mask.\n\n    Args:\n        mask (torch.Tensor): Input mask in BWHC format\n        threshold (float): Threshold value for binary conversion. Default: 0.01\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing the binary mask\n\n    Raises:\n        ValueError: If mask is not a valid torch.Tensor\n\n    Notes:\n        - Values &gt; threshold become 1.0\n        - Values \u2264 threshold become 0.0\n        - Useful for cleaning up masks with intermediate values\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"threshold\": (\n                    \"FLOAT\",\n                    {\"default\": 0.01, \"min\": 0.00, \"max\": 1.00, \"step\": 0.01},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Applies binary thresholding to convert a grayscale mask into a binary mask.\n    Converts all values above threshold to 1 and below threshold to 0, creating a clean\n    black and white mask without intermediate values.\n    \"\"\"\n\n    def execute(self, mask: torch.Tensor, threshold: float = 0.01) -&gt; tuple[torch.Tensor]:\n        step = TensorImage.from_BWHC(mask)\n        step[step &gt; threshold] = 1.0\n        step[step &lt;= threshold] = 0.0\n        output = TensorImage(step).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/mask/#maskbitwise","title":"MaskBitwise","text":"<p>Performs bitwise logical operations between two binary masks.</p> <p>Converts masks to 8-bit format and applies various bitwise operations, useful for combining or comparing mask regions.</p>"},{"location":"nodes/mask/#inputs_7","title":"Inputs","text":"Group Name Type Default Extras required mask_1 <code>MASK</code> required mask_2 <code>MASK</code> required mode <code>LIST</code>"},{"location":"nodes/mask/#returns_7","title":"Returns","text":"Name Type mask <code>MASK</code> Source code <pre><code>class MaskBitwise:\n    \"\"\"Performs bitwise logical operations between two binary masks.\n\n    Converts masks to 8-bit format and applies various bitwise operations, useful for combining\n    or comparing mask regions.\n\n    Args:\n        mask_1 (torch.Tensor): First input mask in BWHC format\n        mask_2 (torch.Tensor): Second input mask in BWHC format\n        mode (str): Bitwise operation to apply. Options:\n            - \"and\": Intersection of masks\n            - \"or\": Union of masks\n            - \"xor\": Exclusive OR of masks\n            - \"left_shift\": Left bit shift using mask_2 as shift amount\n            - \"right_shift\": Right bit shift using mask_2 as shift amount\n\n    Returns:\n        tuple[torch.Tensor]: A single-element tuple containing the resulting mask in BWHC format\n\n    Raises:\n        ValueError: If mode is not one of the supported operations\n\n    Notes:\n        - Masks are converted to 8-bit (0-255) before operations and back to float (0-1) after\n        - Shift operations use the second mask values as the number of bits to shift\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask_1\": (\"MASK\",),\n                \"mask_2\": (\"MASK\",),\n                \"mode\": ([\"and\", \"or\", \"xor\", \"left_shift\", \"right_shift\"],),\n            },\n        }\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Performs bitwise logical operations between two binary masks.\n    Applies operations like AND (intersection), OR (union), XOR, and bit shifts.\n    Useful for combining or comparing mask regions in precise ways.\n    \"\"\"\n\n    def execute(self, mask_1: torch.Tensor, mask_2: torch.Tensor, mode: str = \"and\") -&gt; tuple[torch.Tensor]:\n        input_mask_1 = TensorImage.from_BWHC(mask_1)\n        input_mask_2 = TensorImage.from_BWHC(mask_2)\n        eight_bit_mask_1 = torch.tensor(input_mask_1 * 255, dtype=torch.uint8)\n        eight_bit_mask_2 = torch.tensor(input_mask_2 * 255, dtype=torch.uint8)\n\n        try:\n            result = getattr(torch, f\"bitwise_{mode}\")(eight_bit_mask_1, eight_bit_mask_2)\n        except AttributeError:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n        float_result = result.float() / 255\n        output_mask = TensorImage(float_result).get_BWHC()\n        return (output_mask,)\n</code></pre>"},{"location":"nodes/mask/#maskgaussianblur","title":"MaskGaussianBlur","text":"<p>Applies Gaussian blur to soften mask edges and create smooth transitions.</p> <p>Implements a configurable Gaussian blur with control over blur radius, strength, and iterations.</p>"},{"location":"nodes/mask/#inputs_8","title":"Inputs","text":"Group Name Type Default Extras required mask <code>MASK</code> required radius <code>INT</code> 13 required sigma <code>FLOAT</code> 10.5 required iterations <code>INT</code> 1"},{"location":"nodes/mask/#returns_8","title":"Returns","text":"Name Type mask <code>MASK</code> Source code <pre><code>class MaskGaussianBlur:\n    \"\"\"Applies Gaussian blur to soften mask edges and create smooth transitions.\n\n    Implements a configurable Gaussian blur with control over blur radius, strength, and iterations.\n\n    Args:\n        image (torch.Tensor): Input mask in BWHC format\n        radius (int): Blur kernel radius. Default: 13\n        sigma (float): Blur strength/standard deviation. Default: 10.5\n        iterations (int): Number of blur passes to apply. Default: 1\n        only_outline (bool): Whether to blur only the mask edges. Default: False\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing the blurred mask\n\n    Raises:\n        ValueError: If image is not a valid torch.Tensor\n\n    Notes:\n        - Larger radius values create wider blur effects\n        - Multiple iterations can create stronger blur effects\n        - Sigma controls the falloff of the blur effect\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"radius\": (\"INT\", {\"default\": 13}),\n                \"sigma\": (\"FLOAT\", {\"default\": 10.5}),\n                \"iterations\": (\"INT\", {\"default\": 1}),\n            }\n        }\n\n    RETURN_TYPES = (\"MASK\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Applies Gaussian blur to soften mask edges and create smooth transitions.\n    Configurable blur with control over radius, strength, and number of iterations.\n    Useful for creating gradual falloff at mask boundaries.\n    \"\"\"\n\n    def execute(\n        self,\n        mask: torch.Tensor,\n        radius: int = 13,\n        sigma: float = 10.5,\n        iterations: int = 1,\n    ) -&gt; tuple[torch.Tensor]:\n        tensor_image = TensorImage.from_BWHC(mask)\n        output = gaussian_blur2d(tensor_image, radius, sigma, iterations).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/mask/#maskpreview","title":"MaskPreview","text":"<p>Generates and saves a visual preview of a mask as an image file.</p> <p>Converts mask data to a viewable image format and saves it with optional metadata.</p>"},{"location":"nodes/mask/#inputs_9","title":"Inputs","text":"Group Name Type Default Extras required mask <code>MASK</code> Source code <pre><code>class MaskPreview(SaveImage):\n    \"\"\"Generates and saves a visual preview of a mask as an image file.\n\n    Converts mask data to a viewable image format and saves it with optional metadata.\n\n    Args:\n        mask (torch.Tensor): Input mask in BWHC format\n        filename_prefix (str): Prefix for the output filename. Default: \"Signature\"\n        prompt (Optional[str]): Optional prompt text to include in metadata\n        extra_pnginfo (Optional[dict]): Additional PNG metadata to include\n\n    Returns:\n        tuple[str, str]: Tuple containing paths to the saved preview images\n\n    Raises:\n        ValueError: If mask is not a valid torch.Tensor\n        IOError: If unable to save the preview image\n\n    Notes:\n        - Saves to temporary directory with random suffix\n        - Converts mask to RGB/RGBA format for viewing\n        - Includes compression level 4 for storage efficiency\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_temp_directory()\n        self.type = \"temp\"\n        self.prefix_append = \"_temp_\" + \"\".join(random.choice(\"abcdefghijklmnopqrstupvxyz\") for _ in range(5))\n        self.compress_level = 4\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n            },\n            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n        }\n\n    FUNCTION = \"execute\"\n    CATEGORY = MASK_CAT\n    DESCRIPTION = \"\"\"\n    Generates and saves a visual preview of a mask as an image file.\n    Converts mask data to a viewable RGB/RGBA format and saves it to a temporary directory.\n    Useful for visualizing masks during workflow development.\n    \"\"\"\n\n    def execute(\n        self,\n        mask: torch.Tensor,\n        prompt: Optional[str] = None,\n        extra_pnginfo: Optional[dict] = None,\n    ) -&gt; dict[str, dict[str, list]]:\n        preview = TensorImage.from_BWHC(mask).get_rgb_or_rgba().get_BWHC()\n        return self.save_images(preview, \"Signature\", prompt, extra_pnginfo)\n</code></pre>"},{"location":"nodes/mask/#maskgrowwithblur","title":"MaskGrowWithBlur","text":"<p>Expands or contracts a mask with controllable blur and tapering effects.</p> <p>Provides fine control over mask growth with options for smooth transitions and edge effects.</p>"},{"location":"nodes/mask/#inputs_10","title":"Inputs","text":"Group Name Type Default Extras required mask <code>MASK</code> required expand <code>INT</code> 0 step=1 required incremental_expandrate <code>FLOAT</code> 0.0 min=0.0, max=100.0, step=0.1 required tapered_corners <code>BOOLEAN</code> True required flip_input <code>BOOLEAN</code> False required blur_radius <code>FLOAT</code> 0.0 min=0.0, max=100, step=0.1 required lerp_alpha <code>FLOAT</code> 1.0 min=0.0, max=1.0, step=0.01 required decay_factor <code>FLOAT</code> 1.0 min=0.0, max=1.0, step=0.01"},{"location":"nodes/mask/#returns_9","title":"Returns","text":"Name Type mask <code>MASK</code> mask <code>MASK</code> Source code <pre><code>class MaskGrowWithBlur:\n    \"\"\"Expands or contracts a mask with controllable blur and tapering effects.\n\n    Provides fine control over mask growth with options for smooth transitions and edge effects.\n\n    Args:\n        mask (torch.Tensor): Input mask in BWHC format\n        expand (int): Pixels to grow (positive) or shrink (negative). Default: 0\n        incremental_expandrate (float): Growth rate per iteration. Default: 0.0\n        tapered_corners (bool): Enable corner softening. Default: True\n        flip_input (bool): Invert input before processing. Default: False\n        blur_radius (float): Final blur amount. Default: 0.0\n        lerp_alpha (float): Blend factor for transitions. Default: 1.0\n        decay_factor (float): Growth decay rate. Default: 1.0\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing the processed mask\n\n    Raises:\n        ValueError: If inputs are invalid types or values\n\n    Notes:\n        - Positive expand values grow the mask, negative values shrink it\n        - Decay factor controls how growth diminishes over iterations\n        - Blur radius affects the final edge smoothness\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"expand\": (\n                    \"INT\",\n                    {\n                        \"default\": 0,\n                        \"min\": -MAX_INT,\n                        \"max\": MAX_INT,\n                        \"step\": 1,\n                    },\n                ),\n                \"incremental_expandrate\": (\n                    \"FLOAT\",\n                    {\"default\": 0.0, \"min\": 0.0, \"max\": 100.0, \"step\": 0.1},\n                ),\n                \"tapered_corners\": (\"BOOLEAN\", {\"default\": True}),\n                \"flip_input\": (\"BOOLEAN\", {\"default\": False}),\n                \"blur_radius\": (\n                    \"FLOAT\",\n                    {\"default\": 0.0, \"min\": 0.0, \"max\": 100, \"step\": 0.1},\n                ),\n                \"lerp_alpha\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01},\n                ),\n                \"decay_factor\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01},\n                ),\n            },\n        }\n\n    CATEGORY = MASK_CAT\n    RETURN_TYPES = (\"MASK\", \"MASK\")\n    RETURN_NAMES = (\"mask\", \"inverted mask\")\n    FUNCTION = \"expand_mask\"\n    DESCRIPTION = \"\"\"\n    Expands or contracts a mask with controllable blur and tapering effects.\n    Provides fine control over mask growth with options for smooth transitions, corner softening, and edge effects.\n    Returns both the processed mask and its inverse.\n    \"\"\"\n\n    def expand_mask(\n        self,\n        mask: torch.Tensor,\n        expand: int = 0,\n        incremental_expandrate: float = 0.0,\n        tapered_corners: bool = True,\n        flip_input: bool = False,\n        blur_radius: float = 0.0,\n        lerp_alpha: float = 1.0,\n        decay_factor: float = 1.0,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        mask = TensorImage.from_BWHC(mask)\n        alpha = lerp_alpha\n        decay = decay_factor\n        if flip_input:\n            mask = 1.0 - mask\n        c = 0 if tapered_corners else 1\n        kernel = torch.tensor([[c, 1, c], [1, 1, 1], [c, 1, c]], dtype=torch.float32)\n        growmask = mask.reshape((-1, mask.shape[-2], mask.shape[-1])).cpu()\n        out = []\n        previous_output = None\n        current_expand = expand\n        for m in growmask:\n            m = m.unsqueeze(0).unsqueeze(0)\n            output = m.clone()\n\n            for _ in range(abs(round(current_expand))):\n                if current_expand &lt; 0:\n                    output = morphology.erosion(output, kernel)\n                else:\n                    output = morphology.dilation(output, kernel)\n            if current_expand &lt; 0:\n                current_expand -= abs(incremental_expandrate)\n            else:\n                current_expand += abs(incremental_expandrate)\n\n            output = output.squeeze(0).squeeze(0)\n\n            if alpha &lt; 1.0 and previous_output is not None:\n                output = alpha * output + (1 - alpha) * previous_output\n            if decay &lt; 1.0 and previous_output is not None:\n                output += decay * previous_output\n                output = output / output.max()\n            previous_output = output\n            out.append(output)\n\n        if blur_radius != 0:\n            kernel_size = int(4 * round(blur_radius) + 1)\n            blurred = [\n                filters.gaussian_blur2d(\n                    tensor.unsqueeze(0).unsqueeze(0),\n                    (kernel_size, kernel_size),\n                    (blur_radius, blur_radius),\n                ).squeeze(0)\n                for tensor in out\n            ]\n            blurred = torch.cat(blurred, dim=0)\n            blurred_mask = TensorImage(blurred).get_BWHC()\n            inverted = 1.0 - blurred_mask\n\n            return (\n                blurred_mask,\n                inverted,\n            )\n\n        unblurred = torch.stack(out, dim=0)\n        unblurred_mask = TensorImage(unblurred).get_BWHC()\n        inverted = 1 - unblurred_mask\n\n        return (\n            unblurred_mask,\n            inverted,\n        )\n</code></pre>"},{"location":"nodes/math/","title":"Math Nodes","text":""},{"location":"nodes/models/","title":"Models Nodes","text":""},{"location":"nodes/models/#unblur","title":"Unblur","text":"<p>Enhances image clarity by reducing blur using the SeeMore model.</p> <p>This class implements image deblurring functionality using the SeeMore neural network model. It's effective for correcting motion blur, out-of-focus areas, and general image softness.</p>"},{"location":"nodes/models/#inputs","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required preview <code>LIST</code>"},{"location":"nodes/models/#returns","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class Unblur(SaveImage):\n    \"\"\"Enhances image clarity by reducing blur using the SeeMore model.\n\n    This class implements image deblurring functionality using the SeeMore neural network model.\n    It's effective for correcting motion blur, out-of-focus areas, and general image softness.\n\n    Args:\n        image (torch.Tensor): Input image tensor in BWHC (Batch, Width, Height, Channel) format.\n        preview (str): Controls preview image generation. Options:\n            - \"on\": Saves preview images\n            - \"off\": No preview images\n        filename_prefix (str, optional): Prefix to use for saved output files. Defaults to \"Signature\".\n        prompt (str, optional): Text prompt for metadata. Defaults to None.\n        extra_pnginfo (dict, optional): Additional metadata to save with output images. Defaults to None.\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing the unblurred image in BWHC format.\n\n    Notes:\n        - The model automatically handles memory cleanup after processing\n        - Temporary files are saved with random suffixes to prevent naming conflicts\n        - Preview images are saved at compression level 4 for balance of quality and size\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_temp_directory()\n        self.type = \"temp\"\n        self.prefix_append = \"_temp_\" + \"\".join(random.choice(\"abcdefghijklmnopqrstupvxyz\") for x in range(5))\n        self.compress_level = 4\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"preview\": ([\"on\", \"off\"],),\n            },\n            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MODELS_CAT\n    DEPRECATED = True\n    DESCRIPTION = \"\"\"\n    Enhances image clarity by reducing blur using the SeeMore neural network model.\n    Effective for correcting motion blur, out-of-focus areas, and general image softness.\n    Provides optional preview generation.\n    \"\"\"\n\n    def execute(\n        self,\n        image: torch.Tensor,\n        preview: str = \"on\",\n        prompt: Optional[str] = None,\n        extra_pnginfo: Optional[dict] = None,\n    ):\n        if preview not in [\"on\", \"off\"]:\n            raise ValueError(\"Preview must be either 'on' or 'off'\")\n\n        filename_prefix = \"Signature\"\n\n        model = SeeMore()\n        input_image = TensorImage.from_BWHC(image)\n        output_image = model.forward(input_image)\n        output_images = TensorImage(output_image).get_BWHC()\n\n        if preview == \"off\":\n            return (output_images,)\n        result = self.save_images(output_images, filename_prefix, prompt, extra_pnginfo)\n        result.update({\"result\": (output_images,)})\n        model = None\n        del model\n        return result\n</code></pre>"},{"location":"nodes/models/#magiceraser","title":"MagicEraser","text":"<p>Removes unwanted content from images using the Lama inpainting model.</p> <p>This class provides functionality to erase and reconstruct image regions based on a provided mask. The Lama model intelligently fills in the masked areas with contextually appropriate content.</p>"},{"location":"nodes/models/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code> required mask <code>MASK</code> required preview <code>LIST</code> optional upscale_model <code>&lt;ast.BinOp object at 0x1007bf040&gt;</code>"},{"location":"nodes/models/#returns_1","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class MagicEraser(SaveImage):\n    \"\"\"Removes unwanted content from images using the Lama inpainting model.\n\n    This class provides functionality to erase and reconstruct image regions based on a provided mask.\n    The Lama model intelligently fills in the masked areas with contextually appropriate content.\n\n    Args:\n        image (torch.Tensor): Input image tensor in BWHC (Batch, Width, Height, Channel) format.\n        mask (torch.Tensor): Binary mask tensor in BWHC format where 1 indicates areas to erase.\n        preview (str): Controls preview image generation. Options:\n            - \"on\": Saves preview images\n            - \"off\": No preview images\n        filename_prefix (str, optional): Prefix to use for saved output files. Defaults to \"Signature\".\n        upscale_model (str, optional): Name of the upscale model to use. Defaults to None.\n        prompt (str, optional): Text prompt for metadata. Defaults to None.\n        extra_pnginfo (dict, optional): Additional metadata to save with output images. Defaults to None.\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing the processed image in BWHC format.\n\n    Notes:\n        - The model automatically handles memory cleanup after processing\n        - Temporary files are saved with random suffixes to prevent naming conflicts\n        - Preview images are saved at compression level 4 for balance of quality and size\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_temp_directory()\n        self.type = \"temp\"\n        self.prefix_append = \"_temp_\" + \"\".join(random.choice(\"abcdefghijklmnopqrstupvxyz\") for x in range(5))\n        self.compress_level = 4\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"mask\": (\"MASK\",),\n                \"preview\": ([\"on\", \"off\"],),\n            },\n            \"optional\": {\n                \"upscale_model\": ([\"None\"] + folder_paths.get_filename_list(\"upscale_models\"),),\n            },\n            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = MODELS_CAT\n    DESCRIPTION = \"\"\"\n    Removes unwanted content from images using the Lama inpainting model.\n    Intelligently fills in masked areas with contextually appropriate content.\n    Supports optional upscaling for higher quality results.\n    \"\"\"\n\n    def execute(\n        self,\n        image: torch.Tensor,\n        mask: torch.Tensor,\n        preview: str,\n        upscale_model: str | None,\n        extra_pnginfo: dict,\n        prompt: str = \"\",\n    ):\n        upscale_fn = None\n        loaded_upscale_model = None\n        device = comfy.model_management.get_torch_device()\n        if upscale_model is not None:\n            upscale_model_path = folder_paths.get_full_path(\"upscale_models\", upscale_model)\n            sd = comfy.utils.load_torch_file(upscale_model_path, safe_load=True)\n            if \"module.layers.0.residual_group.blocks.0.norm1.weight\" in sd:\n                sd = comfy.utils.state_dict_prefix_replace(sd, {\"module.\": \"\"})\n            loaded_upscale_model = ModelLoader().load_from_state_dict(sd)\n            loaded_upscale_model.to(device)\n\n            # TODO: See how we can unify this with the UpscaleImage node, probably it makes sense to extract the code\n            # for the pure upscale into a function or class in signature-core\n            upscaler = ImageUpscaleWithModel\n\n            def upscale_image(image: torch.Tensor) -&gt; torch.Tensor:\n                return upscaler.upscale(upscaler, loaded_upscale_model, image)\n\n            upscale_fn = upscale_image\n\n        filename_prefix = \"Signature\"\n\n        model = Lama(device, upscale_fn)\n        input_image = TensorImage.from_BWHC(image)\n        input_mask = TensorImage.from_BWHC(mask)\n        result = TensorImage(model.forward(input_image, input_mask), device=input_mask.device)\n\n        output_images = TensorImage(result * (input_mask) + input_image * (1 - input_mask)).get_BWHC()\n        if preview == \"off\":\n            return (output_images,)\n        result = self.save_images(output_images, filename_prefix, prompt, extra_pnginfo)\n        result.update({\"result\": (output_images,)})\n\n        model = None\n        del model\n        if loaded_upscale_model is not None:\n            loaded_upscale_model = None\n            del loaded_upscale_model\n\n        return result\n</code></pre>"},{"location":"nodes/models/#backgroundremoval","title":"BackgroundRemoval","text":"<p>Separates foreground subjects from image backgrounds using AI segmentation models.</p> <p>This class provides multiple AI models for background removal, offering different approaches and quality levels for various use cases. It can output both masked and RGBA versions of the results.</p>"},{"location":"nodes/models/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required model_name <code>&lt;ast.Attribute object at 0x1007ad930&gt;</code> required preview <code>&lt;ast.Attribute object at 0x1007af400&gt;</code> required image <code>IMAGE</code>"},{"location":"nodes/models/#returns_2","title":"Returns","text":"Name Type image <code>IMAGE</code> image <code>IMAGE</code> mask <code>MASK</code> Source code <pre><code>class BackgroundRemoval(SaveImage):\n    \"\"\"Separates foreground subjects from image backgrounds using AI segmentation models.\n\n    This class provides multiple AI models for background removal, offering different approaches and\n    quality levels for various use cases. It can output both masked and RGBA versions of the results.\n\n    Args:\n        image (torch.Tensor): Input image tensor in BWHC (Batch, Width, Height, Channel) format.\n        model_name (str): The AI model to use for segmentation. Options:\n            - \"inspyrenet\": General-purpose segmentation\n            - \"rmbg14\": Optimized for human subjects\n            - \"isnet_general\": Balanced approach for various subjects\n            - \"fakepng\": Fast but lower quality option\n        preview (str): Controls preview output type. Options:\n            - \"mask\": Shows the segmentation mask\n            - \"rgba\": Shows the transparent background result\n            - \"none\": No preview\n        filename_prefix (str, optional): Prefix to use for saved output files. Defaults to \"Signature\".\n        prompt (str, optional): Text prompt for metadata. Defaults to None.\n        extra_pnginfo (dict, optional): Additional metadata to save with output images. Defaults to None.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n            - rgba: Image with transparent background in BWHC format\n            - rgb: Original image with background in BWHC format\n            - mask: Binary segmentation mask in BWHC format\n\n    Notes:\n        - The model automatically handles memory cleanup after processing\n        - Temporary files are saved with random suffixes to prevent naming conflicts\n        - Preview images are saved at compression level 4 for balance of quality and size\n        - Different models may perform better on different types of images\n    \"\"\"\n\n    model_names = [\"inspyrenet\", \"rmbg14\", \"isnet_general\", \"fakepng\"]\n    preview_types = [\"mask\", \"rgba\", \"none\"]\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_temp_directory()\n        self.type = \"temp\"\n        self.prefix_append = \"_temp_\" + \"\".join(random.choice(\"abcdefghijklmnopqrstupvxyz\") for _ in range(5))\n        self.compress_level = 4\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"model_name\": (cls.model_names,),\n                \"preview\": (cls.preview_types,),\n                \"image\": (\"IMAGE\",),\n            },\n            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, model_name: str, preview: str) -&gt; bool:\n        if not isinstance(model_name, str):\n            raise ValueError(\"Model name must be a string\")\n        if not isinstance(preview, str):\n            raise ValueError(\"Preview must be a string\")\n        if model_name not in cls.model_names:\n            raise ValueError(\"Invalid model name\")\n        if preview not in cls.preview_types:\n            raise ValueError(\"Invalid preview type\")\n        return True\n\n    RETURN_TYPES = (\"IMAGE\", \"IMAGE\", \"MASK\")\n    RETURN_NAMES = (\"rgba\", \"rgb\", \"mask\")\n    FUNCTION = \"execute\"\n    CATEGORY = MODELS_CAT\n    DESCRIPTION = \"\"\"\n    Separates foreground subjects from image backgrounds using AI segmentation models.\n    Offers multiple models with different quality levels and approaches.\n    Returns the transparent background image, original image, and segmentation mask.\n    \"\"\"\n\n    def execute(\n        self,\n        model_name: str,\n        preview: str,\n        image: torch.Tensor,\n        prompt: str,\n        extra_pnginfo: dict,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        filename_prefix = \"Signature\"\n\n        model = SalientObjectDetection(model_name=model_name)\n        input_image = TensorImage.from_BWHC(image)\n        masks = model.forward(input_image)\n\n        output_masks = TensorImage(masks)\n        rgb, rgba = cutout(input_image, output_masks)\n        rgb_output = TensorImage(rgb).get_BWHC()\n        rgba_output = TensorImage(rgba).get_BWHC()\n        mask_output = output_masks.get_BWHC()\n        if preview == \"none\":\n            return (\n                rgba_output,\n                rgb_output,\n                mask_output,\n            )\n        preview_images = output_masks.get_rgb_or_rgba().get_BWHC() if preview == \"mask\" else rgba_output\n        result = self.save_images(preview_images, filename_prefix, prompt, extra_pnginfo)\n        result.update(\n            {\n                \"result\": (\n                    rgba_output,\n                    rgb_output,\n                    mask_output,\n                )\n            }\n        )\n        model = None\n        del model\n        return result\n</code></pre>"},{"location":"nodes/neurochain_utils/","title":"Neurochain Utils Nodes","text":""},{"location":"nodes/numbers/","title":"Numbers Nodes","text":""},{"location":"nodes/numbers/#floatclamp","title":"FloatClamp","text":"<p>Clamps a floating-point value between specified minimum and maximum bounds.</p> <p>This class provides functionality to constrain a float input within a defined range. If the input number is less than the minimum value, it returns the minimum value. If it's greater than the maximum value, it returns the maximum value.</p>"},{"location":"nodes/numbers/#inputs","title":"Inputs","text":"Group Name Type Default Extras required number <code>FLOAT</code> 0 forceInput=True required max_value <code>FLOAT</code> 0 step=0.01 required min_value <code>FLOAT</code> 0 step=0.01"},{"location":"nodes/numbers/#returns","title":"Returns","text":"Name Type float <code>FLOAT</code> Source code <pre><code>class FloatClamp:\n    \"\"\"Clamps a floating-point value between specified minimum and maximum bounds.\n\n    This class provides functionality to constrain a float input within a defined range. If the input\n    number is less than the minimum value, it returns the minimum value. If it's greater than the\n    maximum value, it returns the maximum value.\n\n    Args:\n        number (float): The input float to be clamped.\n        min_value (float): The minimum allowed value.\n        max_value (float): The maximum allowed value.\n\n    Returns:\n        tuple[float]: A single-element tuple containing the clamped float value.\n\n    Raises:\n        ValueError: If any of the inputs (number, min_value, max_value) are not floats.\n\n    Notes:\n        - The input range is limited by MAX_FLOAT constant\n        - The returned value is always wrapped in a tuple to maintain consistency with the node system\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"number\": (\n                    \"FLOAT\",\n                    {\n                        \"default\": 0,\n                        \"forceInput\": True,\n                        \"min\": -MAX_FLOAT,\n                        \"max\": MAX_FLOAT,\n                    },\n                ),\n                \"max_value\": (\n                    \"FLOAT\",\n                    {\"default\": 0, \"min\": -MAX_FLOAT, \"max\": MAX_FLOAT, \"step\": 0.01},\n                ),\n                \"min_value\": (\n                    \"FLOAT\",\n                    {\"default\": 0, \"min\": -MAX_FLOAT, \"max\": MAX_FLOAT, \"step\": 0.01},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"FLOAT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    DESCRIPTION = \"\"\"\n    Clamps a floating-point value between specified minimum and maximum bounds.\n    Constrains a float input within a defined range,\n    returning the min value if input is too low or max value if input is too high.\n    \"\"\"\n\n    def execute(self, number: float = 0.0, max_value: float = 0.0, min_value: float = 0.0) -&gt; tuple[float]:\n        if max_value &lt; min_value:\n            raise ValueError(\"Max value must be greater than or equal to min value\")\n        if number &lt; min_value:\n            return (min_value,)\n        if number &gt; max_value:\n            return (max_value,)\n        return (number,)\n</code></pre>"},{"location":"nodes/numbers/#int2float","title":"Int2Float","text":"<p>Converts an integer to a floating-point number.</p> <p>This class handles the conversion of integer values to floating-point numbers using Python's built-in float() function.</p>"},{"location":"nodes/numbers/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required number <code>INT</code> 0 forceInput=True"},{"location":"nodes/numbers/#returns_1","title":"Returns","text":"Name Type float <code>FLOAT</code> Source code <pre><code>class Int2Float:\n    \"\"\"Converts an integer to a floating-point number.\n\n    This class handles the conversion of integer values to floating-point numbers using Python's\n    built-in float() function.\n\n    Args:\n        number (int): The integer to convert to a float.\n\n    Returns:\n        tuple[float]: A single-element tuple containing the converted float value.\n\n    Raises:\n        ValueError: If the input value is not an integer.\n\n    Notes:\n        - The conversion is exact as all integers can be represented precisely as floats\n        - The returned value is always wrapped in a tuple to maintain consistency with the node system\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"number\": (\"INT\", {\"default\": 0, \"forceInput\": True}),\n            }\n        }\n\n    RETURN_TYPES = (\"FLOAT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    CLASS_ID = \"int2float\"\n    DESCRIPTION = \"\"\"\n    Converts an integer to a floating-point number.\n    Transforms integer values to their exact floating-point representation.\n    Useful when numeric operations require float inputs.\n    \"\"\"\n\n    def execute(self, number: int = 0) -&gt; tuple[float]:\n        try:\n            return (float(number),)\n        except (TypeError, ValueError):\n            raise ValueError(\"Number must be convertible to integer\")\n</code></pre>"},{"location":"nodes/numbers/#intminmax","title":"IntMinMax","text":"<p>Determines the minimum or maximum value between two integers.</p> <p>This class compares two integer inputs and returns either the smaller or larger value based on the specified mode of operation.</p>"},{"location":"nodes/numbers/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required a <code>INT</code> 0 forceInput=True required b <code>INT</code> 0 forceInput=True required mode <code>LIST</code> min"},{"location":"nodes/numbers/#returns_2","title":"Returns","text":"Name Type int <code>INT</code> Source code <pre><code>class IntMinMax:\n    \"\"\"Determines the minimum or maximum value between two integers.\n\n    This class compares two integer inputs and returns either the smaller or larger value based on\n    the specified mode of operation.\n\n    Args:\n        a (int): The first integer to compare.\n        b (int): The second integer to compare.\n        mode (str): The comparison mode ('min' or 'max').\n\n    Returns:\n        tuple[int]: A single-element tuple containing either the minimum or maximum value.\n\n    Raises:\n        ValueError: If either input is not an integer or if the mode is not supported.\n\n    Notes:\n        - The returned value is always wrapped in a tuple to maintain consistency with the node system\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"a\": (\"INT\", {\"default\": 0, \"forceInput\": True}),\n                \"b\": (\"INT\", {\"default\": 0, \"forceInput\": True}),\n                \"mode\": ([\"min\", \"max\"], {\"default\": \"min\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"INT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    CLASS_ID = \"int_minmax\"\n    DESCRIPTION = \"\"\"\n    Determines the minimum or maximum value between two integers.\n    Compares two integer inputs and returns either the smaller or larger value based on the specified mode of operation.\n    \"\"\"\n\n    def execute(self, a: int = 0, b: int = 0, mode: str = \"min\") -&gt; tuple[int]:\n        if mode in OP_FUNCTIONS:\n            return (OP_FUNCTIONS[mode](a, b),)\n        raise ValueError(f\"Unsupported mode: {mode}\")\n</code></pre>"},{"location":"nodes/numbers/#randomnumber","title":"RandomNumber","text":"<p>Generates a random integer and its floating-point representation.</p> <p>This class produces a random integer between 0 and MAX_INT and provides both the integer value and its floating-point equivalent.</p>"},{"location":"nodes/numbers/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras"},{"location":"nodes/numbers/#returns_3","title":"Returns","text":"Name Type int <code>INT</code> float <code>FLOAT</code> Source code <pre><code>class RandomNumber:\n    \"\"\"Generates a random integer and its floating-point representation.\n\n    This class produces a random integer between 0 and MAX_INT and provides both the integer value\n    and its floating-point equivalent.\n\n    Returns:\n        tuple[int, float]: A tuple containing the random integer and its float representation.\n\n    Notes:\n        - The random value is regenerated each time IS_CHANGED is called\n        - The maximum value is limited by MAX_INT constant\n        - No parameters are required for this operation\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\"required\": {}}\n\n    RETURN_TYPES = (\n        \"INT\",\n        \"FLOAT\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    DESCRIPTION = \"\"\"\n    Generates a random integer and its floating-point representation.\n    Produces a random integer between 0 and MAX_INT and provides both the integer value and its float equivalent.\n    The value changes each time the node is evaluated.\n    \"\"\"\n\n    @staticmethod\n    def get_random() -&gt; tuple[int, float]:\n        result = random.randint(0, MAX_INT)\n        return (\n            result,\n            float(result),\n        )\n\n    def execute(self) -&gt; tuple[int, float]:\n        return RandomNumber.get_random()\n\n    @classmethod\n    def IS_CHANGED(cls) -&gt; tuple[int, float]:  # type: ignore\n        return RandomNumber.get_random()\n</code></pre>"},{"location":"nodes/numbers/#floatminmax","title":"FloatMinMax","text":"<p>Determines the minimum or maximum value between two floating-point numbers.</p> <p>This class compares two float inputs and returns either the smaller or larger value based on the specified mode of operation.</p>"},{"location":"nodes/numbers/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required a <code>FLOAT</code> 0.0 forceInput=True required b <code>FLOAT</code> 0.0 forceInput=True required mode <code>LIST</code> min"},{"location":"nodes/numbers/#returns_4","title":"Returns","text":"Name Type float <code>FLOAT</code> Source code <pre><code>class FloatMinMax:\n    \"\"\"Determines the minimum or maximum value between two floating-point numbers.\n\n    This class compares two float inputs and returns either the smaller or larger value based on\n    the specified mode of operation.\n\n    Args:\n        a (float): The first float to compare.\n        b (float): The second float to compare.\n        mode (str): The comparison mode ('min' or 'max').\n\n    Returns:\n        tuple[float]: A single-element tuple containing either the minimum or maximum value.\n\n    Raises:\n        ValueError: If either input is not a float or if the mode is not supported.\n\n    Notes:\n        - The returned value is always wrapped in a tuple to maintain consistency with the node system\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"a\": (\"FLOAT\", {\"default\": 0.0, \"forceInput\": True}),\n                \"b\": (\"FLOAT\", {\"default\": 0.0, \"forceInput\": True}),\n                \"mode\": ([\"min\", \"max\"], {\"default\": \"min\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"FLOAT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    CLASS_ID = \"float_minmax\"\n    DESCRIPTION = \"\"\"\n    Determines the minimum or maximum value between two floating-point numbers.\n    Compares two float inputs and returns either the smaller or larger value based on the specified mode of operation.\n    \"\"\"\n\n    def execute(self, a: float = 0.0, b: float = 0.0, mode: str = \"min\") -&gt; tuple[float]:\n        if mode in OP_FUNCTIONS:\n            return (OP_FUNCTIONS[mode](a, b),)\n        raise ValueError(f\"Unsupported mode: {mode}\")\n</code></pre>"},{"location":"nodes/numbers/#intclamp","title":"IntClamp","text":"<p>Clamps an integer value between specified minimum and maximum bounds.</p> <p>This class provides functionality to constrain an integer input within a defined range. If the input number is less than the minimum value, it returns the minimum value. If it's greater than the maximum value, it returns the maximum value.</p>"},{"location":"nodes/numbers/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required number <code>INT</code> 0 forceInput=True required max_value <code>INT</code> 0 step=1 required min_value <code>INT</code> 0 step=1"},{"location":"nodes/numbers/#returns_5","title":"Returns","text":"Name Type int <code>INT</code> Source code <pre><code>class IntClamp:\n    \"\"\"Clamps an integer value between specified minimum and maximum bounds.\n\n    This class provides functionality to constrain an integer input within a defined range. If the input\n    number is less than the minimum value, it returns the minimum value. If it's greater than the\n    maximum value, it returns the maximum value.\n\n    Args:\n        number (int): The input integer to be clamped.\n        min_value (int): The minimum allowed value.\n        max_value (int): The maximum allowed value.\n\n    Returns:\n        tuple[int]: A single-element tuple containing the clamped integer value.\n\n    Raises:\n        ValueError: If any of the inputs (number, min_value, max_value) are not integers.\n\n    Notes:\n        - The input range is limited by MAX_INT constant\n        - The returned value is always wrapped in a tuple to maintain consistency with the node system\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"number\": (\n                    \"INT\",\n                    {\n                        \"default\": 0,\n                        \"forceInput\": True,\n                        \"min\": -MAX_INT,\n                        \"max\": MAX_INT,\n                    },\n                ),\n                \"max_value\": (\n                    \"INT\",\n                    {\"default\": 0, \"min\": -MAX_INT, \"max\": MAX_INT, \"step\": 1},\n                ),\n                \"min_value\": (\n                    \"INT\",\n                    {\"default\": 0, \"min\": -MAX_INT, \"max\": MAX_INT, \"step\": 1},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"INT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    DESCRIPTION = \"\"\"\n    Clamps an integer value between specified minimum and maximum bounds.\n    Constrains an integer input within a defined range,\n    returning the min value if input is too low or max value if input is too high.\n    \"\"\"\n\n    def execute(self, number: int = 0, max_value: int = 0, min_value: int = 0) -&gt; tuple[int]:\n        if max_value &lt; min_value:\n            raise ValueError(\"Max value must be greater than or equal to min value\")\n        if number &lt; min_value:\n            return (min_value,)\n        if number &gt; max_value:\n            return (max_value,)\n        return (number,)\n</code></pre>"},{"location":"nodes/numbers/#floatoperator","title":"FloatOperator","text":"<p>Performs arithmetic operations on two floating-point numbers.</p> <p>This class supports basic arithmetic operations between two floating-point numbers. The supported operations are addition, subtraction, multiplication, and division.</p>"},{"location":"nodes/numbers/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required left <code>FLOAT</code> 0 step=0.01 required right <code>FLOAT</code> 0 step=0.01 required operator <code>LIST</code>"},{"location":"nodes/numbers/#returns_6","title":"Returns","text":"Name Type float <code>FLOAT</code> Source code <pre><code>class FloatOperator:\n    \"\"\"Performs arithmetic operations on two floating-point numbers.\n\n    This class supports basic arithmetic operations between two floating-point numbers. The supported\n    operations are addition, subtraction, multiplication, and division.\n\n    Args:\n        left (float): The left operand for the arithmetic operation.\n        right (float): The right operand for the arithmetic operation.\n        operator (str): The arithmetic operator to use ('+', '-', '*', or '/').\n\n    Returns:\n        tuple[float]: A single-element tuple containing the result of the operation.\n\n    Raises:\n        ValueError: If either operand is not a float or if the operator is not supported.\n\n    Notes:\n        - Division by zero will raise a Python exception\n        - The returned value is always wrapped in a tuple to maintain consistency with the node system\n        - Input values are limited by MAX_FLOAT constant\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"left\": (\n                    \"FLOAT\",\n                    {\"default\": 0, \"min\": -MAX_FLOAT, \"max\": MAX_FLOAT, \"step\": 0.01},\n                ),\n                \"right\": (\n                    \"FLOAT\",\n                    {\"default\": 0, \"min\": -MAX_FLOAT, \"max\": MAX_FLOAT, \"step\": 0.01},\n                ),\n                \"operator\": ([\"+\", \"-\", \"*\", \"/\", \"%\"],),\n            }\n        }\n\n    RETURN_TYPES = (\"FLOAT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    DEPRECATED = True\n    DESCRIPTION = \"\"\"\n    Performs arithmetic operations on two floating-point numbers.\n    Supports addition, subtraction, multiplication, division, and modulo operations between two float values.\n    Returns the calculated result.\n    \"\"\"\n\n    def execute(self, left: float = 0.0, right: float = 0.0, operator: str = \"+\") -&gt; tuple[float]:\n        if operator in BASIC_OPERATORS:\n            return (BASIC_OPERATORS[operator](left, right),)\n\n        raise ValueError(f\"Unsupported operator: {operator}\")\n</code></pre>"},{"location":"nodes/numbers/#mathoperator","title":"MathOperator","text":"<p>Evaluates mathematical expressions with support for variables and multiple operators.</p> <p>This class provides a powerful expression evaluator that supports variables (a, b, c, d) and various mathematical operations. It can handle arithmetic, comparison, and logical operations.</p>"},{"location":"nodes/numbers/#returns_7","title":"Returns","text":"Name Type int <code>INT</code> float <code>FLOAT</code> Source code <pre><code>class MathOperator:\n    \"\"\"Evaluates mathematical expressions with support for variables and multiple operators.\n\n    This class provides a powerful expression evaluator that supports variables (a, b, c, d) and\n    various mathematical operations. It can handle arithmetic, comparison, and logical operations.\n\n    Args:\n        a (float, optional): Value for variable 'a'. Defaults to 0.0.\n        b (float, optional): Value for variable 'b'. Defaults to 0.0.\n        c (float, optional): Value for variable 'c'. Defaults to 0.0.\n        d (float, optional): Value for variable 'd'. Defaults to 0.0.\n        value (str): The mathematical expression to evaluate.\n\n    Returns:\n        tuple[int, float]: A tuple containing both integer and float representations of the result.\n\n    Raises:\n        ValueError: If the expression contains unsupported operations or invalid syntax.\n\n    Notes:\n        - Supports standard arithmetic operators: +, -, *, /, //, %, **\n        - Supports comparison operators: ==, !=, &lt;, &lt;=, &gt;, &gt;=\n        - Supports logical operators: and, or, not\n        - Supports bitwise XOR operator: ^\n        - Supports exponential and logarithmic functions: base**exponent, log(base, value)\n        - Includes functions: min(), max(), round(), sum(), len(), clamp(value, min, max)\n        - Variables are limited by MAX_FLOAT constant\n        - NaN results are converted to 0.0\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        input_letters = string.ascii_lowercase[:10]  # Get an array of all letters from a to j\n        inputs = {\n            \"required\": {\n                \"num_slots\": ([str(i) for i in range(1, 11)], {\"default\": \"1\"}),\n                \"value\": (\"STRING\", {\"default\": \"\"}),\n            },\n            \"optional\": {},\n        }\n\n        for letter in input_letters:\n            inputs[\"optional\"].update(\n                {\n                    letter: (\n                        \"INT,FLOAT\",\n                        {\n                            \"default\": 0,\n                            \"min\": -MAX_FLOAT,\n                            \"max\": MAX_FLOAT,\n                            \"step\": 0.01,\n                            \"forceInput\": True,\n                        },\n                    )\n                }\n            )\n\n        return inputs\n\n    RETURN_TYPES = (\n        \"INT\",\n        \"FLOAT\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    DESCRIPTION = \"\"\"Evaluates mathematical expressions with support for variables and multiple operators.\n\n    This class provides a powerful expression evaluator that supports variables (a, b, c, d) and\n    various mathematical operations. It can handle arithmetic, comparison, and logical operations.\n\n    Args:\n        a (float, optional): Value for variable 'a'. Defaults to 0.0.\n        b (float, optional): Value for variable 'b'. Defaults to 0.0.\n        c (float, optional): Value for variable 'c'. Defaults to 0.0.\n        d (float, optional): Value for variable 'd'. Defaults to 0.0.\n        value (str): The mathematical expression to evaluate.\n\n    Returns:\n        tuple[int, float]: A tuple containing both integer and float representations of the result.\n\n    Raises:\n        ValueError: If the expression contains unsupported operations or invalid syntax.\n\n    Notes:\n        - Supports standard arithmetic operators: +, -, *, /, //, %, **\n        - Supports comparison operators: ==, !=, &lt;, &lt;=, &gt;, &gt;=\n        - Supports logical operators: and, or, not\n        - Supports bitwise XOR operator: ^\n        - Supports exponential and logarithmic functions: base**exponent, log(base, value)\n        - Includes functions: min(), max(), round(), sum(), len(), clamp(value, min, max)\n        - Variables are limited by MAX_FLOAT constant\n        - NaN results are converted to 0.0\n    \"\"\"\n\n    def execute(self, num_slots: str = \"1\", value: str = \"\", **kwargs) -&gt; tuple[int, float]:\n        if int(num_slots) != len(kwargs.keys()):\n            raise ValueError(\"Number of inputs is not equal to number of slots\")\n\n        def safe_xor(x, y):\n            if isinstance(x, float) or isinstance(y, float):\n                # Convert to integers if either operand is a float\n                return float(int(x) ^ int(y))\n            return op.xor(x, y)\n\n        operators = {\n            ast.Add: op.add,\n            ast.Sub: op.sub,\n            ast.Mult: op.mul,\n            ast.Div: op.truediv,\n            ast.FloorDiv: op.floordiv,\n            ast.Pow: op.pow,\n            ast.USub: op.neg,\n            ast.Mod: op.mod,\n            ast.Eq: op.eq,\n            ast.NotEq: op.ne,\n            ast.Lt: op.lt,\n            ast.LtE: op.le,\n            ast.Gt: op.gt,\n            ast.GtE: op.ge,\n            ast.And: lambda x, y: x and y,\n            ast.Or: lambda x, y: x or y,\n            ast.Not: op.not_,\n            ast.BitXor: safe_xor,  # Use the safe_xor function\n        }\n\n        def eval_(node):\n            if isinstance(node, ast.Constant):  # number\n                return node.n\n            if isinstance(node, ast.Name):  # variable\n                return kwargs.get(node.id)\n            if isinstance(node, ast.BinOp):  # &lt;left&gt; &lt;operator&gt; &lt;right&gt;\n                return operators[type(node.op)](eval_(node.left), eval_(node.right))  # type: ignore\n            if isinstance(node, ast.UnaryOp):  # &lt;operator&gt; &lt;operand&gt; e.g., -1\n                return operators[type(node.op)](eval_(node.operand))  # type: ignore\n            if isinstance(node, ast.Compare):  # comparison operators\n                left = eval_(node.left)\n                for operator, comparator in zip(node.ops, node.comparators):\n                    if not operators[type(operator)](left, eval_(comparator)):  # type: ignore\n                        return 0\n                return 1\n            if isinstance(node, ast.BoolOp):  # boolean operators (And, Or)\n                values = [eval_(value) for value in node.values]\n                return operators[type(node.op)](*values)  # type: ignore\n            if isinstance(node, ast.Call):  # custom function\n                if node.func.id in OP_FUNCTIONS:  # type: ignore\n                    args = [eval_(arg) for arg in node.args]\n                    return OP_FUNCTIONS[node.func.id](*args)  # type: ignore\n            if isinstance(node, ast.Subscript):  # indexing or slicing\n                value = eval_(node.value)\n                if isinstance(node.slice, ast.Constant):\n                    return value[node.slice.value]\n                return 0\n            return 0\n\n        result = eval_(ast.parse(value, mode=\"eval\").body)\n\n        if math.isnan(result):  # type: ignore\n            result = 0.0\n\n        return (\n            round(result),  # type: ignore\n            result,  # type: ignore\n        )\n</code></pre>"},{"location":"nodes/numbers/#intoperator","title":"IntOperator","text":"<p>Performs arithmetic operations on two floats and returns an integer result.</p> <p>This class supports basic arithmetic operations between two floating-point numbers and returns the result as an integer. The supported operations are addition, subtraction, multiplication, and division.</p>"},{"location":"nodes/numbers/#inputs_7","title":"Inputs","text":"Group Name Type Default Extras required left <code>FLOAT</code> 0 step=0.01 required right <code>FLOAT</code> 0 step=0.01 required operator <code>LIST</code>"},{"location":"nodes/numbers/#returns_8","title":"Returns","text":"Name Type int <code>INT</code> Source code <pre><code>class IntOperator:\n    \"\"\"Performs arithmetic operations on two floats and returns an integer result.\n\n    This class supports basic arithmetic operations between two floating-point numbers and returns\n    the result as an integer. The supported operations are addition, subtraction, multiplication,\n    and division.\n\n    Args:\n        left (float): The left operand for the arithmetic operation.\n        right (float): The right operand for the arithmetic operation.\n        operator (str): The arithmetic operator to use ('+', '-', '*', or '/').\n\n    Returns:\n        tuple[int]: A single-element tuple containing the result of the operation as an integer.\n\n    Raises:\n        ValueError: If either operand is not a float or if the operator is not supported.\n\n    Notes:\n        - Division results are converted to integers\n        - The returned value is always wrapped in a tuple to maintain consistency with the node system\n        - Input values are limited by MAX_FLOAT constant\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"left\": (\n                    \"FLOAT\",\n                    {\"default\": 0, \"min\": -MAX_FLOAT, \"max\": MAX_FLOAT, \"step\": 0.01},\n                ),\n                \"right\": (\n                    \"FLOAT\",\n                    {\"default\": 0, \"min\": -MAX_FLOAT, \"max\": MAX_FLOAT, \"step\": 0.01},\n                ),\n                \"operator\": ([\"+\", \"-\", \"*\", \"/\"],),\n            }\n        }\n\n    RETURN_TYPES = (\"INT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    DEPRECATED = True\n    DESCRIPTION = \"\"\"\n    Performs arithmetic operations on two floats and returns an integer result.\n    Supports addition, subtraction, multiplication, and division between two float values.\n    Returns the calculated result as an integer.\n    \"\"\"\n\n    def execute(self, left: float = 0.0, right: float = 0.0, operator: str = \"+\") -&gt; tuple[int]:\n        if operator in BASIC_OPERATORS:\n            return (int(BASIC_OPERATORS[operator](left, right)),)\n\n        raise ValueError(f\"Unsupported operator: {operator}\")\n</code></pre>"},{"location":"nodes/numbers/#float2int","title":"Float2Int","text":"<p>Converts a floating-point number to an integer through truncation.</p> <p>This class handles the conversion of float values to integers by removing the decimal portion. The conversion is performed using Python's built-in int() function, which truncates towards zero.</p>"},{"location":"nodes/numbers/#inputs_8","title":"Inputs","text":"Group Name Type Default Extras required number <code>FLOAT</code> 0 forceInput=True"},{"location":"nodes/numbers/#returns_9","title":"Returns","text":"Name Type int <code>INT</code> Source code <pre><code>class Float2Int:\n    \"\"\"Converts a floating-point number to an integer through truncation.\n\n    This class handles the conversion of float values to integers by removing the decimal portion.\n    The conversion is performed using Python's built-in int() function, which truncates towards zero.\n\n    Args:\n        number (float): The floating-point number to convert to an integer.\n\n    Returns:\n        tuple[int]: A single-element tuple containing the converted integer value.\n\n    Raises:\n        ValueError: If the input value is not a float.\n\n    Notes:\n        - Decimal portions are truncated, not rounded\n        - The returned value is always wrapped in a tuple to maintain consistency with the node system\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"number\": (\"FLOAT\", {\"default\": 0, \"forceInput\": True}),\n            }\n        }\n\n    RETURN_TYPES = (\"INT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = NUMBERS_CAT\n    CLASS_ID = \"float2int\"\n    DESCRIPTION = \"\"\"\n    Converts a floating-point number to an integer through truncation.\n    Removes the decimal portion of a float value, truncating towards zero rather than rounding.\n    Useful for workflows requiring integer values.\n    \"\"\"\n\n    def execute(self, number: float = 0.0) -&gt; tuple[int]:\n        try:\n            return (int(number),)\n        except (TypeError, ValueError):\n            raise ValueError(\"Number must be convertible to float\")\n</code></pre>"},{"location":"nodes/platform_io/","title":"Platform Io Nodes","text":""},{"location":"nodes/platform_io/#platformenvs","title":"PlatformEnvs","text":"<p>Retrieves organization ID and token based on the specified environment.</p>"},{"location":"nodes/platform_io/#inputs","title":"Inputs","text":"Group Name Type Default Extras required environment <code>LIST</code>"},{"location":"nodes/platform_io/#returns","title":"Returns","text":"Name Type string <code>STRING</code> string <code>STRING</code> Source code <pre><code>class PlatformEnvs:\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"environment\": ([\"production\", \"staging\", \"develop\"],),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"STRING\",\n        \"STRING\",\n    )\n    RETURN_NAMES = (\n        \"ORG_ID\",\n        \"ORG_TOKEN\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    DESCRIPTION = \"\"\"Retrieves organization ID and token based on the specified environment.\"\"\"\n\n    def execute(self, **kwargs):\n        env = kwargs.get(\"environment\") or \"\"\n        org_id = \"ORG_ID\" if env == \"production\" else f\"{env.upper()}_ORG_ID\"\n        org_token = \"ORG_TOKEN\" if env == \"production\" else f\"{env.upper()}_ORG_TOKEN\"\n        ORG_ID = os.environ.get(org_id)\n        ORG_TOKEN = os.environ.get(org_token)\n        return (\n            ORG_ID,\n            ORG_TOKEN,\n        )\n</code></pre>"},{"location":"nodes/platform_io/#inputnumber","title":"InputNumber","text":"<p>Processes numeric inputs with type conversion.</p> <p>Handles numeric input processing with support for both integer and float values, including automatic type conversion based on the specified subtype.</p>"},{"location":"nodes/platform_io/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required title <code>STRING</code> Input Number required subtype <code>LIST</code> required required <code>BOOLEAN</code> True required value <code>FLOAT</code> 0.0 max=18446744073709551615, step=0.1 required metadata <code>STRING</code> {} multiline=True Source code <pre><code>class InputNumber:\n    \"\"\"Processes numeric inputs with type conversion.\n\n    Handles numeric input processing with support for both integer and float values, including\n    automatic type conversion based on the specified subtype.\n\n    Args:\n        title (str): Display title for the number input. Defaults to \"Input Number\".\n        subtype (str): Type of number - either \"float\" or \"int\".\n        required (bool): Whether the input is required. Defaults to True.\n        value (float): The input numeric value. Defaults to 0.\n        metadata (str): JSON string containing additional metadata. Defaults to \"{}\".\n\n    Returns:\n        tuple[Union[int, float]]: A tuple containing the processed numeric value.\n\n    Raises:\n        ValueError: If value is not numeric or subtype is invalid.\n\n    Notes:\n        - Automatically converts between float and int based on subtype\n        - Maintains numeric precision during conversion\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"title\": (\"STRING\", {\"default\": \"Input Number\"}),\n                \"subtype\": ([\"float\", \"int\"],),\n                \"required\": (\"BOOLEAN\", {\"default\": True}),\n                \"value\": (\n                    \"FLOAT\",\n                    {\n                        \"default\": 0.0,\n                        \"min\": -18446744073709551615,\n                        \"max\": 18446744073709551615,\n                        \"step\": 0.1,\n                    },\n                ),\n                \"metadata\": (\"STRING\", {\"default\": \"{}\", \"multiline\": True}),\n            },\n        }\n\n    RETURN_TYPES = (any_type,)\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    DESCRIPTION = \"\"\"\n    Processes numeric inputs with type conversion.\n    Handles numeric input processing with support for both integer and float values,\n    including automatic type conversion based on the specified subtype.\n    \"\"\"\n\n    def execute(\n        self,\n        title: str = \"Input Number\",\n        subtype: str = \"float\",\n        required: bool = True,\n        value: float = 0,\n        metadata: str = \"{}\",\n    ) -&gt; tuple[float]:\n        if subtype == \"int\":\n            value = round(value)\n        elif subtype == \"float\":\n            value = float(value)\n        return (value,)\n</code></pre>"},{"location":"nodes/platform_io/#inputconnector","title":"InputConnector","text":"<p>Manages file downloads from external services using authentication tokens.</p> <p>Handles connections to external services (currently Google Drive) to download files using provided authentication tokens and file identifiers.</p>"},{"location":"nodes/platform_io/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required title <code>STRING</code> Input Connector required subtype <code>LIST</code> required required <code>BOOLEAN</code> True required override <code>BOOLEAN</code> False required token <code>STRING</code> required mime_type <code>STRING</code> image/png required value <code>STRING</code> required metadata <code>STRING</code> {} multiline=True"},{"location":"nodes/platform_io/#returns_1","title":"Returns","text":"Name Type file <code>FILE</code> Source code <pre><code>class InputConnector:\n    \"\"\"Manages file downloads from external services using authentication tokens.\n\n    Handles connections to external services (currently Google Drive) to download files using provided\n    authentication tokens and file identifiers.\n\n    Args:\n        title (str): Display title for the connector. Defaults to \"Input Connector\".\n        subtype (str): Service type, currently only supports \"google_drive\".\n        required (bool): Whether the input is required. Defaults to True.\n        override (bool): Whether to override existing files. Defaults to False.\n        token (str): Authentication token for the service.\n        mime_type (str): Expected MIME type of the file. Defaults to \"image/png\".\n        value (str): File identifier for the service.\n        metadata (str): JSON string containing additional metadata. Defaults to \"{}\".\n\n    Returns:\n        tuple[str]: A tuple containing the path to the downloaded file.\n\n    Raises:\n        ValueError: If token, value, mime_type are not strings or override is not boolean.\n\n    Notes:\n        - Files are downloaded to the ComfyUI input directory\n        - Supports Google Drive integration with proper authentication\n        - Can be extended to support other services in the future\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"title\": (\"STRING\", {\"default\": \"Input Connector\"}),\n                \"subtype\": ([\"google_drive\"],),\n                \"required\": (\"BOOLEAN\", {\"default\": True}),\n                \"override\": (\"BOOLEAN\", {\"default\": False}),\n                \"token\": (\"STRING\", {\"default\": \"\"}),\n                \"mime_type\": (\"STRING\", {\"default\": \"image/png\"}),\n                \"value\": (\"STRING\", {\"default\": \"\"}),\n                \"metadata\": (\"STRING\", {\"default\": \"{}\", \"multiline\": True}),\n            },\n        }\n\n    RETURN_TYPES = (\"FILE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    DEPRECATED = True\n    DESCRIPTION = \"\"\"\n    Manages file downloads from external services using authentication tokens.\n    Handles connections to external services (currently Google Drive) to download files\n    using provided authentication tokens and file identifiers.\n    \"\"\"\n\n    def execute(  # nosec: B107\n        self,\n        title: str = \"Input Connector\",\n        subtype: str = \"google_drive\",\n        required: bool = True,\n        override: bool = False,\n        token: str = \"\",\n        mime_type: str = \"image/png\",\n        value: str = \"\",\n        metadata: str = \"{}\",\n    ):\n        connector = GoogleConnector(token=token)\n        input_folder = os.path.join(BASE_COMFY_DIR, \"input\")\n        data = connector.download(\n            file_id=value,\n            mime_type=mime_type,\n            output_path=input_folder,\n            override=override,\n        )\n        return (data,)\n</code></pre>"},{"location":"nodes/platform_io/#inputtext","title":"InputText","text":"<p>Processes text input with fallback support.</p> <p>Handles text input processing with support for different subtypes and optional fallback values when input is empty.</p>"},{"location":"nodes/platform_io/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required title <code>STRING</code> Input Text required subtype <code>LIST</code> required required <code>BOOLEAN</code> True required value <code>STRING</code> multiline=True required metadata <code>STRING</code> {} multiline=True optional fallback <code>STRING</code> forceInput=True"},{"location":"nodes/platform_io/#returns_2","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class InputText:\n    \"\"\"Processes text input with fallback support.\n\n    Handles text input processing with support for different subtypes and optional fallback values\n    when input is empty.\n\n    Args:\n        title (str): Display title for the text input. Defaults to \"Input Text\".\n        subtype (str): Type of text - \"string\", \"positive_prompt\", or \"negative_prompt\".\n        required (bool): Whether the input is required. Defaults to True.\n        value (str): The input text value.\n        metadata (str): JSON string containing additional metadata. Defaults to \"{}\".\n        fallback (str): Optional fallback text if input is empty.\n\n    Returns:\n        tuple[str]: A tuple containing the processed text value.\n\n    Raises:\n        ValueError: If value or fallback are not strings.\n\n    Notes:\n        - Empty inputs will use the fallback value if provided\n        - Supports multiline text input\n        - Special handling for prompt-type inputs\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"title\": (\"STRING\", {\"default\": \"Input Text\"}),\n                \"subtype\": ([\"string\"],),\n                \"required\": (\"BOOLEAN\", {\"default\": True}),\n                \"value\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"metadata\": (\"STRING\", {\"default\": \"{}\", \"multiline\": True}),\n            },\n            \"optional\": {\n                \"fallback\": (\"STRING\", {\"forceInput\": True}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    DESCRIPTION = \"\"\"\n    Processes text input with fallback support.\n    Handles text input processing with support for different subtypes and optional fallback values when input is empty.\n    \"\"\"\n\n    def execute(\n        self,\n        title: str = \"Input Text\",\n        subtype: str = \"string\",\n        required: bool = True,\n        value: str = \"\",\n        metadata: str = \"{}\",\n        fallback: str = \"\",\n    ) -&gt; tuple[str]:\n        if fallback and value == \"\":\n            value = fallback\n        return (value,)\n</code></pre>"},{"location":"nodes/platform_io/#inputboolean","title":"InputBoolean","text":"<p>Processes boolean inputs for the platform.</p> <p>Handles boolean input processing with validation and type checking.</p>"},{"location":"nodes/platform_io/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required title <code>STRING</code> Input Boolean required subtype <code>LIST</code> boolean required required <code>BOOLEAN</code> True required value <code>BOOLEAN</code> False required metadata <code>STRING</code> {} multiline=True"},{"location":"nodes/platform_io/#returns_3","title":"Returns","text":"Name Type boolean <code>BOOLEAN</code> Source code <pre><code>class InputBoolean:\n    \"\"\"Processes boolean inputs for the platform.\n\n    Handles boolean input processing with validation and type checking.\n\n    Args:\n        title (str): Display title for the boolean input. Defaults to \"Input Boolean\".\n        subtype (str): Must be \"boolean\".\n        required (bool): Whether the input is required. Defaults to True.\n        value (bool): The input boolean value. Defaults to False.\n        metadata (str): JSON string containing additional metadata. Defaults to \"{}\".\n\n    Returns:\n        tuple[bool]: A tuple containing the boolean value.\n\n    Raises:\n        ValueError: If value is not a boolean.\n\n    Notes:\n        - Simple boolean validation and processing\n        - Returns original boolean value without modification\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"title\": (\"STRING\", {\"default\": \"Input Boolean\"}),\n                \"subtype\": ([\"boolean\"], {\"default\": \"boolean\"}),\n                \"required\": (\"BOOLEAN\", {\"default\": True}),\n                \"value\": (\"BOOLEAN\", {\"default\": False}),\n                \"metadata\": (\"STRING\", {\"default\": \"{}\", \"multiline\": True}),\n            }\n        }\n\n    RETURN_TYPES = (\"BOOLEAN\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    DESCRIPTION = \"\"\"\n    Processes boolean inputs for the platform.\n    Handles boolean input processing with validation and type checking.\n    Returns the original boolean value without modification.\n    \"\"\"\n\n    def execute(\n        self,\n        title: str = \"Input Boolean\",\n        subtype: str = \"boolean\",\n        required: bool = True,\n        value: bool = False,\n        metadata: str = \"{}\",\n    ) -&gt; tuple[bool]:\n        return (value,)\n</code></pre>"},{"location":"nodes/platform_io/#workflowexecutionmetadata","title":"WorkflowExecutionMetadata","text":"<p>Extracts platform execution metadata from a JSON string.</p> <p>This node parses a JSON string to extract platform execution metadata including backend API host, generate service host, organisation ID, and client ID.</p> <p>Parameters:     json_str (str): A JSON string containing platform execution metadata.</p>"},{"location":"nodes/platform_io/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras hidden json_str <code>STRING</code> {}"},{"location":"nodes/platform_io/#returns_4","title":"Returns","text":"Name Type string <code>STRING</code> string <code>STRING</code> string <code>STRING</code> string <code>STRING</code> string <code>STRING</code> string <code>STRING</code> string <code>STRING</code> Source code <pre><code>class WorkflowExecutionMetadata:\n    \"\"\"\n    Extracts platform execution metadata from a JSON string.\n\n    This node parses a JSON string to extract platform execution metadata\n    including backend API host, generate service host, organisation ID,\n    and client ID.\n\n    Parameters:\n        json_str (str): A JSON string containing platform execution metadata.\n\n    Returns:\n        tuple[str, str, str, str]: A tuple containing the extracted metadata values.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"hidden\": {\n                \"json_str\": (\n                    \"STRING\",\n                    {\n                        \"default\": \"{}\",\n                    },\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\", \"STRING\", \"STRING\", \"STRING\", \"STRING\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\n        \"backend_api_host\",\n        \"generate_service_host\",\n        \"organisation_id\",\n        \"user_id\",\n        \"environment\",\n        \"workflow_id\",\n        \"backend_cognito_secret\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    DESCRIPTION = \"\"\"\n    Extracts platform execution metadata from a JSON string.\n    Parses a JSON string to extract platform execution metadata\n    including backend API host, generate service host, organisation ID,\n    and client ID.\n    \"\"\"\n\n    def execute(self, json_str: str) -&gt; tuple[str, str, str, str, str, str, str]:\n        json_dict = json.loads(json_str)\n        return (\n            json_dict.get(\"backend_api_host\", \"\"),\n            json_dict.get(\"generate_service_host\", \"\"),\n            json_dict.get(\"organisation_id\", \"\"),\n            json_dict.get(\"user_id\", \"\"),\n            json_dict.get(\"environment\", \"\"),\n            json_dict.get(\"workflow_id\", \"\"),\n            json_dict.get(\"backend_cognito_secret\", \"\"),\n        )\n</code></pre>"},{"location":"nodes/platform_io/#inputimage","title":"InputImage","text":"<p>Processes and validates image inputs from various sources for the platform.</p> <p>This class handles image input processing, supporting both single and multiple images from URLs. It includes functionality for alpha channel management and mask generation.</p>"},{"location":"nodes/platform_io/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required title <code>STRING</code> Input Image required subtype <code>LIST</code> image required required <code>BOOLEAN</code> True required include_alpha <code>BOOLEAN</code> False required multiple <code>BOOLEAN</code> False required value <code>STRING</code> https://www.example.com/images/sample.jpg multiline=True required metadata <code>STRING</code> {} multiline=True optional fallback <code>any_type</code> Source code <pre><code>class InputImage:\n    \"\"\"Processes and validates image inputs from various sources for the platform.\n\n    This class handles image input processing, supporting both single and multiple images from URLs. It includes\n    functionality for alpha channel management and mask generation.\n\n    Args:\n        title (str): Display title for the input node. Defaults to \"Input Image\".\n        subtype (str): Type of input - either \"image\" or \"mask\".\n        required (bool): Whether the input is required. Defaults to True.\n        include_alpha (bool): Whether to preserve alpha channel. Defaults to False.\n        multiple (bool): Allow multiple image inputs. Defaults to False.\n        value (str): Image data as URL.\n        metadata (str): JSON string containing additional metadata. Defaults to \"{}\".\n        fallback (any): Optional fallback value if no input is provided.\n\n    Returns:\n        tuple[list]: A tuple containing a list of processed images as torch tensors in BWHC format.\n\n    Raises:\n        ValueError: If value is not a string, subtype is invalid, or no valid input is found.\n\n    Notes:\n        - URLs must start with \"http\" to be recognized\n        - Multiple images can be provided as comma-separated values\n        - Alpha channels are removed by default unless include_alpha is True\n        - Mask inputs are automatically converted to grayscale\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"title\": (\"STRING\", {\"default\": \"Input Image\"}),\n                \"subtype\": ([\"image\", \"mask\"], {\"default\": \"image\"}),\n                \"required\": (\"BOOLEAN\", {\"default\": True}),\n                \"include_alpha\": (\"BOOLEAN\", {\"default\": False}),\n                \"multiple\": (\"BOOLEAN\", {\"default\": False}),\n                \"value\": (\n                    \"STRING\",\n                    {\n                        \"default\": \"https://www.example.com/images/sample.jpg\",\n                        \"multiline\": True,\n                    },\n                ),\n                \"metadata\": (\"STRING\", {\"default\": \"{}\", \"multiline\": True}),\n            },\n            \"optional\": {\n                \"fallback\": (any_type,),\n            },\n        }\n\n    RETURN_TYPES = (any_type,)\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    OUTPUT_IS_LIST = (True,)\n    DESCRIPTION = \"\"\"# InputImage Node - Your Gateway for Images in ComfyUI\n\n    ## What it Does \ud83c\udfa8\n    This node is your main entry point for bringing images into ComfyUI workflows. Think of it as a universal image\n    loader that can handle:\n    - Images from web URLs (anything starting with \"http\")\n    - Single images or multiple images at once\n    - Regular images and masks\n    - Images with or without transparency\n\n    ## How to Use It \ud83d\ude80\n\n    ### Basic Settings\n    - **Title**: Just a label for your node (default: \"Input Image\")\n    - **Subtype**: Choose between:\n        - `image` - for regular images\n        - `mask` - for masks (automatically converts to grayscale)\n    - **Include Alpha**: Toggle transparency handling\n        - OFF: Removes transparency (default)\n        - ON: Keeps transparency channel\n    - **Multiple**: Allow multiple images\n        - OFF: Takes only first image (default)\n        - ON: Processes all provided images\n\n    ### Input Methods\n    1. Web Images: Just paste the image URL (must start with \"http\")\n    2. Multiple Images: With \"Multiple\" enabled, separate URLs with spaces\n    3. Fallback: Optional backup image if main input fails\n\n    ## Tips &amp; Tricks \ud83d\udca1\n    - For batch processing, enable \"Multiple\" and input several URLs separated by spaces\n    - When working with masks, set \"subtype\" to \"mask\" for automatic grayscale conversion\n    - If you need transparency in your workflow, make sure to enable \"Include Alpha\"\n    - The node automatically handles various image formats and color spaces\n\n    ## Output\n    - Outputs images in the format ComfyUI expects (BCHW tensor format)\n    - Perfect for feeding into other ComfyUI nodes like upscalers, ControlNet, or image processors\n\n    Think of this node as your universal image importer - it handles all the technical conversion stuff so you can focus\n    on the creative aspects of your workflow! \ud83c\udfa8\u2728\"\"\"\n\n    def execute(\n        self,\n        title: str = \"Input Image\",\n        subtype: str = \"image\",\n        required: bool = True,\n        include_alpha: bool = False,\n        multiple: bool = False,\n        value: str = \"\",\n        metadata: str = \"{}\",\n        fallback: Any = None,\n    ) -&gt; tuple[list[torch.Tensor]]:\n        def post_process(output: TensorImage, include_alpha: bool) -&gt; TensorImage:\n            if output.shape[1] not in [3, 4]:\n                if len(output.shape) == 2:  # (H,W)\n                    output = TensorImage(output.unsqueeze(0).unsqueeze(0).expand(-1, 3, -1, -1))\n                elif len(output.shape) == 3:  # (B,H,W)\n                    output = TensorImage(output.unsqueeze(1).expand(-1, 3, -1, -1))\n                elif len(output.shape) == 4 and output.shape[1] == 1:  # (B,1,H,W)\n                    output = TensorImage(output.expand(-1, 3, -1, -1))\n                else:\n                    raise ValueError(f\"Unsupported shape: {output.shape}\")\n            else:\n                if not include_alpha and output.shape[1] == 4:\n                    rgb = TensorImage(output[:, :3, :, :])\n                    alpha = TensorImage(output[:, -1, :, :])\n                    output, _ = cutout(rgb, alpha)\n            return output\n\n        def process_value(value: str, multiple: bool) -&gt; list[str]:\n            if not value:\n                return []\n            if \" \" in value:\n                items = value.split(\" \")\n                return items if multiple else [items[0]]\n            return [value]\n\n        def load_image(url: str) -&gt; Optional[TensorImage]:\n            if not url:\n                raise ValueError(\"Empty input string\")\n\n            try:\n                if url.startswith(\"http\"):\n                    return TensorImage.from_web(url)\n            except Exception as e:\n                raise ValueError(f\"Unsupported input format: {url}\") from e\n\n        value_list = process_value(value, multiple)\n        outputs: list[torch.Tensor] = []\n\n        # Process each input value\n        for item in value_list:\n            if isinstance(item, str):\n                try:\n                    output = load_image(item)\n                    if output is not None:\n                        outputs.append(output)\n                except ValueError as e:\n                    if not outputs:\n                        raise e\n\n        if len(outputs) == 0:\n            if fallback is None:\n                raise ValueError(\"No input found and no fallback provided\")\n            outputs.append(TensorImage.from_BWHC(fallback))\n\n        for i, output in enumerate(outputs):\n            if not isinstance(output, TensorImage):\n                raise ValueError(f\"Output {i} must be a TensorImage\")\n\n            if subtype == \"mask\":\n                if output.shape[1] == 4:\n                    rgb = TensorImage(output[:, :3, :, :])\n                    outputs[i] = rgb_to_grayscale(rgb).get_BWHC()\n                else:\n                    outputs[i] = output.get_BWHC()\n            else:\n                outputs[i] = post_process(output, include_alpha).get_BWHC()\n        return (outputs,)\n</code></pre>"},{"location":"nodes/platform_io/#report","title":"Report","text":"<p>Manages report generation and output.     Handles the generation and output of reports with support for various data types and formats.     Includes support for metadata management and output formatting.</p>"},{"location":"nodes/platform_io/#inputs_7","title":"Inputs","text":"Group Name Type Default Extras required value <code>any_type</code> required report <code>STRING</code> multiline=True Source code <pre><code>class Report:\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (any_type,),\n                \"report\": (\"STRING\", {\"default\": \"\", \"multiline\": True}),\n            },\n        }\n\n    RETURN_TYPES = (any_type,)\n    RETURN_NAMES = (\"value\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    OUTPUT_NODE = True\n    DESCRIPTION = \"\"\"Manages report generation and output.\n    Handles the generation and output of reports with support for various data types and formats.\n    Includes support for metadata management and output formatting.\"\"\"\n\n    def execute(self, value, report):\n        data = {\"type\": \"string\", \"value\": report}\n        return {\"ui\": {\"signature_report\": [data]}, \"result\": (value,)}\n</code></pre>"},{"location":"nodes/platform_io/#output","title":"Output","text":"<p>Manages output processing and file saving for various data types.</p> <p>Handles the processing and saving of different output types including images, masks, numbers, and strings. Includes support for thumbnail generation and metadata management.</p>"},{"location":"nodes/platform_io/#inputs_8","title":"Inputs","text":"Group Name Type Default Extras required title <code>STRING</code> Output Image required subtype <code>LIST</code> required metadata <code>STRING</code> {} multiline=True required value <code>any_type</code> hidden output_path <code>STRING</code> output Source code <pre><code>class Output:\n    \"\"\"Manages output processing and file saving for various data types.\n\n    Handles the processing and saving of different output types including images, masks, numbers, and\n    strings. Includes support for thumbnail generation and metadata management.\n\n    Args:\n        title (str): Display title for the output. Defaults to \"Output Image\".\n        subtype (str): Type of output - \"image\", \"mask\", \"int\", \"float\", \"string\", or \"dict\".\n        metadata (str): JSON string containing additional metadata.\n        value (any): The value to output.\n        output_path (str): Path for saving outputs. Defaults to \"output\".\n\n    Returns:\n        dict: UI configuration with signature_output containing processed results.\n\n    Raises:\n        ValueError: If inputs are invalid or output type is unsupported.\n\n    Notes:\n        - Automatically generates thumbnails for image outputs\n        - Saves images with unique filenames including timestamps\n        - Supports batch processing of multiple outputs\n        - Creates both full-size PNG and compressed JPEG thumbnails\n        - Handles various data types with appropriate serialization\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"title\": (\"STRING\", {\"default\": \"Output Image\"}),\n                \"subtype\": ([\"image\", \"mask\", \"int\", \"float\", \"string\", \"dict\"],),\n                \"metadata\": (\"STRING\", {\"default\": \"{}\", \"multiline\": True}),\n                \"value\": (any_type,),\n            },\n            \"hidden\": {\n                \"output_path\": (\"STRING\", {\"default\": \"output\"}),\n            },\n        }\n\n    RETURN_TYPES = ()\n    OUTPUT_NODE = True\n    INPUT_IS_LIST = True\n    FUNCTION = \"execute\"\n    CATEGORY = PLATFORM_IO_CAT\n    DESCRIPTION = \"\"\"\n    Manages output processing and file saving for various data types.\n    Handles the processing and saving of different output types including images, masks, numbers, and strings.\n    Includes support for thumbnail generation and metadata management.\n    \"\"\"\n\n    @classmethod\n    def IS_CHANGED(cls, **kwargs):  # type: ignore\n        return time.time()\n\n    def __save_outputs(self, **kwargs) -&gt; dict | None:\n        img = kwargs.get(\"img\")\n        if not isinstance(img, (torch.Tensor, TensorImage)):\n            raise ValueError(\"Image must be a tensor or TensorImage\")\n\n        title = kwargs.get(\"title\", \"\")\n        if not isinstance(title, str):\n            title = str(title)\n\n        subtype = kwargs.get(\"subtype\", \"image\")\n        if not isinstance(subtype, str):\n            subtype = str(subtype)\n\n        thumbnail_size = kwargs.get(\"thumbnail_size\", 1024)\n        if not isinstance(thumbnail_size, int):\n            try:\n                thumbnail_size = int(thumbnail_size)\n            except (ValueError, TypeError):\n                thumbnail_size = 1024\n\n        output_dir = kwargs.get(\"output_dir\", \"output\")\n        if not isinstance(output_dir, str):\n            output_dir = str(output_dir)\n\n        metadata = kwargs.get(\"metadata\", \"\")\n        if not isinstance(metadata, str):\n            metadata = str(metadata)\n\n        current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        file_name = f\"signature_{current_time_str}_{uuid7str()}.png\"\n        save_path = os.path.join(output_dir, file_name)\n        if os.path.exists(save_path):\n            file_name = f\"signature_{current_time_str}_{uuid7str()}_{uuid7str()}.png\"\n            save_path = os.path.join(output_dir, file_name)\n\n        output_img = img if isinstance(img, TensorImage) else TensorImage(img)\n\n        thumbnail_img = output_img.get_resized(thumbnail_size)\n        thumbnail_path = save_path.replace(\".png\", \"_thumbnail.jpeg\")\n        thumbnail_file_name = file_name.replace(\".png\", \"_thumbnail.jpeg\")\n        thumbnail_saved = thumbnail_img.save(thumbnail_path)\n\n        image_saved = output_img.save(save_path)\n\n        if image_saved and thumbnail_saved:\n            return {\n                \"title\": title,\n                \"type\": subtype,\n                \"metadata\": metadata,\n                \"value\": file_name,\n                \"thumbnail\": thumbnail_file_name if thumbnail_saved else None,\n            }\n\n        return None\n\n    def execute(self, **kwargs):\n        title_list = kwargs.get(\"title\")\n        if not isinstance(title_list, list):\n            raise ValueError(\"Title must be a list\")\n        metadata_list = kwargs.get(\"metadata\")\n        if not isinstance(metadata_list, list):\n            raise ValueError(\"Metadata must be a list\")\n        subtype_list = kwargs.get(\"subtype\")\n        if not isinstance(subtype_list, list):\n            raise ValueError(\"Subtype must be a list\")\n        output_path_list = kwargs.get(\"output_path\")\n        if not isinstance(output_path_list, list):\n            output_path_list = [\"output\"] * len(title_list)\n        value_list = kwargs.get(\"value\")\n        if not isinstance(value_list, list):\n            raise ValueError(\"Value must be a list\")\n        main_subtype = subtype_list[0]\n        supported_types = [\"image\", \"mask\", \"int\", \"float\", \"string\", \"dict\"]\n        if main_subtype not in supported_types:\n            raise ValueError(f\"Unsupported output type: {main_subtype}\")\n\n        results = []\n        thumbnail_size = 1024\n        for idx, item in enumerate(value_list):\n            title = title_list[idx]\n            metadata = metadata_list[idx]\n            output_dir = os.path.join(BASE_COMFY_DIR, output_path_list[idx])\n            if isinstance(item, torch.Tensor):\n                if main_subtype in [\"image\", \"mask\"]:\n                    tensor_images = TensorImage.from_BWHC(item.to(\"cpu\"))\n                    for img in tensor_images:\n                        result = self.__save_outputs(\n                            img=img,\n                            title=title,\n                            subtype=main_subtype,\n                            thumbnail_size=thumbnail_size,\n                            output_dir=output_dir,\n                            metadata=metadata,\n                        )\n                        if result:\n                            results.append(result)\n                else:\n                    raise ValueError(f\"Unsupported output type: {type(item)}\")\n            else:\n                value_json = json.dumps(item) if main_subtype == \"dict\" else item\n                results.append(\n                    {\n                        \"title\": title,\n                        \"type\": main_subtype,\n                        \"metadata\": metadata,\n                        \"value\": value_json,\n                    }\n                )\n        return {\"ui\": {\"signature_output\": results}}\n</code></pre>"},{"location":"nodes/primitives/","title":"Primitives Nodes","text":""},{"location":"nodes/primitives/#stringmultiline","title":"StringMultiline","text":"<p>A node that handles multi-line string inputs.</p> <p>This node provides functionality for processing multi-line text input. It can be used as a basic input node in computational graphs where larger text blocks or formatted text processing is required.</p>"},{"location":"nodes/primitives/#inputs","title":"Inputs","text":"Group Name Type Default Extras required value <code>STRING</code> multiline=True"},{"location":"nodes/primitives/#returns","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class StringMultiline:\n    \"\"\"A node that handles multi-line string inputs.\n\n    This node provides functionality for processing multi-line text input. It can be used as a\n    basic input node in computational graphs where larger text blocks or formatted text\n    processing is required.\n\n    Args:\n        value (str): The input multi-line string to process.\n                    Default: \"\" (empty string)\n\n    Returns:\n        tuple[str]: A single-element tuple containing the processed multi-line string value.\n\n    Notes:\n        - The node maintains the exact input value without any transformation\n        - Newline characters are preserved in the input\n        - Suitable for longer text inputs, code blocks, or formatted text\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (\"STRING\", {\"default\": \"\", \"multiline\": True}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PRIMITIVES_CAT\n    DESCRIPTION = \"\"\"\n    A node that handles multi-line string inputs.\n    Provides functionality for processing multi-line text input.\n    Can be used as a basic input node in computational graphs where,\n    larger text blocks or formatted text processing is required.\n    \"\"\"\n\n    def execute(self, value: str = \"\") -&gt; tuple[str]:\n        return (value,)\n</code></pre>"},{"location":"nodes/primitives/#float","title":"Float","text":"<p>A node that handles floating-point number inputs with configurable parameters.</p> <p>This node provides functionality for processing floating-point numbers within a specified range and step size. It can be used as a basic input node in computational graphs where decimal number precision is required.</p>"},{"location":"nodes/primitives/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required value <code>FLOAT</code> 0 max=18446744073709551615, step=0.01"},{"location":"nodes/primitives/#returns_1","title":"Returns","text":"Name Type float <code>FLOAT</code> Source code <pre><code>class Float:\n    \"\"\"A node that handles floating-point number inputs with configurable parameters.\n\n    This node provides functionality for processing floating-point numbers within a specified range\n    and step size. It can be used as a basic input node in computational graphs where decimal\n    number precision is required.\n\n    Args:\n        value (float): The input floating-point number to process.\n                      Default: 0\n                      Min: -18446744073709551615\n                      Max: 18446744073709551615\n                      Step: 0.01\n\n    Returns:\n        tuple[float]: A single-element tuple containing the processed float value.\n\n    Notes:\n        - The node maintains the exact input value without any transformation\n        - The step value of 0.01 provides two decimal places of precision by default\n        - The min/max values correspond to the 64-bit integer limits\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (\n                    \"FLOAT\",\n                    {\n                        \"default\": 0,\n                        \"min\": -18446744073709551615,\n                        \"max\": 18446744073709551615,\n                        \"step\": 0.01,\n                    },\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\"FLOAT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PRIMITIVES_CAT\n    DESCRIPTION = \"\"\"\n    A node that handles floating-point number inputs with configurable parameters.\n    Provides functionality for processing floating-point numbers within a specified range and step size.\n    Can be used as a basic input node in computational graphs where decimal number precision is required.\n    \"\"\"\n\n    def execute(self, value: float = 0) -&gt; tuple[float]:\n        return (value,)\n</code></pre>"},{"location":"nodes/primitives/#boolean","title":"Boolean","text":"<p>A node that handles boolean inputs.</p> <p>This node provides functionality for processing boolean (True/False) values. It can be used as a basic input node in computational graphs where conditional logic is required.</p>"},{"location":"nodes/primitives/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required value <code>BOOLEAN</code> False"},{"location":"nodes/primitives/#returns_2","title":"Returns","text":"Name Type boolean <code>BOOLEAN</code> Source code <pre><code>class Boolean:\n    \"\"\"A node that handles boolean inputs.\n\n    This node provides functionality for processing boolean (True/False) values. It can be used\n    as a basic input node in computational graphs where conditional logic is required.\n\n    Args:\n        value (bool): The input boolean value to process.\n                     Default: False\n\n    Returns:\n        tuple[bool]: A single-element tuple containing the processed boolean value.\n\n    Notes:\n        - The node maintains the exact input value without any transformation\n        - Typically displayed as a checkbox in user interfaces\n        - Useful for conditional branching in node graphs\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (\"BOOLEAN\", {\"default\": False}),\n            },\n        }\n\n    RETURN_TYPES = (\"BOOLEAN\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PRIMITIVES_CAT\n    DESCRIPTION = \"\"\"\n    A node that handles boolean inputs.\n    Provides functionality for processing boolean (True/False) values.\n    Can be used as a basic input node in computational graphs where conditional logic is required.\n    \"\"\"\n\n    def execute(self, value: bool = False) -&gt; tuple[bool]:\n        return (value,)\n</code></pre>"},{"location":"nodes/primitives/#string","title":"String","text":"<p>A node that handles single-line string inputs.</p> <p>This node provides functionality for processing single-line text input. It can be used as a basic input node in computational graphs where text processing is required.</p>"},{"location":"nodes/primitives/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required value <code>STRING</code>"},{"location":"nodes/primitives/#returns_3","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class StringMultiline:\n    \"\"\"A node that handles multi-line string inputs.\n\n    This node provides functionality for processing multi-line text input. It can be used as a\n    basic input node in computational graphs where larger text blocks or formatted text\n    processing is required.\n\n    Args:\n        value (str): The input multi-line string to process.\n                    Default: \"\" (empty string)\n\n    Returns:\n        tuple[str]: A single-element tuple containing the processed multi-line string value.\n\n    Notes:\n        - The node maintains the exact input value without any transformation\n        - Newline characters are preserved in the input\n        - Suitable for longer text inputs, code blocks, or formatted text\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (\"STRING\", {\"default\": \"\", \"multiline\": True}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PRIMITIVES_CAT\n    DESCRIPTION = \"\"\"\n    A node that handles multi-line string inputs.\n    Provides functionality for processing multi-line text input.\n    Can be used as a basic input node in computational graphs where,\n    larger text blocks or formatted text processing is required.\n    \"\"\"\n\n    def execute(self, value: str = \"\") -&gt; tuple[str]:\n        return (value,)\n</code></pre>"},{"location":"nodes/primitives/#joinstringmulti","title":"JoinStringMulti","text":"<p>Creates single string, or a list of strings, from multiple input strings. You can set how many inputs the node has, with the inputcount and clicking update.</p>"},{"location":"nodes/primitives/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required inputcount <code>INT</code> 2 min=2, max=1000, step=1 required delimiter <code>STRING</code> multiline=False required return_list <code>BOOLEAN</code> False required string_1 <code>STRING</code> forceInput=True required string_2 <code>STRING</code> forceInput=True"},{"location":"nodes/primitives/#returns_4","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class JoinStringMulti:\n    \"\"\"\n    Creates single string, or a list of strings, from\n    multiple input strings.\n    You can set how many inputs the node has,\n    with the **inputcount** and clicking update.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"inputcount\": (\"INT\", {\"default\": 2, \"min\": 2, \"max\": 1000, \"step\": 1}),\n                \"delimiter\": (\"STRING\", {\"default\": \" \", \"multiline\": False}),\n                \"return_list\": (\"BOOLEAN\", {\"default\": False}),\n                \"string_1\": (\"STRING\", {\"default\": \"\", \"forceInput\": True}),\n                \"string_2\": (\"STRING\", {\"default\": \"\", \"forceInput\": True}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"combine\"\n    CATEGORY = PRIMITIVES_CAT\n    DESCRIPTION = \"\"\"\n    Creates single string, or a list of strings, from multiple input strings.\n    You can set how many inputs the node has, with the **inputcount** and clicking update.\n    \"\"\"\n\n    def combine(\n        self,\n        inputcount: int = 2,\n        delimiter: str = \" \",\n        return_list: bool = False,\n        **kwargs,\n    ) -&gt; tuple[str] | tuple[list[str]]:\n        string = kwargs[\"string_1\"]\n        strings = [string]  # Initialize a list with the first string\n        for c in range(1, inputcount):\n            new_string = kwargs[f\"string_{c + 1}\"]\n            if return_list:\n                strings.append(new_string)  # Add new string to the list\n            else:\n                string = string + delimiter + new_string\n        if return_list:\n            return (strings,)  # Return the list of strings\n        else:\n            return (string,)  # Return the combined string\n</code></pre>"},{"location":"nodes/primitives/#int","title":"Int","text":"<p>A node that handles integer number inputs with configurable parameters.</p> <p>This node provides functionality for processing integer numbers within a specified range and step size. It can be used as a basic input node in computational graphs where whole number values are required.</p>"},{"location":"nodes/primitives/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required value <code>INT</code> 0 max=18446744073709551615, step=1"},{"location":"nodes/primitives/#returns_5","title":"Returns","text":"Name Type int <code>INT</code> Source code <pre><code>class Int:\n    \"\"\"A node that handles integer number inputs with configurable parameters.\n\n    This node provides functionality for processing integer numbers within a specified range\n    and step size. It can be used as a basic input node in computational graphs where whole\n    number values are required.\n\n    Args:\n        value (int): The input integer number to process.\n                    Default: 0\n                    Min: -18446744073709551615\n                    Max: 18446744073709551615\n                    Step: 1\n\n    Returns:\n        tuple[int]: A single-element tuple containing the processed integer value.\n\n    Notes:\n        - The node maintains the exact input value without any transformation\n        - The step value of 1 ensures whole number increments\n        - The min/max values correspond to the 64-bit integer limits\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (\n                    \"INT\",\n                    {\n                        \"default\": 0,\n                        \"min\": -18446744073709551615,\n                        \"max\": 18446744073709551615,\n                        \"step\": 1,\n                    },\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\"INT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = PRIMITIVES_CAT\n    DESCRIPTION = \"\"\"\n    A node that handles integer number inputs with configurable parameters.\n    Provides functionality for processing integer numbers within a specified range and step size.\n    Can be used as a basic input node in computational graphs where whole number values are required.\n    \"\"\"\n\n    def execute(self, value: int = 0) -&gt; tuple[int]:\n        return (value,)\n</code></pre>"},{"location":"nodes/s3/","title":"S3 Nodes","text":""},{"location":"nodes/segmentation/","title":"Segmentation Nodes","text":""},{"location":"nodes/storage/","title":"Storage Nodes","text":""},{"location":"nodes/tabular/","title":"Tabular Nodes","text":""},{"location":"nodes/text/","title":"Text Nodes","text":""},{"location":"nodes/text/#textsplit","title":"TextSplit","text":"<p>Splits text into a list of segments using a specified delimiter.</p> <p>A utility node that divides input text into multiple segments based on a delimiter, creating a list of substrings.</p>"},{"location":"nodes/text/#inputs","title":"Inputs","text":"Group Name Type Default Extras required text <code>STRING</code> forceInput=True required delimiter <code>STRING</code>"},{"location":"nodes/text/#returns","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class TextSplit:\n    \"\"\"Splits text into a list of segments using a specified delimiter.\n\n    A utility node that divides input text into multiple segments based on a delimiter,\n    creating a list of substrings.\n\n    Args:\n        text (str): The input text to be split. Required.\n        delimiter (str): The character or string to use as the splitting point. Defaults to space.\n\n    Returns:\n        tuple[list[str]]: A single-element tuple containing a list of split text segments.\n\n    Notes:\n        - Empty input text will result in a list with one empty string\n        - If the delimiter is not found, the result will be a single-element list\n        - Consecutive delimiters will result in empty strings in the output list\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"text\": (\"STRING\", {\"forceInput\": True}),\n                \"delimiter\": (\"STRING\", {\"default\": \" \"}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = TEXT_CAT\n    OUTPUT_IS_LIST = (True,)\n    DESCRIPTION = \"\"\"\n    Splits text into a list of segments using a specified delimiter.\n    Divides input text into multiple segments based on a delimiter,\n    creating a list of substrings.\n    \"\"\"\n\n    def execute(self, text: str, delimiter: str = \" \") -&gt; tuple[list[str]]:\n        return (text.split(delimiter),)\n</code></pre>"},{"location":"nodes/text/#texttrim","title":"TextTrim","text":"<p>Removes whitespace from text according to specified trimming rules.</p> <p>A utility node that trims whitespace from text input, offering options to remove whitespace from the beginning, end, or both sides of the text.</p>"},{"location":"nodes/text/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required text <code>STRING</code> forceInput=True required trim_type <code>LIST</code>"},{"location":"nodes/text/#returns_1","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class TextTrim:\n    \"\"\"Removes whitespace from text according to specified trimming rules.\n\n    A utility node that trims whitespace from text input, offering options to remove whitespace\n    from the beginning, end, or both sides of the text.\n\n    Args:\n        text (str): The input text to be trimmed. Required.\n        trim_type (str): The type of trimming to apply. Must be one of:\n            - 'both': Trim whitespace from both ends\n            - 'left': Trim whitespace from the start\n            - 'right': Trim whitespace from the end\n\n    Returns:\n        tuple[str]: A single-element tuple containing the trimmed text.\n\n    Notes:\n        - Whitespace includes spaces, tabs, and newlines\n        - Empty input text will result in an empty string output\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"text\": (\"STRING\", {\"forceInput\": True}),\n                \"trim_type\": ([\"both\", \"left\", \"right\"],),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = TEXT_CAT\n    DESCRIPTION = \"\"\"\n    Removes whitespace from text according to specified trimming rules.\n    Trims whitespace from text input, offering options to remove whitespace from the beginning,\n    end, or both sides of the text.\n    \"\"\"\n\n    def execute(self, text: str, trim_type: str = \"both\") -&gt; tuple[str]:\n        trim_types = {\n            \"both\": text.strip,\n            \"left\": text.lstrip,\n            \"right\": text.rstrip,\n        }\n        return (trim_types[trim_type](),)\n</code></pre>"},{"location":"nodes/text/#textfindreplace","title":"TextFindReplace","text":"<p>Performs simple text replacement without regex support.</p> <p>A straightforward text processing node that replaces all occurrences of a substring with another substring, using exact matching.</p>"},{"location":"nodes/text/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required text <code>STRING</code> required find <code>STRING</code> required replace <code>STRING</code>"},{"location":"nodes/text/#returns_2","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class TextFindReplace:\n    \"\"\"Performs simple text replacement without regex support.\n\n    A straightforward text processing node that replaces all occurrences of a substring with\n    another substring, using exact matching.\n\n    Args:\n        text (str): The input text to process. Defaults to empty string.\n        find (str): The substring to search for. Defaults to empty string.\n        replace (str): The substring to replace matches with. Defaults to empty string.\n\n    Returns:\n        tuple[str]: A single-element tuple containing the processed text.\n\n    Notes:\n        - Case-sensitive matching\n        - All occurrences of the 'find' string will be replaced\n        - Empty strings for any parameter are handled safely\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"text\": (\"STRING\", {\"default\": \"\"}),\n                \"find\": (\"STRING\", {\"default\": \"\"}),\n                \"replace\": (\"STRING\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = TEXT_CAT\n    DESCRIPTION = \"\"\"\n    Performs simple text replacement without regex support.\n    Replaces all occurrences of a substring with another substring,\n    using exact matching.\n    \"\"\"\n\n    def execute(self, text: str = \"\", find: str = \"\", replace: str = \"\") -&gt; tuple[str]:\n        return (text.replace(find, replace),)\n</code></pre>"},{"location":"nodes/text/#textregexreplace","title":"TextRegexReplace","text":"<p>Performs pattern-based text replacement using regular expressions.</p> <p>A powerful text processing node that uses regex patterns to find and replace text patterns, supporting complex pattern matching and replacement operations.</p>"},{"location":"nodes/text/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required text <code>STRING</code> forceInput=True required pattern <code>STRING</code> required replacement <code>STRING</code>"},{"location":"nodes/text/#returns_3","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class TextRegexReplace:\n    \"\"\"Performs pattern-based text replacement using regular expressions.\n\n    A powerful text processing node that uses regex patterns to find and replace text patterns,\n    supporting complex pattern matching and replacement operations.\n\n    Args:\n        text (str): The input text to process. Required.\n        pattern (str): The regular expression pattern to match. Required.\n        replacement (str): The string to use as replacement for matched patterns. Required.\n\n    Returns:\n        tuple[str]: A single-element tuple containing the processed text.\n\n    Notes:\n        - Invalid regex patterns will cause errors\n        - Empty pattern or replacement strings are allowed\n        - Supports all Python regex syntax including groups and backreferences\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"text\": (\"STRING\", {\"forceInput\": True}),\n                \"pattern\": (\"STRING\", {\"default\": \"\"}),\n                \"replacement\": (\"STRING\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = TEXT_CAT\n    DESCRIPTION = \"\"\"\n    Performs pattern-based text replacement using regular expressions.\n    Uses regex patterns to find and replace text patterns,\n    supporting complex pattern matching and replacement operations.\n    \"\"\"\n\n    def execute(self, text: str, pattern: str = \"\", replacement: str = \"\") -&gt; tuple[str]:\n        return (re.sub(pattern, replacement, text),)\n</code></pre>"},{"location":"nodes/text/#textcase","title":"TextCase","text":"<p>Transforms text case according to specified formatting rules.</p> <p>A utility node that provides various case transformation options for input text, including lowercase, uppercase, capitalization, and title case conversion.</p>"},{"location":"nodes/text/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required text <code>STRING</code> forceInput=True required case <code>LIST</code>"},{"location":"nodes/text/#returns_4","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class TextCase:\n    \"\"\"Transforms text case according to specified formatting rules.\n\n    A utility node that provides various case transformation options for input text, including\n    lowercase, uppercase, capitalization, and title case conversion.\n\n    Args:\n        text (str): The input text to be transformed. Required.\n        case (str): The case transformation to apply. Must be one of:\n            - 'lower': Convert text to lowercase\n            - 'upper': Convert text to uppercase\n            - 'capitalize': Capitalize the first character\n            - 'title': Convert text to title case\n\n    Returns:\n        tuple[str]: A single-element tuple containing the transformed text.\n\n    Notes:\n        - Empty input text will result in an empty string output\n        - The transformation preserves any existing spacing and special characters\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"text\": (\"STRING\", {\"forceInput\": True}),\n                \"case\": ([\"lower\", \"upper\", \"capitalize\", \"title\"],),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = TEXT_CAT\n    DESCRIPTION = \"\"\"\n    Transforms text case according to specified formatting rules.\n    Provides various case transformation options for input text,\n    including lowercase, uppercase, capitalization, and title case conversion.\n    \"\"\"\n\n    def execute(self, text: str, case: str = \"lower\") -&gt; tuple[str]:\n        return (getattr(text, case)(),)\n</code></pre>"},{"location":"nodes/text/#textconcatenate","title":"TextConcatenate","text":"<p>Combines two text strings into a single string.</p> <p>A basic text manipulation node that joins two input strings together in sequence, without any separator between them.</p>"},{"location":"nodes/text/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required text1 <code>STRING</code> required text2 <code>STRING</code>"},{"location":"nodes/text/#returns_5","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class TextConcatenate:\n    \"\"\"Combines two text strings into a single string.\n\n    A basic text manipulation node that joins two input strings together in sequence,\n    without any separator between them.\n\n    Args:\n        text1 (str): The first text string to concatenate. Defaults to empty string.\n        text2 (str): The second text string to concatenate. Defaults to empty string.\n\n    Returns:\n        tuple[str]: A single-element tuple containing the concatenated text.\n\n    Notes:\n        - No separator is added between the strings\n        - Empty strings are handled safely\n        - The result will be the direct combination of text1 followed by text2\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"text1\": (\"STRING\", {\"default\": \"\"}),\n                \"text2\": (\"STRING\", {\"default\": \"\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = TEXT_CAT\n    DESCRIPTION = \"\"\"\n    Combines two text strings into a single string.\n    Joins two input strings together in sequence, without any separator between them.\n    \"\"\"\n\n    def execute(self, text1: str = \"\", text2: str = \"\") -&gt; tuple[str]:\n        return (text1 + text2,)\n</code></pre>"},{"location":"nodes/text/#textpreview","title":"TextPreview","text":"<p>Processes and generates a preview of text inputs, supporting both strings and tensors.</p> <p>This node takes a list of text inputs and generates a formatted preview string. For tensor inputs, it includes shape information in the preview. The node is designed to handle multiple input types and provide a consistent preview format.</p>"},{"location":"nodes/text/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required value <code>any_type</code> Source code <pre><code>class TextPreview:\n    \"\"\"Processes and generates a preview of text inputs, supporting both strings and tensors.\n\n    This node takes a list of text inputs and generates a formatted preview string. For tensor inputs,\n    it includes shape information in the preview. The node is designed to handle multiple input types\n    and provide a consistent preview format.\n\n    Args:\n        text (Any): A list of text inputs that can be strings, tensors, or other objects that can be\n            converted to strings.\n\n    Returns:\n        dict: A dictionary containing:\n            - ui (dict): UI-specific data with the preview text under the 'text' key\n            - result (tuple): A tuple containing the generated preview string\n\n    Notes:\n        - Tensor inputs are displayed with their shape information\n        - Multiple inputs are separated by newlines\n        - None values are skipped in the preview generation\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (any_type,),\n            },\n        }\n\n    INPUT_IS_LIST = True\n    RETURN_TYPES = ()\n    FUNCTION = \"execute\"\n    OUTPUT_NODE = True\n    CATEGORY = TEXT_CAT\n    DESCRIPTION = \"\"\"\n    Processes and generates a preview of text inputs, supporting both strings and tensors.\n    Takes a list of text inputs and generates a formatted preview string. For tensor inputs,\n    it includes shape information in the preview. The node is designed to handle multiple input types\n    and provide a consistent preview format.\n    \"\"\"\n\n    def execute(self, value: Any = []) -&gt; tuple[dict]:\n        text_string = \"\"\n        for t in value:\n            if t is None:\n                continue\n            if text_string != \"\":\n                text_string += \"\\n\"\n            text_string += str(t.shape) if isinstance(t, torch.Tensor) else str(t)\n        return {\"ui\": {\"text\": [text_string]}}  # type: ignore\n</code></pre>"},{"location":"nodes/utils/","title":"Utils Nodes","text":""},{"location":"nodes/utils/#any2string","title":"Any2String","text":"<p>Converts any input value to its string representation.</p> <p>This utility node provides a simple way to convert any input value into a string format using Python's built-in str() function. Useful for debugging, logging, or text-based operations.</p>"},{"location":"nodes/utils/#inputs","title":"Inputs","text":"Group Name Type Default Extras required value <code>any_type</code>"},{"location":"nodes/utils/#returns","title":"Returns","text":"Name Type string <code>STRING</code> Source code <pre><code>class Any2String:\n    \"\"\"Converts any input value to its string representation.\n\n    This utility node provides a simple way to convert any input value into a string format using\n    Python's built-in str() function. Useful for debugging, logging, or text-based operations.\n\n    Args:\n        value (Any): The input value to be converted to a string.\n\n    Returns:\n        tuple[str]: A single-element tuple containing the string representation of the input value.\n\n    Notes:\n        - The conversion is done using Python's native str() function\n        - All Python types are supported as they all implement __str__\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (any_type,),\n            }\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    CLASS_ID = \"any_string\"\n    DESCRIPTION = \"\"\"\n    Converts any input value to its string representation.\n    Converts any input value into a string format using Python's built-in str() function.\n    Useful for debugging, logging, or text-based operations.\n    \"\"\"\n\n    def execute(self, value: Any) -&gt; tuple[str]:\n        return (str(value),)\n</code></pre>"},{"location":"nodes/utils/#string2any","title":"String2Any","text":"<p>Safely converts a string representation to its Python object.</p> <p>Uses Python's ast.literal_eval for secure string evaluation, which only allows literal expressions (strings, numbers, tuples, lists, dicts, booleans, None).</p>"},{"location":"nodes/utils/#inputs_1","title":"Inputs","text":"Group Name Type Default Extras required string <code>STRING</code> Source code <pre><code>class String2Any:\n    \"\"\"Safely converts a string representation to its Python object.\n\n    Uses Python's ast.literal_eval for secure string evaluation, which only allows\n    literal expressions (strings, numbers, tuples, lists, dicts, booleans, None).\n\n    Args:\n        string (str): String representation of a Python literal.\n\n    Returns:\n        tuple[Any]: A single-element tuple containing the evaluated Python object.\n\n    Notes:\n        - Only evaluates literal expressions, preventing code execution\n        - Supports: strings, numbers, tuples, lists, dicts, booleans, None\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"string\": (\"STRING\",),\n            }\n        }\n\n    RETURN_TYPES = (any_type,)\n    RETURN_NAMES = (\"value\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    DESCRIPTION = \"\"\"\n    Safely converts a string representation to its Python object.\n    Uses Python's ast.literal_eval for secure string evaluation,\n    which only allows literal expressions (strings, numbers, tuples, lists, dicts, booleans, None).\n    Useful for converting string representations of Python objects back to their original Python objects.\n    \"\"\"\n\n    def execute(self, string: str) -&gt; tuple[Any]:\n        try:\n            return (ast.literal_eval(string),)\n        except (ValueError, SyntaxError) as e:\n            raise ValueError(f\"Invalid literal expression: {str(e)}\")\n</code></pre>"},{"location":"nodes/utils/#any2int","title":"Any2Int","text":"<p>Converts any input value to its int representation.</p> <p>This utility node provides a simple way to convert any input value into a int format using Python's built-in int() function. Useful for debugging, logging, or text-based operations.</p>"},{"location":"nodes/utils/#inputs_2","title":"Inputs","text":"Group Name Type Default Extras required value <code>any_type</code>"},{"location":"nodes/utils/#returns_1","title":"Returns","text":"Name Type int <code>INT</code> Source code <pre><code>class Any2Int:\n    \"\"\"Converts any input value to its int representation.\n\n    This utility node provides a simple way to convert any input value into a int format using\n    Python's built-in int() function. Useful for debugging, logging, or text-based operations.\n\n    Args:\n        value (Any): The input value to be converted to a int.\n\n    Returns:\n        tuple[int]: A single-element tuple containing the int representation of the input value.\n\n    Notes:\n        - The conversion is done using Python's native int() function\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (any_type,),\n            }\n        }\n\n    RETURN_TYPES = (\"INT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    DESCRIPTION = \"\"\"\n    Converts any input value to its int representation.\n    Converts any input value into a int format using Python's built-in int() function.\n    Useful for debugging, logging, or text-based operations.\n    \"\"\"\n\n    def execute(self, value: Any) -&gt; tuple[int]:\n        return (int(value),)\n</code></pre>"},{"location":"nodes/utils/#any2float","title":"Any2Float","text":"<p>Converts any input value to its float representation.</p> <p>This utility node provides a simple way to convert any input value into a float format using Python's built-in float() function. Useful for debugging, logging, or text-based operations.</p>"},{"location":"nodes/utils/#inputs_3","title":"Inputs","text":"Group Name Type Default Extras required value <code>any_type</code>"},{"location":"nodes/utils/#returns_2","title":"Returns","text":"Name Type float <code>FLOAT</code> Source code <pre><code>class Any2Float:\n    \"\"\"Converts any input value to its float representation.\n\n    This utility node provides a simple way to convert any input value into a float format using\n    Python's built-in float() function. Useful for debugging, logging, or text-based operations.\n\n    Args:\n        value (Any): The input value to be converted to a float.\n\n    Returns:\n        tuple[float]: A single-element tuple containing the float representation of the input value.\n\n    Notes:\n        - The conversion is done using Python's native float() function\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (any_type,),\n            }\n        }\n\n    RETURN_TYPES = (\"FLOAT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    DESCRIPTION = \"\"\"\n    Converts any input value to its float representation.\n    Converts any input value into a float format using Python's built-in float() function.\n    Useful for debugging, logging, or text-based operations.\n    \"\"\"\n\n    def execute(self, value: Any) -&gt; tuple[float]:\n        return (float(value),)\n</code></pre>"},{"location":"nodes/utils/#any2image","title":"Any2Image","text":"<p>Converts any inputs value to image format.</p> <p>A utility node that handles conversion of tensor inputs to a compatible image format for use in image processing workflows.</p>"},{"location":"nodes/utils/#inputs_4","title":"Inputs","text":"Group Name Type Default Extras required value <code>any_type</code>"},{"location":"nodes/utils/#returns_3","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class Any2Image:\n    \"\"\"Converts any inputs value to image format.\n\n    A utility node that handles conversion of tensor inputs to a compatible image format for use in\n    image processing workflows.\n\n    Args:\n        value (Any): The input value to be converted to image format.\n\n    Returns:\n        tuple[torch.Tensor]: A single-element tuple containing the image tensor.\n\n    Raises:\n        ValueError: If the input value is not a torch.Tensor or cannot be converted to image format.\n\n    Notes:\n        - Currently only supports torch.Tensor inputs\n        - Input tensors should be in a format compatible with image processing (BWHC format)\n        - Future versions may support additional input types and automatic conversion\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (any_type,),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    CLASS_ID = \"any_image\"\n    DESCRIPTION = \"\"\"\n    Converts any inputs value to image format.\n    A utility node that handles conversion of tensor inputs to a compatible image format,\n    for use in image processing workflows.\n    \"\"\"\n\n    def execute(self, value: Any) -&gt; tuple[torch.Tensor]:\n        if isinstance(value, torch.Tensor):\n            return (value,)\n        raise ValueError(f\"Unsupported type: {type(value)}\")\n</code></pre>"},{"location":"nodes/utils/#any2any","title":"Any2Any","text":"<p>Passes through any input value unchanged.</p> <p>A utility node that acts as a pass-through or identity function, returning the input value without any modifications. Useful for workflow organization or debugging.</p>"},{"location":"nodes/utils/#inputs_5","title":"Inputs","text":"Group Name Type Default Extras required value <code>any_type</code> Source code <pre><code>class Any2Any:\n    \"\"\"Passes through any input value unchanged.\n\n    A utility node that acts as a pass-through or identity function, returning the input value\n    without any modifications. Useful for workflow organization or debugging.\n\n    Args:\n        value (Any): Any input value to be passed through.\n\n    Returns:\n        tuple[Any]: A single-element tuple containing the unchanged input value.\n\n    Notes:\n        - No validation or transformation is performed on the input\n        - Useful as a placeholder or for debugging workflow connections\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (any_type,),\n            }\n        }\n\n    RETURN_TYPES = (any_type,)\n    RETURN_NAMES = (\"value\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    CLASS_ID = \"any2any\"\n\n    def execute(self, value: Any) -&gt; tuple[Any]:\n        return (value,)\n</code></pre>"},{"location":"nodes/utils/#rgb2hsv","title":"RGB2HSV","text":"<p>Converts RGB images to HSV color space.</p> <p>Transforms images from RGB (Red, Green, Blue) color space to HSV (Hue, Saturation, Value) color space while preserving the image structure and dimensions.</p>"},{"location":"nodes/utils/#inputs_6","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/utils/#returns_4","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class RGB2HSV:\n    \"\"\"Converts RGB images to HSV color space.\n\n    Transforms images from RGB (Red, Green, Blue) color space to HSV (Hue, Saturation, Value)\n    color space while preserving the image structure and dimensions.\n\n    Args:\n        image (torch.Tensor): Input RGB image tensor in BWHC format.\n\n    Returns:\n        tuple[torch.Tensor]: A single-element tuple containing the HSV image tensor in BWHC format.\n\n    Notes:\n        - Input must be in BWHC (Batch, Width, Height, Channels) format\n        - RGB values should be normalized to [0, 1] range\n        - Output HSV values are in ranges: H[0,360], S[0,1], V[0,1]\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    CLASS_ID = \"rgb_hsv\"\n    DESCRIPTION = \"\"\"\n    Converts RGB images to HSV color space.\n    Transforms images from RGB (Red, Green, Blue) color space to HSV (Hue, Saturation, Value)\n    color space while preserving the image structure and dimensions.\n    \"\"\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        image_tensor = TensorImage.from_BWHC(image)\n        output = rgb_to_hsv(image_tensor).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/utils/#rgb2hls","title":"RGB2HLS","text":"<p>Converts RGB images to HLS color space.</p> <p>Transforms images from RGB (Red, Green, Blue) color space to HLS (Hue, Lightness, Saturation) color space while preserving the image structure and dimensions.</p>"},{"location":"nodes/utils/#inputs_7","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/utils/#returns_5","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class RGB2HLS:\n    \"\"\"Converts RGB images to HLS color space.\n\n    Transforms images from RGB (Red, Green, Blue) color space to HLS (Hue, Lightness, Saturation)\n    color space while preserving the image structure and dimensions.\n\n    Args:\n        image (torch.Tensor): Input RGB image tensor in BWHC format.\n\n    Returns:\n        tuple[torch.Tensor]: A single-element tuple containing the HLS image tensor in BWHC format.\n\n    Notes:\n        - Input must be in BWHC (Batch, Width, Height, Channels) format\n        - RGB values should be normalized to [0, 1] range\n        - Output HLS values are in ranges: H[0,360], L[0,1], S[0,1]\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    CLASS_ID = \"rgb_hls\"\n    DESCRIPTION = \"\"\"\n    Converts RGB images to HLS color space.\n    Transforms images from RGB (Red, Green, Blue) color space to HLS (Hue, Lightness, Saturation)\n    color space while preserving the image structure and dimensions.\n    \"\"\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        image_tensor = TensorImage.from_BWHC(image)\n        output = rgb_to_hls(image_tensor).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/utils/#rgba2rgb","title":"RGBA2RGB","text":"<p>Converts RGBA images to RGB format.</p> <p>Transforms images from RGBA (Red, Green, Blue, Alpha) format to RGB format by removing the alpha channel. Passes through RGB images unchanged.</p>"},{"location":"nodes/utils/#inputs_8","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/utils/#returns_6","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class RGBA2RGB:\n    \"\"\"Converts RGBA images to RGB format.\n\n    Transforms images from RGBA (Red, Green, Blue, Alpha) format to RGB format by removing the\n    alpha channel. Passes through RGB images unchanged.\n\n    Args:\n        image (torch.Tensor): Input image tensor in BWHC format (either RGBA or RGB).\n\n    Returns:\n        tuple[torch.Tensor]: A single-element tuple containing the RGB image tensor in BWHC format.\n\n    Notes:\n        - Input must be in BWHC (Batch, Width, Height, Channels) format\n        - Handles both 3-channel (RGB) and 4-channel (RGBA) inputs\n        - RGB images are passed through unchanged\n        - Alpha channel is removed from RGBA images using rgba_to_rgb conversion\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    CLASS_ID = \"rgba2rgb\"\n    DESCRIPTION = \"\"\"\n    Converts RGBA images to RGB format.\n    Transforms images from RGBA (Red, Green, Blue, Alpha) format to RGB format by removing the alpha channel.\n    Passes through RGB images unchanged.\n    \"\"\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        image_tensor = TensorImage.from_BWHC(image)\n        if image_tensor.shape[1] == 4:\n            image_tensor = rgba_to_rgb(image_tensor)\n        output = image_tensor.get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/utils/#rgb2gray","title":"RGB2GRAY","text":"<p>Converts RGB images to grayscale format.</p> <p>This node transforms RGB color images to single-channel grayscale images using standard luminance conversion factors.</p>"},{"location":"nodes/utils/#inputs_9","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/utils/#returns_7","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class RGB2GRAY:\n    \"\"\"Converts RGB images to grayscale format.\n\n    This node transforms RGB color images to single-channel grayscale images using\n    standard luminance conversion factors.\n\n    Args:\n        image (torch.Tensor): Input RGB image in BWHC format\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing grayscale image in BWHC format\n\n    Notes:\n        - Uses standard RGB to grayscale conversion weights\n        - Output is single-channel\n        - Preserves image dimensions\n        - Values remain in [0,1] range\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    DESCRIPTION = \"\"\"\n    Converts RGB images to grayscale format.\n    This node transforms RGB color images to single-channel grayscale images\n    using standard luminance conversion factors.\n    \"\"\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        image_tensor = TensorImage.from_BWHC(image)\n        output = rgb_to_grayscale(image_tensor).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/utils/#gray2rgb","title":"GRAY2RGB","text":"<p>Converts grayscale images to RGB format.</p> <p>This node transforms single-channel grayscale images to three-channel RGB images by replicating the grayscale values across channels.</p>"},{"location":"nodes/utils/#inputs_10","title":"Inputs","text":"Group Name Type Default Extras required image <code>IMAGE</code>"},{"location":"nodes/utils/#returns_8","title":"Returns","text":"Name Type image <code>IMAGE</code> Source code <pre><code>class GRAY2RGB:\n    \"\"\"Converts grayscale images to RGB format.\n\n    This node transforms single-channel grayscale images to three-channel RGB images\n    by replicating the grayscale values across channels.\n\n    Args:\n        image (torch.Tensor): Input grayscale image in BWHC format\n\n    Returns:\n        tuple[torch.Tensor]: Single-element tuple containing RGB image in BWHC format\n\n    Notes:\n        - Replicates grayscale values to all RGB channels\n        - Output has three identical channels\n        - Preserves image dimensions\n        - Values remain in [0,1] range\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    DESCRIPTION = \"\"\"\n    Converts grayscale images to RGB format.\n    This node transforms single-channel grayscale images to three-channel RGB images\n    by replicating the grayscale values across channels.\n    \"\"\"\n\n    def execute(self, image: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        output = image\n        image_tensor = TensorImage.from_BWHC(image)\n        if image_tensor.shape[1] == 1:\n            output = grayscale_to_rgb(image_tensor).get_BWHC()\n        return (output,)\n</code></pre>"},{"location":"nodes/utils/#purgevram","title":"PurgeVRAM","text":"<p>Cleans up VRAM by forcing memory deallocation and cache clearing.</p> <p>A utility node that performs comprehensive VRAM cleanup by collecting garbage, emptying CUDA cache, and unloading models. Useful for managing memory usage in complex workflows.</p>"},{"location":"nodes/utils/#inputs_11","title":"Inputs","text":"Group Name Type Default Extras required anything <code>any_type</code> Source code <pre><code>class PurgeVRAM:\n    \"\"\"Cleans up VRAM by forcing memory deallocation and cache clearing.\n\n    A utility node that performs comprehensive VRAM cleanup by collecting garbage, emptying CUDA cache,\n    and unloading models. Useful for managing memory usage in complex workflows.\n\n    Args:\n        anything (Any): Any input value that will be passed through unchanged.\n\n    Returns:\n        tuple[Any]: A single-element tuple containing the unchanged input value.\n\n    Notes:\n        - Calls Python's garbage collector\n        - Clears CUDA cache if available\n        - Unloads and cleans up ComfyUI models\n        - Performs soft cache emptying\n        - Input value is passed through unchanged\n        - Useful for preventing out-of-memory errors\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"anything\": (any_type,),\n            },\n        }\n\n    RETURN_TYPES = (any_type,)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    OUTPUT_NODE = True\n    # DEPRECATED = True\n    CLASS_ID = \"purge_vram\"\n    DESCRIPTION = \"\"\"\n    Cleans up VRAM by forcing memory deallocation and cache clearing.\n    A utility node that performs comprehensive VRAM cleanup by collecting garbage,\n    emptying CUDA cache, and unloading models.\n    Useful for managing memory usage in complex workflows.\n    \"\"\"\n\n    def execute(self, anything: Any) -&gt; tuple[Any]:\n        clean_memory()\n        return (anything,)\n</code></pre>"},{"location":"nodes/utils/#waitseconds","title":"WaitSeconds","text":"<p>Pauses execution for a specified number of seconds.</p> <p>A utility node that introduces a delay in the workflow by sleeping for a given duration. This can be useful for timing control, pacing operations, or waiting for external processes to complete.</p>"},{"location":"nodes/utils/#inputs_12","title":"Inputs","text":"Group Name Type Default Extras required value <code>any_type</code> required seconds <code>FLOAT</code> 1.0 Source code <pre><code>class WaitSeconds:\n    \"\"\"Pauses execution for a specified number of seconds.\n\n    A utility node that introduces a delay in the workflow by sleeping for a given duration. This can\n    be useful for timing control, pacing operations, or waiting for external processes to complete.\n\n    Args:\n        value (Any): Any input value to be returned after the wait period.\n        seconds (float): The duration to wait in seconds. Defaults to 1.0 seconds.\n\n    Returns:\n        tuple[Any]: A single-element tuple containing the unchanged input value after the wait.\n\n    Notes:\n        - The wait time can be adjusted by changing the `seconds` argument.\n        - The function uses Python's time.sleep() to implement the delay.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):  # type: ignore\n        return {\n            \"required\": {\n                \"value\": (any_type,),\n                \"seconds\": (\n                    \"FLOAT\",\n                    {\n                        \"default\": 1.0,\n                    },\n                ),\n            }\n        }\n\n    RETURN_TYPES = (any_type,)\n    RETURN_NAMES = (\"value\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    DESCRIPTION = \"\"\"\n    Pauses execution for a specified number of seconds.\n    A utility node that introduces a delay in the workflow by sleeping for a given duration.\n    This can be useful for timing control, pacing operations, or waiting for external processes to complete.\n    \"\"\"\n\n    def execute(self, value: Any, seconds: float = 1.0) -&gt; tuple[Any]:\n        time.sleep(seconds)\n        return (value,)\n</code></pre>"},{"location":"nodes/utils/#listbuilder","title":"ListBuilder","text":"<p>Builds a list from input elements.</p> <p>A node that constructs a list from provided input elements. Used in node-based workflows to combine multiple elements into a single list output.</p> Source code <pre><code>class ListBuilder:\n    \"\"\"Builds a list from input elements.\n\n    A node that constructs a list from provided input elements. Used in node-based\n    workflows to combine multiple elements into a single list output.\n\n    Args:\n        elements (Any): Input elements to combine into a list. The specific types\n            accepted are defined in INPUT_TYPES.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The constructed list containing all input elements\n\n    Notes:\n        - The actual input types and number of elements that can be added to the list\n          are defined in the INPUT_TYPES class method\n        - This node is typically used in node graph systems to aggregate multiple\n          inputs into a single list output\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"required\": {\n                \"num_slots\": ([str(i) for i in range(1, 11)], {\"default\": \"1\"}),\n            },\n            \"optional\": {},\n        }\n\n        for i in range(1, 11):\n            inputs[\"optional\"].update(\n                {\n                    f\"value_{i}\": (any_type, {\"forceInput\": True}),\n                }\n            )\n        return inputs\n\n    RETURN_TYPES = (\n        any_type,\n        \"LIST\",\n    )\n    RETURN_NAMES = (\n        \"ANY\",\n        \"LIST\",\n    )\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    CLASS_ID = \"list_builder\"\n    OUTPUT_IS_LIST = (\n        True,\n        False,\n    )\n    DESCRIPTION = \"\"\"\n    Builds a list from input elements.\n    A node that constructs a list from provided input elements.\n    Used in node-based workflows to combine multiple elements into a single list output.\n    \"\"\"\n\n    def execute(self, num_slots: str = \"1\", **kwargs) -&gt; tuple[Any, list[Any]]:\n        list_stack = []\n        for i in range(1, int(num_slots) + 1):\n            list_value = kwargs.get(f\"value_{i}\")\n            if list_value is not None:\n                list_stack.append(list_value)\n        return (\n            list_stack,\n            list_stack,\n        )\n</code></pre>"},{"location":"nodes/utils/#latent2dict","title":"Latent2Dict","text":"<p>Converts a latent tensor representation to a dictionary format.</p> <p>Transforms a LATENT input (containing tensor data) into a structured dictionary that includes type information, shape, and tensor values.</p>"},{"location":"nodes/utils/#inputs_13","title":"Inputs","text":"Group Name Type Default Extras required latent <code>LATENT</code>"},{"location":"nodes/utils/#returns_9","title":"Returns","text":"Name Type dict <code>DICT</code> Source code <pre><code>class Latent2Dict:\n    \"\"\"Converts a latent tensor representation to a dictionary format.\n\n    Transforms a LATENT input (containing tensor data) into a structured dictionary\n    that includes type information, shape, and tensor values.\n\n    Args:\n        latent (LATENT): A latent tensor input.\n\n    Returns:\n        tuple[dict]: A single-element tuple containing a dictionary with the structure:\n            {\n                \"type\": \"LATENT\",\n                \"data\": {\n                    \"samples\": {\n                        \"type\": str,  # Tensor type name\n                        \"shape\": tuple,  # Tensor dimensions\n                        \"values\": list  # Tensor data as nested lists\n                    }\n                }\n            }\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"latent\": (\"LATENT\",),\n            }\n        }\n\n    RETURN_TYPES = (\"DICT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    OUTPUT_NODE = True\n    DESCRIPTION = \"\"\"\n    Converts a latent tensor representation to a dictionary format.\n    Transforms a LATENT input (containing tensor data) into a structured dictionary\n    that includes type information, shape, and tensor values.\n    \"\"\"\n\n    def execute(self, latent: dict) -&gt; tuple[dict]:\n        latent_dict = {\n            \"type\": \"LATENT\",\n            \"data\": {\n                \"samples\": {\n                    \"type\": str(type(latent[\"samples\"]).__name__),\n                    \"shape\": latent[\"samples\"].shape,\n                    \"values\": latent[\"samples\"].tolist(),\n                }\n            },\n        }\n\n        return (latent_dict,)\n</code></pre>"},{"location":"nodes/utils/#dict2latent","title":"Dict2Latent","text":"<p>Converts a dictionary representation back to a latent tensor format.</p> <p>Transforms a structured dictionary containing tensor data back into the LATENT format used by the system.</p>"},{"location":"nodes/utils/#inputs_14","title":"Inputs","text":"Group Name Type Default Extras required dict <code>DICT</code>"},{"location":"nodes/utils/#returns_10","title":"Returns","text":"Name Type latent <code>LATENT</code> Source code <pre><code>class Dict2Latent:\n    \"\"\"Converts a dictionary representation back to a latent tensor format.\n\n    Transforms a structured dictionary containing tensor data back into the LATENT\n    format used by the system.\n\n    Args:\n        dict (DICT): A dictionary containing tensor data in the format:\n            {\n                \"type\": \"LATENT\",\n                \"data\": {\n                    \"samples\": {\n                        \"type\": str,  # Tensor type name\n                        \"shape\": tuple,  # Tensor dimensions\n                        \"values\": list  # Tensor data as nested lists\n                    }\n                }\n            }\n\n    Returns:\n        tuple[LATENT]: A single-element tuple containing the reconstructed latent\n            tensor in the format: {\"samples\": tensor}\n\n    Raises:\n        ValueError: If the input dictionary is not of type \"LATENT\" or contains an\n            unsupported tensor type.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"dict\": (\"DICT\",),\n            }\n        }\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    OUTPUT_NODE = True\n    DESCRIPTION = \"\"\"\n    Converts a dictionary representation back to a latent tensor format.\n    Transforms a structured dictionary containing tensor data back into the LATENT\n    format used by the system.\n    \"\"\"\n\n    def execute(self, dict: dict) -&gt; tuple[dict]:\n        if dict.get(\"type\") != \"LATENT\":\n            raise ValueError(\"Input dictionary is not a LATENT type\")\n\n        samples_data = dict[\"data\"][\"samples\"]\n        tensor_type = samples_data[\"type\"]\n        if \"Tensor\" in tensor_type or \"GGMLTensor\" in tensor_type or \"TensorImage\" in tensor_type:\n            tensor_data = torch.tensor(samples_data[\"values\"])\n            tensor_data = tensor_data.reshape(samples_data[\"shape\"])\n        else:\n            raise ValueError(f\"Unsupported tensor type: {tensor_type}\")\n\n        latent = {\"samples\": tensor_data}\n\n        return (latent,)\n</code></pre>"},{"location":"nodes/utils/#inputlisttolist","title":"InputListToList","text":"<p>Converts a list input to a list as as single output.</p> <p>A utility node that takes an input list and returns a single list containing all the inputs.</p>"},{"location":"nodes/utils/#inputs_15","title":"Inputs","text":"Group Name Type Default Extras required list <code>any_type</code>"},{"location":"nodes/utils/#returns_11","title":"Returns","text":"Name Type list <code>LIST</code> Source code <pre><code>class InputListToList:\n    \"\"\"Converts a list input to a list as as single output.\n\n    A utility node that takes an input list and returns a single list containing all the inputs.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\"list\": (any_type,)},\n        }\n\n    RETURN_TYPES = (\"LIST\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    OUTPUT_NODE = True\n    INPUT_IS_LIST = True\n    CLASS_ID = \"input_list_to_list\"\n    DESCRIPTION = \"\"\"\n    Converts a list input to a list as a single output.\n    A utility node that takes an input list and returns a single list containing all the inputs.\n    \"\"\"\n\n    def execute(self, list: list[Any]) -&gt; tuple[list[Any]]:\n        return (list,)\n</code></pre>"},{"location":"nodes/utils/#listtooutputlist","title":"ListToOutputList","text":"<p>Converts a list input to a list as a single output.</p> <p>A utility node that takes a list and returns an output list for iterations.</p>"},{"location":"nodes/utils/#inputs_16","title":"Inputs","text":"Group Name Type Default Extras required list <code>LIST</code> Source code <pre><code>class ListToOutputList:\n    \"\"\"Converts a list input to a list as a single output.\n\n    A utility node that takes a list and returns an output list for iterations.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\"list\": (\"LIST\",)},\n        }\n\n    RETURN_TYPES = (any_type,)\n    RETURN_NAMES = (\"ANY\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    OUTPUT_IS_LIST = (True,)\n    CLASS_ID = \"list_to_output_list\"\n    DESCRIPTION = \"\"\"\n    Converts a list input to a list as a single output.\n    A utility node that takes a list and returns an output list for iterations.\n    \"\"\"\n\n    def execute(self, list: list[Any]) -&gt; tuple[list[Any]]:\n        return (list,)\n</code></pre>"},{"location":"nodes/utils/#batchbuilder","title":"BatchBuilder","text":"<p>Builds a batch from input images.</p> <p>A node that constructs a batch from provided input images usign the first one as the base. Used in node-based workflows to combine multiple images into a single batch output.</p> Source code <pre><code>class BatchBuilder:\n    \"\"\"Builds a batch from input images.\n\n    A node that constructs a batch from provided input images usign the first one as the base. Used in node-based\n    workflows to combine multiple images into a single batch output.\n\n    Args:\n        images (Image): Input images to combine into a batch.\n\n    Returns:\n        tuple: A tuple containing:\n            - batch: The constructed batch containing all input images\n\n    Notes:\n        - This node is typically used in node graph systems to\n        aggregate multiple image inputs into a single batch output\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        inputs = {\n            \"required\": {\n                \"num_slots\": ([str(i) for i in range(1, 11)], {\"default\": \"1\"}),\n            },\n            \"optional\": {},\n        }\n\n        for i in range(1, 11):\n            inputs[\"optional\"].update(\n                {\n                    f\"value_{i}\": (\"IMAGE, MASK\", {\"forceInput\": True}),\n                }\n            )\n        return inputs\n\n    RETURN_TYPES = (any_type,)\n    RETURN_NAMES = (\"ANY\",)\n    FUNCTION = \"execute\"\n    CATEGORY = UTILS_CAT\n    CLASS_ID = \"batch_builder\"\n    DESCRIPTION = \"\"\"\n    Builds a batch from input images.\n    A node that constructs a batch from provided input images usign the first one as the base.\n    Used in node-based workflows to combine multiple images into a single batch output.\n    \"\"\"\n\n    def execute(self, num_slots: str = \"1\", **kwargs) -&gt; tuple[torch.Tensor]:\n        if f\"value_{int(num_slots)}\" not in kwargs.keys():\n            raise ValueError(\"Number of inputs is not equal to number of slots\")\n\n        base = kwargs.get(\"value_1\")\n        if base is None:\n            raise ValueError(\"Base image is not provided\")\n        base_shape = TensorImage.from_BWHC(base).shape\n        images = []\n\n        for i in range(1, int(num_slots) + 1):\n            image = kwargs.get(f\"value_{i}\")\n            if image is None:\n                raise ValueError(f\"Image in value_{i} is not provided\")\n            image_shape = TensorImage.from_BWHC(image).shape\n\n            # Ensure mask tensors are properly shaped (add channels dimension if needed)\n            if len(image_shape) == 3 or (len(image_shape) == 4 and image_shape[1] == 1):\n                # For masks, ensure they're in the correct format (B,1,H,W)\n                if len(image.shape) == 3:\n                    image = image.unsqueeze(3)  # Add channel dimension\n\n            if base_shape[1:] == image_shape[1:]:\n                images.append(image)\n            else:\n                raise ValueError(f\"Image/Mask in value_{i} is not the same shape as the image/mask in value_1\")\n\n        images = torch.cat(images, dim=0)\n        return (images,)\n</code></pre>"},{"location":"nodes/vectorstore/","title":"Vectorstore Nodes","text":""}]}